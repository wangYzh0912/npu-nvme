# 1 "/home/user3/npu-nvme/hello_world.cpp"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 404 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 1 3
# 17 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 1 3
# 35 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 3
typedef long int ptrdiff_t;
# 46 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 3
typedef long unsigned int size_t;
# 102 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__stddef_max_align_t.h" 1 3
# 19 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__stddef_max_align_t.h" 3
typedef struct {
  long long __clang_max_align_nonce1
      __attribute__((__aligned__(__alignof__(long long))));
  long double __clang_max_align_nonce2
      __attribute__((__aligned__(__alignof__(long double))));
} max_align_t;
# 103 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 2 3
# 18 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdint.h" 1 3
# 52 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdint.h" 3
# 1 "/usr/include/stdint.h" 1 3 4
# 26 "/usr/include/stdint.h" 3 4
# 1 "/usr/include/bits/libc-header-start.h" 1 3 4
# 33 "/usr/include/bits/libc-header-start.h" 3 4
# 1 "/usr/include/features.h" 1 3 4
# 394 "/usr/include/features.h" 3 4
# 1 "/usr/include/features-time64.h" 1 3 4
# 20 "/usr/include/features-time64.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 21 "/usr/include/features-time64.h" 2 3 4
# 1 "/usr/include/bits/timesize.h" 1 3 4
# 22 "/usr/include/features-time64.h" 2 3 4
# 395 "/usr/include/features.h" 2 3 4
# 481 "/usr/include/features.h" 3 4
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 482 "/usr/include/features.h" 2 3 4
# 503 "/usr/include/features.h" 3 4
# 1 "/usr/include/sys/cdefs.h" 1 3 4
# 576 "/usr/include/sys/cdefs.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 577 "/usr/include/sys/cdefs.h" 2 3 4
# 1 "/usr/include/bits/long-double.h" 1 3 4
# 578 "/usr/include/sys/cdefs.h" 2 3 4
# 504 "/usr/include/features.h" 2 3 4
# 527 "/usr/include/features.h" 3 4
# 1 "/usr/include/gnu/stubs.h" 1 3 4




# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 6 "/usr/include/gnu/stubs.h" 2 3 4


# 1 "/usr/include/gnu/stubs-lp64.h" 1 3 4
# 9 "/usr/include/gnu/stubs.h" 2 3 4
# 528 "/usr/include/features.h" 2 3 4
# 34 "/usr/include/bits/libc-header-start.h" 2 3 4
# 27 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/bits/types.h" 1 3 4
# 27 "/usr/include/bits/types.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 28 "/usr/include/bits/types.h" 2 3 4
# 1 "/usr/include/bits/timesize.h" 1 3 4
# 29 "/usr/include/bits/types.h" 2 3 4


typedef unsigned char __u_char;
typedef unsigned short int __u_short;
typedef unsigned int __u_int;
typedef unsigned long int __u_long;


typedef signed char __int8_t;
typedef unsigned char __uint8_t;
typedef signed short int __int16_t;
typedef unsigned short int __uint16_t;
typedef signed int __int32_t;
typedef unsigned int __uint32_t;

typedef signed long int __int64_t;
typedef unsigned long int __uint64_t;






typedef __int8_t __int_least8_t;
typedef __uint8_t __uint_least8_t;
typedef __int16_t __int_least16_t;
typedef __uint16_t __uint_least16_t;
typedef __int32_t __int_least32_t;
typedef __uint32_t __uint_least32_t;
typedef __int64_t __int_least64_t;
typedef __uint64_t __uint_least64_t;



typedef long int __quad_t;
typedef unsigned long int __u_quad_t;







typedef long int __intmax_t;
typedef unsigned long int __uintmax_t;
# 141 "/usr/include/bits/types.h" 3 4
# 1 "/usr/include/bits/typesizes.h" 1 3 4
# 142 "/usr/include/bits/types.h" 2 3 4
# 1 "/usr/include/bits/time64.h" 1 3 4
# 143 "/usr/include/bits/types.h" 2 3 4


typedef unsigned long int __dev_t;
typedef unsigned int __uid_t;
typedef unsigned int __gid_t;
typedef unsigned long int __ino_t;
typedef unsigned long int __ino64_t;
typedef unsigned int __mode_t;
typedef unsigned int __nlink_t;
typedef long int __off_t;
typedef long int __off64_t;
typedef int __pid_t;
typedef struct { int __val[2]; } __fsid_t;
typedef long int __clock_t;
typedef unsigned long int __rlim_t;
typedef unsigned long int __rlim64_t;
typedef unsigned int __id_t;
typedef long int __time_t;
typedef unsigned int __useconds_t;
typedef long int __suseconds_t;
typedef long int __suseconds64_t;

typedef int __daddr_t;
typedef int __key_t;


typedef int __clockid_t;


typedef void * __timer_t;


typedef int __blksize_t;




typedef long int __blkcnt_t;
typedef long int __blkcnt64_t;


typedef unsigned long int __fsblkcnt_t;
typedef unsigned long int __fsblkcnt64_t;


typedef unsigned long int __fsfilcnt_t;
typedef unsigned long int __fsfilcnt64_t;


typedef long int __fsword_t;

typedef long int __ssize_t;


typedef long int __syscall_slong_t;

typedef unsigned long int __syscall_ulong_t;



typedef __off64_t __loff_t;
typedef char *__caddr_t;


typedef long int __intptr_t;


typedef unsigned int __socklen_t;




typedef int __sig_atomic_t;
# 28 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/bits/wchar.h" 1 3 4
# 29 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 30 "/usr/include/stdint.h" 2 3 4




# 1 "/usr/include/bits/stdint-intn.h" 1 3 4
# 24 "/usr/include/bits/stdint-intn.h" 3 4
typedef __int8_t int8_t;
typedef __int16_t int16_t;
typedef __int32_t int32_t;
typedef __int64_t int64_t;
# 35 "/usr/include/stdint.h" 2 3 4


# 1 "/usr/include/bits/stdint-uintn.h" 1 3 4
# 24 "/usr/include/bits/stdint-uintn.h" 3 4
typedef __uint8_t uint8_t;
typedef __uint16_t uint16_t;
typedef __uint32_t uint32_t;
typedef __uint64_t uint64_t;
# 38 "/usr/include/stdint.h" 2 3 4





typedef __int_least8_t int_least8_t;
typedef __int_least16_t int_least16_t;
typedef __int_least32_t int_least32_t;
typedef __int_least64_t int_least64_t;


typedef __uint_least8_t uint_least8_t;
typedef __uint_least16_t uint_least16_t;
typedef __uint_least32_t uint_least32_t;
typedef __uint_least64_t uint_least64_t;





typedef signed char int_fast8_t;

typedef long int int_fast16_t;
typedef long int int_fast32_t;
typedef long int int_fast64_t;
# 71 "/usr/include/stdint.h" 3 4
typedef unsigned char uint_fast8_t;

typedef unsigned long int uint_fast16_t;
typedef unsigned long int uint_fast32_t;
typedef unsigned long int uint_fast64_t;
# 87 "/usr/include/stdint.h" 3 4
typedef long int intptr_t;


typedef unsigned long int uintptr_t;
# 101 "/usr/include/stdint.h" 3 4
typedef __intmax_t intmax_t;
typedef __uintmax_t uintmax_t;
# 53 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdint.h" 2 3
# 19 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3






# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 1 3
# 10 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_defines.h" 1 3
# 11 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 2 3





typedef __bf16 bfloat16_t;
# 28 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
struct dim3 {
  unsigned int x, y, z;

  dim3(unsigned int vx = 1, unsigned int vy = 1, unsigned int vz = 1)
      : x(vx), y(vy), z(vz) {}

};

typedef struct dim3 dim3;
# 71 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 1 3




typedef enum {

  CRFMODE_NONE = 0,
  CRFMODE_DEQSCALE_VDEQ8 = 8,
  CRFMODE_DEQSCALE_DEQ8 = 9,
  CRFMODE_DEQSCALE_VDEQ16 = 10,
  CRFMODE_DEQSCALE_DEQ16 = 11,
  CRFMODE_DEQSCALE_VDEQS16 = 12,
  CRFMODE_DEQSCALE_DEQS16 = 13,
  CRFMODE_DEQSCALE_VDEQ2S16 = 14,
  CRFMODE_DEQSCALE_DEQ2S16 = 15,

} ConvReluFix_t;

typedef enum {


  CRMODE_NONE = 0,
  CRMODE_F32toF16_NONE = 1,
  CRMODE_F32toF16_RELU = 2,
  CRMODE_S32toF16_NONE = 3,
  CRMODE_F16toF32_NONE = 4,
  CRMODE_NONE_RELU = 5,
  CRMODE_F16_MUL = 6,
  CRMODE_S32toF16_DEQSCALE_SPR = 7,
  CRMODE_DEQSCALE_VDEQ8 = 8,
  CRMODE_DEQSCALE_DEQ8 = 9,
  CRMODE_DEQSCALE_VDEQ16 = 10,
  CRMODE_DEQSCALE_DEQ16 = 11,
  CRMODE_DEQSCALE_VDEQS16 = 12,
  CRMODE_DEQSCALE_DEQS16 = 13,

} ConvRelu_t;

typedef enum {

  NoConversion = 0,
  CvtMode1 = 1,
  CvtMode2 = 2,

} CvtMode_t;

typedef enum {

  DI_featuremap = 0,
  DI_others = 1,

} DI_t;

typedef enum {


  DUAL_MODE0 = 0,
  DUAL_MODE1 = 1,
  DUAL_MODE2 = 2,
  DUAL_MODE3 = 3,
  DUAL_MODE4 = 4,

} DualMode_t;

typedef enum {

    VALUE_INDEX = 0,
    INDEX_VALUE = 1,
    ONLY_VALUE = 2,
    ONLY_INDEX = 3,

} Order_t;

typedef enum {


  NoPooling = 0,
  AVGPooling = 1,
  MAXPooling = 2,
  GAVGPooling = 3,

} Pool_t;

typedef enum {

  NoConv= 0,
  VQS162B8_POST = 1,
  QS162B8_POST = 2,
  VQF162B8_POST = 3,
  QF162B8_POST = 4,
  VQS162S4_POST = 5,
  QS162S4_POST = 6,
  VQF162S4_POST = 7,
  QF162S4_POST = 8,
  VQS162S16_POST = 9,
  QS162S16_POST = 10,
  VQF162S16_POST = 11,
  QF162S16_POST = 12,

} QuantMode_post;

typedef enum {

  NoQuant= 0,
# 119 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
  F322F16 = 1,

  AttachF16Mul = 6,

  VREQ8 = 8,
  REQ8 = 9,
  VDEQF16 = 10,
  DEQF16 = 11,

  VSHIFTS322S16 = 12,
  SHIFTS322S16 = 13,

  F322BF16 = 16,

  VQF162B8_PRE = 17,
  QF162B8_PRE = 18,
  VQF162S4_PRE = 19,
  QF162S4_PRE = 20,

  VREQ4 = 21,
  REQ4 = 22,
  VQF322B8_PRE = 23,
  QF322B8_PRE = 24,
  VQF322S4_PRE = 25,
  QF322S4_PRE = 26,

  VDEQS16 = 27,
  DEQS16 = 28,
  VQF162S16_PRE = 29,
  QF162S16_PRE = 30,



} QuantMode_t;

typedef enum {

  NoRelu = 0,
  NormalRelu = 1,
  ScalarRelu = 2,
  VectorRelu = 3,
  LUTActivation = 4,

} ReluMode_t;

typedef enum {


  NoRELU = 0,
  NormalRELU = 1,
  LeakyRELU = 2,
  PRELU = 3,

} Relu_t;

typedef enum {


  NoREQ = 0,
  REQ = 1,
  VREQ = 2,

} Req_t;

typedef enum {

    MODE0 = 0,
    MODE1 = 1,
    MODE2 = 2,

} VSEL_mode_t;

typedef enum {




  DATA_EXP_0 = 48,
  DATA_EXP_1 = 49,
  DATA_EXP_2 = 50,
  DATA_EXP_3 = 51,

} VSPR_t;

typedef enum {

  inc = 0,
  dec = 1,

} addr_cal_mode_t;

typedef enum {

  ATOMIC_SUM = 0,


} atomic_op_t;

typedef enum {

  ATOMIC_NONE = 0,
  ATOMIC_F32 = 1,
  ATOMIC_F16 = 2,
  ATOMIC_S16 = 3,
  ATOMIC_S32 = 4,
  ATOMIC_S8 = 5,
  ATOMIC_BF16 = 6,

} atomic_type_t;

typedef enum {


  BM_DISABLE = 0,
  BM_ENABLE = 1,

} bm_t;

typedef enum {

  SINGLE_CACHE_LINE = 0,
  ENTIRE_DATA_CACHE,

} cache_line_t;

typedef enum {

  CSIZE0 = 0,
  CSIZE1 = 1,

} csize_t;

typedef enum {

  CACHELINE_ALL = 0,
  CACHELINE_UB,
  CACHELINE_OUT,

  CACHELINE_ATOMIC,


} dcci_dst_t;

typedef enum {

  e0 = 0,
  e2 = 2,
  e4 = 4,
  e6 = 6,

} even0_7_t;

typedef enum {

  EVENT_ID0 = 0,
  EVENT_ID1,
  EVENT_ID2,
  EVENT_ID3,

  EVENT_ID4,
  EVENT_ID5,
  EVENT_ID6,
  EVENT_ID7,


} event_t;

typedef enum {

  DSB_ALL = 0,
  DSB_DDR,
  DSB_UB,
  DSB_SEQ,

} mem_dsb_t;

typedef enum {

  L1 = 0,
  L0A,
  L0B,
  L0C,
  UB,
  BT,

} mem_t;

typedef enum {


  PAD_NONE = 0,
  PAD_MODE1 = 1,
  PAD_MODE2 = 2,
  PAD_MODE3 = 3,
  PAD_MODE4 = 4,
  PAD_MODE5 = 5,
  PAD_MODE6 = 6,
  PAD_MODE7 = 7,
  PAD_MODE8 = 8,

} pad_t;

typedef enum {

  PIPE_S = 0,
  PIPE_V,
  PIPE_M,
  PIPE_MTE1,
  PIPE_MTE2,
  PIPE_MTE3,
  PIPE_ALL,

  PIPE_MTE4 = 7,
  PIPE_MTE5 = 8,

  PIPE_V2 = 9,



  PIPE_FIX = 10,



} pipe_t;

typedef enum {







  VA0 = 0,
  VA1,
  VA2,
  VA3,
  VA4,
  VA5,
  VA6,
  VA7,


} ub_addr8_t;

typedef enum {

  UFMode0 = 0,
  Reserved,
  UFMode2,
  UFMode3,

} unit_flag_t;

typedef enum {

  L128 = 0,
  H128,

} vpart_t;

typedef enum {

  b8 = 0,
  b16 = 1,
  b32 = 2,
  s8 = 3,
  s32 = 4,
  f16 = 5,
  fmix = 6,
  f32 = 7,

} vtype_t;

typedef enum {

  W_3 = 0,
  W_5,

} w_size_t;
# 72 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 2 3
# 26 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 1 3
# 13 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
struct ulong_2;
# 52 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
struct __cce_builtin_block_t {
  __declspec( property(get = __fetch_builtin_idx)) unsigned long long idx; static inline __attribute__((cce_builtin_api, always_inline))[aicore] unsigned long long __fetch_builtin_idx(void) { return get_block_idx(); };
  __declspec( property(get = __fetch_builtin_num)) unsigned long long num; static inline __attribute__((cce_builtin_api, always_inline))[aicore] unsigned long long __fetch_builtin_num(void) { return get_block_num(); };

private:
  [aicore] __cce_builtin_block_t() = delete; [aicore] __cce_builtin_block_t(const __cce_builtin_block_t &) = delete; [aicore] void operator=(const __cce_builtin_block_t &) const = delete; [aicore] __cce_builtin_block_t *operator&() const = delete;
};


extern const[aicore] __attribute__((weak)) __cce_builtin_block_t block;







struct ulong_2 {
  unsigned long long idx, num;
};

typedef struct ulong_2 ulong_2;
# 28 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime.h" 1 3
# 15 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime.h" 3
extern "C" {



unsigned int rtConfigureCall(uint32_t numBlocks, void *smDesc = nullptr, void *stream = nullptr);





}
# 30 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 1 3
# 131 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void trap(uint64_t err_code) {
  __builtin_cce_trap_mov(err_code);
}
# 32 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 1 3
# 642 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void
copy_data_align64(uint8_t *dst, __attribute__((cce_unif_buff)) uint8_t *src, uint64_t size) {
  __builtin_cce_copy_from_ub_align64(dst, src, size);
}







static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data_align64(uint8_t *dst, __attribute__((cce_global)) uint8_t *src,
                                             uint64_t size) {
  __builtin_cce_copy_from_gm_align64(dst, src, size);
}







static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data_align64(uint8_t *dst, uint8_t *src,
                                             uint64_t size) {
  __builtin_cce_copy_from_stack_align64(dst, src, size);
}







static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, __attribute__((cce_unif_buff)) uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_ub(dst, src, size);
}







static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, __attribute__((cce_global)) uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_gm(dst, src, size);
}







static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_stack(dst, src, size);
}
# 719 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
template <class T1, class T2>


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void nop_reinterpret_cast(const T1 *dst, const T2 *src) {
  return __builtin_nop_reinterpret_cast(dst, src);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_set_flag(pipe_t p, pipe_t tp, event_t n) {
  set_flag(PIPE_M, PIPE_V, n);
  (void)p;
  (void)tp;
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_wait_flag(pipe_t p, pipe_t tp, event_t n) {
  wait_flag(PIPE_M, PIPE_V, n);
  (void)p;
  (void)tp;
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_pipe_barrier(pipe_t p) {
  pipe_barrier(PIPE_V);
  (void)p;
}






static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long abs(long long in) { return __builtin_cce_llabs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long abs(long in) { return __builtin_cce_labs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int abs(int in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short abs(short in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char abs(char in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float abs(float in) { return __builtin_cce_fabsf(in); };
# 766 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(long long in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(long in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(int in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(short in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(char in) { return SQRT_s64(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float sqrt(float in) { return __builtin_cce_sqrtf(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] half sqrt(half in) { return __builtin_cce_sqrtf16(in); }


static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_S)))[aicore] void __dummy_pipe_s() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_M)))[aicore] void __dummy_pipe_m() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void __dummy_pipe_v() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE1)))[aicore] void __dummy_pipe_mte1() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE2)))[aicore] void __dummy_pipe_mte2() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE3)))[aicore] void __dummy_pipe_mte3() {}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long max(long long in1, long long in2) {
  return __builtin_cce_llsmax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long max(long in1, long in2) {
  return __builtin_cce_lsmax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int max(int in1, int in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short max(short in1, short in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char max(char in1, char in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long long max(unsigned long long in1,
                                             unsigned long long in2) {
  return __builtin_cce_llumax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long max(unsigned long in1, unsigned long in2) {
  return __builtin_cce_lumax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned int max(unsigned int in1, unsigned int in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned short max(unsigned short in1,
                                         unsigned short in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned char max(unsigned char in1, unsigned char in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float max(float in1, float in2) {
  return __builtin_cce_fmaxf(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] half max(half in1, half in2) {
  return __builtin_cce_fmaxf16(in1, in2);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long min(long long in1, long long in2) {
  return __builtin_cce_llsmin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long min(long in1, long in2) {
  return __builtin_cce_lsmin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int min(int in1, int in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short min(short in1, short in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char min(char in1, char in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long long min(unsigned long long in1,
                                             unsigned long long in2) {
  return __builtin_cce_llumin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long min(unsigned long in1, unsigned long in2) {
  return __builtin_cce_lumin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned int min(unsigned int in1, unsigned int in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned short min(unsigned short in1,
                                         unsigned short in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned char min(unsigned char in1, unsigned char in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float min(float in1, float in2) {
  return __builtin_cce_fminf(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] half min(half in1, half in2) {
  return __builtin_cce_fminf16(in1, in2);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_ubuf_to_sbuf(void *dst, __attribute__((cce_unif_buff)) void *src,
                                             uint64_t size, int64_t inc) {
  MOV_UB_TO_SB(dst, src, size, inc);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_sbuf_to_ubuf(__attribute__((cce_unif_buff)) void *dst, void *src,
                                             uint64_t size, int64_t inc) {
  MOV_SB_TO_UB(dst, src, size, inc);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS16_f16(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS16_f32(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src0,
                                    __attribute__((cce_unif_buff)) unsigned int *src1,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS32_f16(dst, src0, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                    __attribute__((cce_unif_buff)) unsigned int *src1,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VBS32_f32(dst, src0, src1, config);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vaadd(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src0,
                                 __attribute__((cce_unif_buff)) half *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VAADD_f16(dst, src0, src1, config);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vaadd(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                 __attribute__((cce_unif_buff)) float *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VAADD_f32(dst, src0, src1, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vmergech(__attribute__((cce_unif_buff)) uint8_t *dst,
                                    __attribute__((cce_unif_buff)) uint8_t *src, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VMERGECH_b8(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vmergech(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VMERGECH_f16(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vrpac(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src,
                                 uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VRPAC_f16(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vrpac(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src,
                                 uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VRPAC_f32(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void viou(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src0,
                                __attribute__((cce_unif_buff)) half *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VIOU_f16(dst, src0, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void viou(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                __attribute__((cce_unif_buff)) float *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  VIOU_f32(dst, src0, src1, config);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] inline void icache_preload(int64_t n) {
  preload((const void *)get_pc(), n);
}
# 1104 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) uint64_t *src, unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) half *src[4], unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) float *src[4], unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) half *dst,
                                                     __attribute__((cce_unif_buff)) uint64_t *src0,
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) half *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) uint64_t *src, uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f16_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) half *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) half *dst,
                                                     __attribute__((cce_unif_buff)) half *src0[4],
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) half *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) half *src[4], uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f16_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) half *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) float *dst,
                                                     __attribute__((cce_unif_buff)) uint64_t *src0,
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) uint64_t *src, uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f32_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) float *dst,
                                                     __attribute__((cce_unif_buff)) float *src0[4],
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src[4], uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask) __attribute__((cce_range_check("VMRGSORT_f32_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, repeat, list0, list1, list2, list3,
            enable_exh_sus, mask);
}
# 1233 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(uint32_t value) { set_l0_set_value_ui(value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(half value) { set_l0_set_value_h(value); };

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(bfloat16_t value) { set_l0_set_value_bf16(value); };
# 1248 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, half value) { create_ca_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, uint32_t value) { create_ca_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { create_ca_matrix_bf16(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, half value) { create_cb_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, uint32_t value) { create_cb_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { create_cb_matrix_bf16(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, half value) { create_cbuf_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, uint32_t value) { create_cbuf_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { create_cbuf_matrix_bf16(dst, repeat, value); };
# 1505 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_va_reg_sb(ub_addr8_t addr, uint64_t *array) {
  switch (addr) {
  case VA0:
    MOVEVA((VA0), 0, (array[0]), (array[1]));
    MOVEVA((VA0), 2, (array[2]), (array[3]));
    MOVEVA((VA0), 4, (array[4]), (array[5]));
    MOVEVA((VA0), 6, (array[6]), (array[7]));
    break;
  case VA1:
    MOVEVA((VA1), 0, (array[0]), (array[1]));
    MOVEVA((VA1), 2, (array[2]), (array[3]));
    MOVEVA((VA1), 4, (array[4]), (array[5]));
    MOVEVA((VA1), 6, (array[6]), (array[7]));
    break;
  case VA2:
    MOVEVA((VA2), 0, (array[0]), (array[1]));
    MOVEVA((VA2), 2, (array[2]), (array[3]));
    MOVEVA((VA2), 4, (array[4]), (array[5]));
    MOVEVA((VA2), 6, (array[6]), (array[7]));
    break;
  case VA3:
    MOVEVA((VA3), 0, (array[0]), (array[1]));
    MOVEVA((VA3), 2, (array[2]), (array[3]));
    MOVEVA((VA3), 4, (array[4]), (array[5]));
    MOVEVA((VA3), 6, (array[6]), (array[7]));
    break;
  case VA4:
    MOVEVA((VA4), 0, (array[0]), (array[1]));
    MOVEVA((VA4), 2, (array[2]), (array[3]));
    MOVEVA((VA4), 4, (array[4]), (array[5]));
    MOVEVA((VA4), 6, (array[6]), (array[7]));
    break;
  case VA5:
    MOVEVA((VA5), 0, (array[0]), (array[1]));
    MOVEVA((VA5), 2, (array[2]), (array[3]));
    MOVEVA((VA5), 4, (array[4]), (array[5]));
    MOVEVA((VA5), 6, (array[6]), (array[7]));
    break;
  case VA6:
    MOVEVA((VA6), 0, (array[0]), (array[1]));
    MOVEVA((VA6), 2, (array[2]), (array[3]));
    MOVEVA((VA6), 4, (array[4]), (array[5]));
    MOVEVA((VA6), 6, (array[6]), (array[7]));
    break;
  case VA7:
    MOVEVA((VA7), 0, (array[0]), (array[1]));
    MOVEVA((VA7), 2, (array[2]), (array[3]));
    MOVEVA((VA7), 4, (array[4]), (array[5]));
    MOVEVA((VA7), 6, (array[6]), (array[7]));
    break;
  default:
    break;
  }
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_va_reg(ub_addr8_t addr,
                                      __attribute__((cce_unif_buff)) uint64_t *array) {
  switch (addr) {
  case VA0:
    MOVEVA((VA0), 0, (array[0]), (array[1]));
    MOVEVA((VA0), 2, (array[2]), (array[3]));
    MOVEVA((VA0), 4, (array[4]), (array[5]));
    MOVEVA((VA0), 6, (array[6]), (array[7]));
    break;
  case VA1:
    MOVEVA((VA1), 0, (array[0]), (array[1]));
    MOVEVA((VA1), 2, (array[2]), (array[3]));
    MOVEVA((VA1), 4, (array[4]), (array[5]));
    MOVEVA((VA1), 6, (array[6]), (array[7]));
    break;
  case VA2:
    MOVEVA((VA2), 0, (array[0]), (array[1]));
    MOVEVA((VA2), 2, (array[2]), (array[3]));
    MOVEVA((VA2), 4, (array[4]), (array[5]));
    MOVEVA((VA2), 6, (array[6]), (array[7]));
    break;
  case VA3:
    MOVEVA((VA3), 0, (array[0]), (array[1]));
    MOVEVA((VA3), 2, (array[2]), (array[3]));
    MOVEVA((VA3), 4, (array[4]), (array[5]));
    MOVEVA((VA3), 6, (array[6]), (array[7]));
    break;
  case VA4:
    MOVEVA((VA4), 0, (array[0]), (array[1]));
    MOVEVA((VA4), 2, (array[2]), (array[3]));
    MOVEVA((VA4), 4, (array[4]), (array[5]));
    MOVEVA((VA4), 6, (array[6]), (array[7]));
    break;
  case VA5:
    MOVEVA((VA5), 0, (array[0]), (array[1]));
    MOVEVA((VA5), 2, (array[2]), (array[3]));
    MOVEVA((VA5), 4, (array[4]), (array[5]));
    MOVEVA((VA5), 6, (array[6]), (array[7]));
    break;
  case VA6:
    MOVEVA((VA6), 0, (array[0]), (array[1]));
    MOVEVA((VA6), 2, (array[2]), (array[3]));
    MOVEVA((VA6), 4, (array[4]), (array[5]));
    MOVEVA((VA6), 6, (array[6]), (array[7]));
    break;
  case VA7:
    MOVEVA((VA7), 0, (array[0]), (array[1]));
    MOVEVA((VA7), 2, (array[2]), (array[3]));
    MOVEVA((VA7), 4, (array[4]), (array[5]));
    MOVEVA((VA7), 6, (array[6]), (array[7]));
    break;
  default:
    break;
  }
}
# 2286 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void clear_overflow_status() {
  uint64_t a = fake_overflow_status_1();
  uint64_t b = fake_overflow_status_2();
  CLEAR_OVERFLOW_STATUS(a, b);
}
# 2545 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
# 2557 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) half *c, __attribute__((cce_cube_a)) half *a, __attribute__((cce_cube_b)) half *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) half *a, __attribute__((cce_cube_b)) half *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) half *c, __attribute__((cce_cube_a)) half *a, __attribute__((cce_cube_b)) half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) half *a, __attribute__((cce_cube_b)) half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) half *c, __attribute__((cce_cube_a)) half *a, __attribute__((cce_cube_b)) half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) half *a, __attribute__((cce_cube_b)) half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint64_t config) { mad_s4((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint64_t config) { mad_tf322f32((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_s4((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad_s4(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_tf322f32((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { mad_tf322f32(c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_s4((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad_s4(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_tf322f32((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { mad_tf322f32(c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { mad_sp((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { mad_sp((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } else { mad_sp(c, a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } };
# 2651 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int8_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int8_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint8_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint8_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int16_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int16_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint16_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint16_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) half *
get_ub_virtual_address(__attribute__((cce_unif_buff)) half *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int32_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int32_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint32_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint32_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) float *
get_ub_virtual_address(__attribute__((cce_unif_buff)) float *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] uint64_t get_ub_virtual_address(uint64_t addr) {
  return addr;
}
# 2727 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
template <typename T>
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(int64_t value, __attribute__((cce_global)) T *addr) {
  __atomic_store_n(addr, value, 0);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(float value, __attribute__((cce_global)) float *addr) {
  typedef union {
    int32_t i;
    float f;
  } val;
  val tmp;
  tmp.f = value;
  __atomic_store_n(reinterpret_cast<__attribute__((cce_global)) int32_t *>(addr), tmp.i,
                   0);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(half value, __attribute__((cce_global)) half *addr) {
  typedef union {
    int16_t i;
    half f;
  } val;
  val tmp;
  tmp.f = value;
  __atomic_store_n(reinterpret_cast<__attribute__((cce_global)) int16_t *>(addr), tmp.i,
                   0);
}


namespace bisheng {
namespace cce {


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void metrics_prof_start() {
  pipe_barrier(PIPE_ALL);
  set_ctrl(sbitset1(get_ctrl(), 0));
  pipe_barrier(PIPE_ALL);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void metrics_prof_stop() {
  pipe_barrier(PIPE_ALL);
  set_ctrl(sbitset0(get_ctrl(), 0));
  pipe_barrier(PIPE_ALL);
}

}
}
# 34 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicpu_neon.h" 1 3
# 71 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_aicpu_neon.h" 3
typedef float float32_t;
typedef half float16_t;
typedef double float64_t;
typedef int8_t poly8_t;
typedef int16_t poly16_t;
typedef uint64_t poly64_t;
typedef __uint128_t poly128_t;

typedef __attribute__((invalid_vector_type(8))) int8_t int8x8_t;
typedef __attribute__((invalid_vector_type(16))) int8_t int8x16_t;
typedef __attribute__((invalid_vector_type(4))) int16_t int16x4_t;
typedef __attribute__((invalid_vector_type(8))) int16_t int16x8_t;
typedef __attribute__((invalid_vector_type(2))) int32_t int32x2_t;
typedef __attribute__((invalid_vector_type(4))) int32_t int32x4_t;
typedef __attribute__((invalid_vector_type(1))) int64_t int64x1_t;
typedef __attribute__((invalid_vector_type(2))) int64_t int64x2_t;
typedef __attribute__((invalid_vector_type(8))) uint8_t uint8x8_t;
typedef __attribute__((invalid_vector_type(16))) uint8_t uint8x16_t;
typedef __attribute__((invalid_vector_type(4))) uint16_t uint16x4_t;
typedef __attribute__((invalid_vector_type(8))) uint16_t uint16x8_t;
typedef __attribute__((invalid_vector_type(2))) uint32_t uint32x2_t;
typedef __attribute__((invalid_vector_type(4))) uint32_t uint32x4_t;
typedef __attribute__((invalid_vector_type(1))) uint64_t uint64x1_t;
typedef __attribute__((invalid_vector_type(2))) uint64_t uint64x2_t;
typedef __attribute__((invalid_vector_type(4))) float16_t float16x4_t;
typedef __attribute__((invalid_vector_type(8))) float16_t float16x8_t;
typedef __attribute__((invalid_vector_type(2))) float32_t float32x2_t;
typedef __attribute__((invalid_vector_type(4))) float32_t float32x4_t;
typedef __attribute__((invalid_vector_type(1))) float64_t float64x1_t;
typedef __attribute__((invalid_vector_type(2))) float64_t float64x2_t;
typedef __attribute__((invalid_vector_type(8))) poly8_t poly8x8_t;
typedef __attribute__((invalid_vector_type(16))) poly8_t poly8x16_t;
typedef __attribute__((invalid_vector_type(4))) poly16_t poly16x4_t;
typedef __attribute__((invalid_vector_type(8))) poly16_t poly16x8_t;
typedef __attribute__((invalid_vector_type(1))) poly64_t poly64x1_t;
typedef __attribute__((invalid_vector_type(2))) poly64_t poly64x2_t;
# 36 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3



# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 1 3
# 58 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef long long vector_s64
    __attribute__((ext_vector_type(32), ));
typedef int vector_s32
    __attribute__((ext_vector_type(64), ));
typedef short vector_s16
    __attribute__((ext_vector_type(128), ));
typedef char vector_s8
    __attribute__((ext_vector_type(256), ));
typedef vector_s8 vector_s4x2;

typedef unsigned long long vector_u64
    __attribute__((ext_vector_type(32), ));
typedef unsigned int vector_u32
    __attribute__((ext_vector_type(64), ));
typedef unsigned short vector_u16
    __attribute__((ext_vector_type(128), ));
typedef unsigned char vector_u8
    __attribute__((ext_vector_type(256), ));
typedef float vector_f32
    __attribute__((ext_vector_type(64), ));
typedef half vector_f16
    __attribute__((ext_vector_type(128), ));




typedef bfloat16_t vector_bf16 __attribute__((
    ext_vector_type(128), ));



typedef long long vector_bool
    __attribute__((ext_vector_type(4), ));


typedef char vector_align_data __attribute__((ext_vector_type(32)));





typedef struct vector_align {
  [aicore] __attribute__((cce_builtin_api, always_inline)) vector_align() {
    Data = __builtin_cce_init_vector_align_data();
  }

  [aicore] __attribute__((cce_builtin_api, always_inline))
  operator vector_align_data &() {
    return Data;
  }

  [aicore] __attribute__((cce_builtin_api, always_inline)) vector_align &
  operator=(vector_align_data alignData) {
    Data = alignData;
    return *this;
  }
  vector_align_data Data;
} vector_align;

typedef uint32_t vector_address __attribute__((ext_vector_type(1)));




typedef int wvector_s24
    __attribute__((ext_vector_type(256), ));
typedef long long wvector_s48
    __attribute__((ext_vector_type(128), ));
typedef signed __int128 wvector_s64
    __attribute__((ext_vector_type(64), ));






typedef struct vector_s64x2_t {
  vector_s64 val[2];
} vector_s64x2_t;

typedef struct vector_u64x2_t {
  vector_u64 val[2];
} vector_u64x2_t;

typedef struct vector_s32x2_t {
  vector_s32 val[2];
} vector_s32x2_t;

typedef struct vector_u32x2_t {
  vector_u32 val[2];
} vector_u32x2_t;

typedef struct vector_s16x2_t {
  vector_s16 val[2];
} vector_s16x2_t;

typedef struct vector_u16x2_t {
  vector_u16 val[2];
} vector_u16x2_t;

typedef struct vector_s8x2_t {
  vector_s8 val[2];
} vector_s8x2_t;

typedef struct vector_u8x2_t {
  vector_u8 val[2];
} vector_u8x2_t;

typedef struct vector_f32x2_t {
  vector_f32 val[2];
} vector_f32x2_t;

typedef struct vector_f16x2_t {
  vector_f16 val[2];
} vector_f16x2_t;

typedef struct vector_boolx2_t {
  vector_bool val[2];
} vector_boolx2_t;
# 40 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3
# 85 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
extern "C" {

inline __attribute__((alway_inline)) int __cce_getOrSetBlockNum(int value,
                                                                int type) {
  static thread_local int local = 0;
  if (type == 0)
    local = value;
  return local;
}

}

inline __attribute__((alway_inline)) unsigned int
__cce_rtConfigureCall(unsigned int numBlocks, void *smDesc = nullptr,
                      void *stream = nullptr) {
  __cce_getOrSetBlockNum(numBlocks, 0);
  return rtConfigureCall(numBlocks, smDesc, stream);
}
# 2 "<built-in>" 2
# 1 "/usr/include/stdio.h" 1 3 4
# 27 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/bits/libc-header-start.h" 1 3 4
# 28 "/usr/include/stdio.h" 2 3 4

extern "C" {



# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stddef.h" 1 3 4
# 34 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdarg.h" 1 3 4
# 14 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdarg.h" 3 4
typedef __builtin_va_list va_list;
# 32 "/usr/local/Ascend/ascend-toolkit/8.0.RC3/aarch64-linux/ccec_compiler/lib/clang/15.0.5/include/stdarg.h" 3 4
typedef __builtin_va_list __gnuc_va_list;
# 37 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/include/bits/types/__fpos_t.h" 1 3 4




# 1 "/usr/include/bits/types/__mbstate_t.h" 1 3 4
# 13 "/usr/include/bits/types/__mbstate_t.h" 3 4
typedef struct
{
  int __count;
  union
  {
    int __wch;
    char __wchb[4];
  } __value;
} __mbstate_t;
# 6 "/usr/include/bits/types/__fpos_t.h" 2 3 4




typedef struct _G_fpos_t
{
  __off_t __pos;
  __mbstate_t __state;
} __fpos_t;
# 40 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/bits/types/__fpos64_t.h" 1 3 4
# 10 "/usr/include/bits/types/__fpos64_t.h" 3 4
typedef struct _G_fpos64_t
{
  __off64_t __pos;
  __mbstate_t __state;
} __fpos64_t;
# 41 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/bits/types/__FILE.h" 1 3 4



struct _IO_FILE;
typedef struct _IO_FILE __FILE;
# 42 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/bits/types/FILE.h" 1 3 4



struct _IO_FILE;


typedef struct _IO_FILE FILE;
# 43 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/bits/types/struct_FILE.h" 1 3 4
# 35 "/usr/include/bits/types/struct_FILE.h" 3 4
struct _IO_FILE;
struct _IO_marker;
struct _IO_codecvt;
struct _IO_wide_data;




typedef void _IO_lock_t;





struct _IO_FILE
{
  int _flags;


  char *_IO_read_ptr;
  char *_IO_read_end;
  char *_IO_read_base;
  char *_IO_write_base;
  char *_IO_write_ptr;
  char *_IO_write_end;
  char *_IO_buf_base;
  char *_IO_buf_end;


  char *_IO_save_base;
  char *_IO_backup_base;
  char *_IO_save_end;

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;
  int _flags2;
  __off_t _old_offset;


  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];

  _IO_lock_t *_lock;







  __off64_t _offset;

  struct _IO_codecvt *_codecvt;
  struct _IO_wide_data *_wide_data;
  struct _IO_FILE *_freeres_list;
  void *_freeres_buf;
  size_t __pad5;
  int _mode;

  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];
};
# 44 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/include/bits/types/cookie_io_functions_t.h" 1 3 4
# 27 "/usr/include/bits/types/cookie_io_functions_t.h" 3 4
typedef __ssize_t cookie_read_function_t (void *__cookie, char *__buf,
                                          size_t __nbytes);







typedef __ssize_t cookie_write_function_t (void *__cookie, const char *__buf,
                                           size_t __nbytes);







typedef int cookie_seek_function_t (void *__cookie, __off64_t *__pos, int __w);


typedef int cookie_close_function_t (void *__cookie);






typedef struct _IO_cookie_io_functions_t
{
  cookie_read_function_t *read;
  cookie_write_function_t *write;
  cookie_seek_function_t *seek;
  cookie_close_function_t *close;
} cookie_io_functions_t;
# 47 "/usr/include/stdio.h" 2 3 4





typedef __gnuc_va_list va_list;
# 63 "/usr/include/stdio.h" 3 4
typedef __off_t off_t;






typedef __off64_t off64_t;






typedef __ssize_t ssize_t;






typedef __fpos_t fpos_t;




typedef __fpos64_t fpos64_t;
# 128 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/bits/stdio_lim.h" 1 3 4
# 129 "/usr/include/stdio.h" 2 3 4
# 148 "/usr/include/stdio.h" 3 4
extern FILE *stdin;
extern FILE *stdout;
extern FILE *stderr;






extern int remove (const char *__filename) noexcept (true);

extern int rename (const char *__old, const char *__new) noexcept (true);



extern int renameat (int __oldfd, const char *__old, int __newfd,
       const char *__new) noexcept (true);
# 175 "/usr/include/stdio.h" 3 4
extern int renameat2 (int __oldfd, const char *__old, int __newfd,
        const char *__new, unsigned int __flags) noexcept (true);






extern int fclose (FILE *__stream) __attribute__ ((__nonnull__ (1)));
# 193 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile (void)
  __attribute__ ((__malloc__)) ;
# 205 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile64 (void)
   __attribute__ ((__malloc__)) ;



extern char *tmpnam (char[20]) noexcept (true) ;




extern char *tmpnam_r (char __s[20]) noexcept (true) ;
# 227 "/usr/include/stdio.h" 3 4
extern char *tempnam (const char *__dir, const char *__pfx)
   noexcept (true) __attribute__ ((__malloc__)) ;






extern int fflush (FILE *__stream);
# 244 "/usr/include/stdio.h" 3 4
extern int fflush_unlocked (FILE *__stream);
# 254 "/usr/include/stdio.h" 3 4
extern int fcloseall (void);
# 263 "/usr/include/stdio.h" 3 4
extern FILE *fopen (const char *__restrict __filename,
      const char *__restrict __modes)
  __attribute__ ((__malloc__)) ;




extern FILE *freopen (const char *__restrict __filename,
        const char *__restrict __modes,
        FILE *__restrict __stream) __attribute__ ((__nonnull__ (3)));
# 288 "/usr/include/stdio.h" 3 4
extern FILE *fopen64 (const char *__restrict __filename,
        const char *__restrict __modes)
  __attribute__ ((__malloc__)) ;
extern FILE *freopen64 (const char *__restrict __filename,
   const char *__restrict __modes,
   FILE *__restrict __stream) __attribute__ ((__nonnull__ (3)));




extern FILE *fdopen (int __fd, const char *__modes) noexcept (true)
  __attribute__ ((__malloc__)) ;





extern FILE *fopencookie (void *__restrict __magic_cookie,
     const char *__restrict __modes,
     cookie_io_functions_t __io_funcs) noexcept (true)
  __attribute__ ((__malloc__)) ;




extern FILE *fmemopen (void *__s, size_t __len, const char *__modes)
  noexcept (true) __attribute__ ((__malloc__)) ;




extern FILE *open_memstream (char **__bufloc, size_t *__sizeloc) noexcept (true)
  __attribute__ ((__malloc__)) ;
# 333 "/usr/include/stdio.h" 3 4
extern void setbuf (FILE *__restrict __stream, char *__restrict __buf) noexcept (true);



extern int setvbuf (FILE *__restrict __stream, char *__restrict __buf,
      int __modes, size_t __n) noexcept (true);




extern void setbuffer (FILE *__restrict __stream, char *__restrict __buf,
         size_t __size) noexcept (true);


extern void setlinebuf (FILE *__stream) noexcept (true);







extern int fprintf (FILE *__restrict __stream,
      const char *__restrict __format, ...);




extern int printf (const char *__restrict __format, ...);

extern int sprintf (char *__restrict __s,
      const char *__restrict __format, ...) noexcept (true);





extern int vfprintf (FILE *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg);




extern int vprintf (const char *__restrict __format, __gnuc_va_list __arg);

extern int vsprintf (char *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg) noexcept (true);



extern int snprintf (char *__restrict __s, size_t __maxlen,
       const char *__restrict __format, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 3, 4)));

extern int vsnprintf (char *__restrict __s, size_t __maxlen,
        const char *__restrict __format, __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__printf__, 3, 0)));





extern int vasprintf (char **__restrict __ptr, const char *__restrict __f,
        __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 0))) ;
extern int __asprintf (char **__restrict __ptr,
         const char *__restrict __fmt, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3))) ;
extern int asprintf (char **__restrict __ptr,
       const char *__restrict __fmt, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3))) ;




extern int vdprintf (int __fd, const char *__restrict __fmt,
       __gnuc_va_list __arg)
     __attribute__ ((__format__ (__printf__, 2, 0)));
extern int dprintf (int __fd, const char *__restrict __fmt, ...)
     __attribute__ ((__format__ (__printf__, 2, 3)));







extern int fscanf (FILE *__restrict __stream,
     const char *__restrict __format, ...) ;




extern int scanf (const char *__restrict __format, ...) ;

extern int sscanf (const char *__restrict __s,
     const char *__restrict __format, ...) noexcept (true);





# 1 "/usr/include/bits/floatn.h" 1 3 4
# 23 "/usr/include/bits/floatn.h" 3 4
# 1 "/usr/include/bits/long-double.h" 1 3 4
# 24 "/usr/include/bits/floatn.h" 2 3 4
# 80 "/usr/include/bits/floatn.h" 3 4
typedef long double _Float128;
# 95 "/usr/include/bits/floatn.h" 3 4
# 1 "/usr/include/bits/floatn-common.h" 1 3 4
# 24 "/usr/include/bits/floatn-common.h" 3 4
# 1 "/usr/include/bits/long-double.h" 1 3 4
# 25 "/usr/include/bits/floatn-common.h" 2 3 4
# 214 "/usr/include/bits/floatn-common.h" 3 4
typedef float _Float32;
# 251 "/usr/include/bits/floatn-common.h" 3 4
typedef double _Float64;
# 268 "/usr/include/bits/floatn-common.h" 3 4
typedef double _Float32x;
# 285 "/usr/include/bits/floatn-common.h" 3 4
typedef long double _Float64x;
# 96 "/usr/include/bits/floatn.h" 2 3 4
# 436 "/usr/include/stdio.h" 2 3 4




extern int fscanf (FILE *__restrict __stream, const char *__restrict __format, ...) __asm__ ("" "__isoc23_fscanf") ;


extern int scanf (const char *__restrict __format, ...) __asm__ ("" "__isoc23_scanf") ;

extern int sscanf (const char *__restrict __s, const char *__restrict __format, ...) noexcept (true) __asm__ ("" "__isoc23_sscanf");
# 486 "/usr/include/stdio.h" 3 4
extern int vfscanf (FILE *__restrict __s, const char *__restrict __format,
      __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 2, 0))) ;





extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 1, 0))) ;


extern int vsscanf (const char *__restrict __s,
      const char *__restrict __format, __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__scanf__, 2, 0)));






extern int vfscanf (FILE *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc23_vfscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0))) ;
extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc23_vscanf")

     __attribute__ ((__format__ (__scanf__, 1, 0))) ;
extern int vsscanf (const char *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) noexcept (true) __asm__ ("" "__isoc23_vsscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0)));
# 571 "/usr/include/stdio.h" 3 4
extern int fgetc (FILE *__stream);
extern int getc (FILE *__stream);





extern int getchar (void);






extern int getc_unlocked (FILE *__stream);
extern int getchar_unlocked (void);
# 596 "/usr/include/stdio.h" 3 4
extern int fgetc_unlocked (FILE *__stream);
# 607 "/usr/include/stdio.h" 3 4
extern int fputc (int __c, FILE *__stream);
extern int putc (int __c, FILE *__stream);





extern int putchar (int __c);
# 623 "/usr/include/stdio.h" 3 4
extern int fputc_unlocked (int __c, FILE *__stream);







extern int putc_unlocked (int __c, FILE *__stream);
extern int putchar_unlocked (int __c);






extern int getw (FILE *__stream);


extern int putw (int __w, FILE *__stream);







extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
                                                         ;
# 673 "/usr/include/stdio.h" 3 4
extern char *fgets_unlocked (char *__restrict __s, int __n,
        FILE *__restrict __stream)
                                                  ;
# 690 "/usr/include/stdio.h" 3 4
extern __ssize_t __getdelim (char **__restrict __lineptr,
                             size_t *__restrict __n, int __delimiter,
                             FILE *__restrict __stream) ;
extern __ssize_t getdelim (char **__restrict __lineptr,
                           size_t *__restrict __n, int __delimiter,
                           FILE *__restrict __stream) ;







extern __ssize_t getline (char **__restrict __lineptr,
                          size_t *__restrict __n,
                          FILE *__restrict __stream) ;







extern int fputs (const char *__restrict __s, FILE *__restrict __stream);





extern int puts (const char *__s);






extern int ungetc (int __c, FILE *__stream);






extern size_t fread (void *__restrict __ptr, size_t __size,
       size_t __n, FILE *__restrict __stream) ;




extern size_t fwrite (const void *__restrict __ptr, size_t __size,
        size_t __n, FILE *__restrict __s);
# 749 "/usr/include/stdio.h" 3 4
extern int fputs_unlocked (const char *__restrict __s,
      FILE *__restrict __stream);
# 760 "/usr/include/stdio.h" 3 4
extern size_t fread_unlocked (void *__restrict __ptr, size_t __size,
         size_t __n, FILE *__restrict __stream) ;
extern size_t fwrite_unlocked (const void *__restrict __ptr, size_t __size,
          size_t __n, FILE *__restrict __stream);







extern int fseek (FILE *__stream, long int __off, int __whence);




extern long int ftell (FILE *__stream) ;




extern void rewind (FILE *__stream);
# 794 "/usr/include/stdio.h" 3 4
extern int fseeko (FILE *__stream, __off_t __off, int __whence);




extern __off_t ftello (FILE *__stream) ;
# 818 "/usr/include/stdio.h" 3 4
extern int fgetpos (FILE *__restrict __stream, fpos_t *__restrict __pos);




extern int fsetpos (FILE *__stream, const fpos_t *__pos);
# 837 "/usr/include/stdio.h" 3 4
extern int fseeko64 (FILE *__stream, __off64_t __off, int __whence);
extern __off64_t ftello64 (FILE *__stream) ;
extern int fgetpos64 (FILE *__restrict __stream, fpos64_t *__restrict __pos);
extern int fsetpos64 (FILE *__stream, const fpos64_t *__pos);



extern void clearerr (FILE *__stream) noexcept (true);

extern int feof (FILE *__stream) noexcept (true) ;

extern int ferror (FILE *__stream) noexcept (true) ;



extern void clearerr_unlocked (FILE *__stream) noexcept (true);
extern int feof_unlocked (FILE *__stream) noexcept (true) ;
extern int ferror_unlocked (FILE *__stream) noexcept (true) ;







extern void perror (const char *__s) __attribute__ ((__cold__));




extern int fileno (FILE *__stream) noexcept (true) ;




extern int fileno_unlocked (FILE *__stream) noexcept (true) ;
# 881 "/usr/include/stdio.h" 3 4
extern int pclose (FILE *__stream);





extern FILE *popen (const char *__command, const char *__modes)
  __attribute__ ((__malloc__)) ;






extern char *ctermid (char *__s) noexcept (true)
                                     ;





extern char *cuserid (char *__s)
                                     ;




struct obstack;


extern int obstack_printf (struct obstack *__restrict __obstack,
      const char *__restrict __format, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3)));
extern int obstack_vprintf (struct obstack *__restrict __obstack,
       const char *__restrict __format,
       __gnuc_va_list __args)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 0)));







extern void flockfile (FILE *__stream) noexcept (true);



extern int ftrylockfile (FILE *__stream) noexcept (true) ;


extern void funlockfile (FILE *__stream) noexcept (true);
# 943 "/usr/include/stdio.h" 3 4
extern int __uflow (FILE *);
extern int __overflow (FILE *, int);
# 960 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/bits/stdio.h" 1 3 4
# 38 "/usr/include/bits/stdio.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) int
vprintf (const char *__restrict __fmt, __gnuc_va_list __arg)
{
  return vfprintf (stdout, __fmt, __arg);
}



extern __inline __attribute__ ((__gnu_inline__)) int
getchar (void)
{
  return getc (stdin);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fgetc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}





extern __inline __attribute__ ((__gnu_inline__)) int
getc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}


extern __inline __attribute__ ((__gnu_inline__)) int
getchar_unlocked (void)
{
  return (__builtin_expect (((stdin)->_IO_read_ptr >= (stdin)->_IO_read_end), 0) ? __uflow (stdin) : *(unsigned char *) (stdin)->_IO_read_ptr++);
}




extern __inline __attribute__ ((__gnu_inline__)) int
putchar (int __c)
{
  return putc (__c, stdout);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fputc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) int
putc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}


extern __inline __attribute__ ((__gnu_inline__)) int
putchar_unlocked (int __c)
{
  return (__builtin_expect (((stdout)->_IO_write_ptr >= (stdout)->_IO_write_end), 0) ? __overflow (stdout, (unsigned char) (__c)) : (unsigned char) (*(stdout)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) __ssize_t
getline (char **__lineptr, size_t *__n, FILE *__stream)
{
  return __getdelim (__lineptr, __n, '\n', __stream);
}





extern __inline __attribute__ ((__gnu_inline__)) int
 feof_unlocked (FILE *__stream) noexcept (true)
{
  return (((__stream)->_flags & 0x0010) != 0);
}


extern __inline __attribute__ ((__gnu_inline__)) int
 ferror_unlocked (FILE *__stream) noexcept (true)
{
  return (((__stream)->_flags & 0x0020) != 0);
}
# 961 "/usr/include/stdio.h" 2 3 4






}
# 3 "<built-in>" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/../include/version/cann_version.h" 1
# 4 "<built-in>" 2
# 1 "/home/user3/npu-nvme/hello_world.cpp" 2
# 10 "/home/user3/npu-nvme/hello_world.cpp"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h"
# 1 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/cstdint" 1 3
# 33 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/cstdint" 3





# 1 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 1 3
# 262 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 3
namespace std
{
  typedef long unsigned int size_t;
  typedef long int ptrdiff_t;


  typedef decltype(nullptr) nullptr_t;

}
# 284 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 3
namespace std
{
  inline namespace __cxx11 __attribute__((__abi_tag__ ("cxx11"))) { }
}
namespace __gnu_cxx
{
  inline namespace __cxx11 __attribute__((__abi_tag__ ("cxx11"))) { }
}
# 522 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 3
# 1 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/os_defines.h" 1 3
# 523 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 2 3


# 1 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/cpu_defines.h" 1 3
# 526 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 2 3
# 692 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 3
# 1 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/pstl/pstl_config.h" 1 3
# 693 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/aarch64-linux-gnu/bits/c++config.h" 2 3
# 39 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/cstdint" 2 3





namespace std
{

  using ::int8_t;
  using ::int16_t;
  using ::int32_t;
  using ::int64_t;

  using ::int_fast8_t;
  using ::int_fast16_t;
  using ::int_fast32_t;
  using ::int_fast64_t;

  using ::int_least8_t;
  using ::int_least16_t;
  using ::int_least32_t;
  using ::int_least64_t;

  using ::intmax_t;
  using ::intptr_t;

  using ::uint8_t;
  using ::uint16_t;
  using ::uint32_t;
  using ::uint64_t;

  using ::uint_fast8_t;
  using ::uint_fast16_t;
  using ::uint_fast32_t;
  using ::uint_fast64_t;

  using ::uint_least8_t;
  using ::uint_least16_t;
  using ::uint_least32_t;
  using ::uint_least64_t;

  using ::uintmax_t;
  using ::uintptr_t;





}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h" 2
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_macros.h"
namespace AscendC {

constexpr int32_t QUE_MAX_EVENT = 8;



constexpr int32_t HF32_MODE_BIT = 46;
constexpr int32_t HF32_TRANS_MODE_BIT = 47;
constexpr int32_t MM_LAYOUT_MODE_BIT = 51;
constexpr int32_t LEAKY_RELU_MODE_BIT = 50;
constexpr int32_t CAST_MODE_BIT = 59;
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_log.h" 1
# 218 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_log.h"
namespace AscendC {
template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void AssertImpl(__attribute__((cce_global)) const char* fmt, Args&&... args);
# 277 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_log.h"
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
namespace AscendC {
enum class TPosition : uint8_t {
    GM,
    A1,
    A2,
    B1,
    B2,
    C1,
    C2,
    CO1,
    CO2,
    VECIN,
    VECOUT,
    VECCALC,
    LCM = VECCALC,
    SPM,
    SHM = SPM,
    TSCM,
    C2PIPE2GM,
    C2PIPE2LOCAL,
    MAX,
};

using QuePosition = TPosition;
enum class Hardware : uint8_t { GM, UB, L1, L0A, L0B, L0C, BIAS, FIXBUF, MAX };

enum class HardEvent : uint8_t {

    MTE2_MTE1,
    MTE1_MTE2,
    MTE1_M,
    M_MTE1,
    MTE2_V,
    V_MTE2,
    MTE3_V,
    V_MTE3,
    M_V,
    V_M,
    V_V,
    MTE3_MTE1,
    MTE1_MTE3,
    MTE1_V,
    MTE2_M,
    M_MTE2,
    V_MTE1,
    M_FIX,
    FIX_M,
    MTE3_MTE2,
    MTE2_MTE3,
    S_V,
    V_S,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MTE2_FIX,
    FIX_MTE2,
    FIX_S,
    M_S,
    FIX_MTE3,
    MTE1_FIX,
    FIX_MTE1,
    FIX_FIX,
    MAX,
};

enum class HardEventAic : uint8_t {

    MTE2_MTE1,
    MTE1_MTE2,
    MTE1_M,
    M_MTE1,
    MTE3_MTE1,
    MTE1_MTE3,
    MTE2_M,
    M_MTE2,
    M_FIX,
    FIX_M,
    MTE3_MTE2,
    MTE2_MTE3,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MTE2_FIX,
    FIX_MTE2,
    FIX_S,
    M_S,
    FIX_MTE3,
    MTE1_FIX,
    FIX_MTE1,
    FIX_FIX,
    MAX,
};

enum class HardEventAiv : uint8_t {

    MTE2_V,
    V_MTE2,
    MTE3_V,
    V_MTE3,
    V_V,
    MTE3_MTE2,
    MTE2_MTE3,
    S_V,
    V_S,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MAX,
};

enum class MemoryT : uint8_t { L1 = 0, L0A, L0B, L0C, UB, BIAS };

enum class MemDsbT : uint8_t { ALL = 0, DDR, UB, SEQ };







constexpr uint8_t EVENT_NUM = static_cast<uint8_t>(HardEventAiv::MAX);



[aicore] constexpr uint8_t EventToIndexAic(HardEvent evt)
{


    if (evt == HardEvent::MTE2_MTE1) {
        return static_cast<uint8_t>(HardEventAic::MTE2_MTE1);
    } else if (evt == HardEvent::MTE1_MTE2) {
        return static_cast<uint8_t>(HardEventAic::MTE1_MTE2);
    } else if (evt == HardEvent::MTE1_M) {
        return static_cast<uint8_t>(HardEventAic::MTE1_M);
    } else if (evt == HardEvent::M_MTE1) {
        return static_cast<uint8_t>(HardEventAic::M_MTE1);
    } else if (evt == HardEvent::MTE3_MTE1) {
        return static_cast<uint8_t>(HardEventAic::MTE3_MTE1);
    } else if (evt == HardEvent::MTE1_MTE3) {
        return static_cast<uint8_t>(HardEventAic::MTE1_MTE3);
    } else if (evt == HardEvent::MTE2_M) {
        return static_cast<uint8_t>(HardEventAic::MTE2_M);
    } else if (evt == HardEvent::M_MTE2) {
        return static_cast<uint8_t>(HardEventAic::M_MTE2);
    } else if (evt == HardEvent::M_FIX) {
        return static_cast<uint8_t>(HardEventAic::M_FIX);
    } else if (evt == HardEvent::FIX_M) {
        return static_cast<uint8_t>(HardEventAic::FIX_M);
    } else if (evt == HardEvent::MTE3_MTE2) {
        return static_cast<uint8_t>(HardEventAic::MTE3_MTE2);
    } else if (evt == HardEvent::MTE2_MTE3) {
        return static_cast<uint8_t>(HardEventAic::MTE2_MTE3);
    } else if (evt == HardEvent::S_MTE2) {
        return static_cast<uint8_t>(HardEventAic::S_MTE2);
    } else if (evt == HardEvent::MTE2_S) {
        return static_cast<uint8_t>(HardEventAic::MTE2_S);
    } else if (evt == HardEvent::S_MTE3) {
        return static_cast<uint8_t>(HardEventAic::S_MTE3);
    } else if (evt == HardEvent::MTE3_S) {
        return static_cast<uint8_t>(HardEventAic::MTE3_S);
    } else if (evt == HardEvent::MTE2_FIX) {
        return static_cast<uint8_t>(HardEventAic::MTE2_FIX);
    } else if (evt == HardEvent::FIX_MTE2) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE2);
    } else if (evt == HardEvent::FIX_S) {
        return static_cast<uint8_t>(HardEventAic::FIX_S);
    } else if (evt == HardEvent::M_S) {
        return static_cast<uint8_t>(HardEventAic::M_S);
    } else if (evt == HardEvent::FIX_MTE3) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE3);
    } else if (evt == HardEvent::FIX_MTE1) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE1);
    } else if (evt == HardEvent::MTE1_FIX) {
        return static_cast<uint8_t>(HardEventAic::MTE1_FIX);
    } else if (evt == HardEvent::FIX_FIX) {
        return static_cast<uint8_t>(HardEventAic::FIX_FIX);
    } else {
        return static_cast<uint8_t>(HardEventAic::MAX);
    }
}

[aicore] constexpr uint8_t EventToIndexAiv(HardEvent evt)
{


    if (evt == HardEvent::MTE2_V) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_V);
    } else if (evt == HardEvent::V_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::V_MTE2);
    } else if (evt == HardEvent::MTE3_V) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_V);
    } else if (evt == HardEvent::V_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::V_MTE3);
    } else if (evt == HardEvent::V_V) {
        return static_cast<uint8_t>(HardEventAiv::V_V);
    } else if (evt == HardEvent::MTE3_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_MTE2);
    } else if (evt == HardEvent::MTE2_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_MTE3);
    } else if (evt == HardEvent::S_V) {
        return static_cast<uint8_t>(HardEventAiv::S_V);
    } else if (evt == HardEvent::V_S) {
        return static_cast<uint8_t>(HardEventAiv::V_S);
    } else if (evt == HardEvent::S_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::S_MTE2);
    } else if (evt == HardEvent::MTE2_S) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_S);
    } else if (evt == HardEvent::S_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::S_MTE3);
    } else if (evt == HardEvent::MTE3_S) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_S);
    } else {
        return static_cast<uint8_t>(HardEventAiv::MAX);
    }
}

[aicore] constexpr uint8_t EventToIndex(HardEvent evt)
{





    return EventToIndexAiv(evt);

}





constexpr int32_t PIPE_NUM = 7;
constexpr pipe_t SUPPORTED_PIPE[PIPE_NUM] = { PIPE_S, PIPE_V, PIPE_M, PIPE_MTE1, PIPE_MTE2, PIPE_MTE3, PIPE_FIX };


[aicore] constexpr bool IsSupportedPipe(pipe_t pipe)
{
    for (int i = 0; i < PIPE_NUM; i++) {
        if (pipe == SUPPORTED_PIPE[i]) {
            return true;
        }
    }
    return false;
}

[aicore] constexpr Hardware GetPhyType(TPosition pos)
{
                                 ;
    Hardware hard = Hardware::UB;
    if (pos == TPosition::GM) {
        hard = Hardware::GM;
    } else if (pos == TPosition::A1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::A2) {
        hard = Hardware::L0A;
    } else if (pos == TPosition::B1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::B2) {
        hard = Hardware::L0B;
# 302 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
    } else if (pos == TPosition::C1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::C2) {
        hard = Hardware::BIAS;
    } else if (pos == TPosition::CO2) {
        hard = Hardware::GM;
    } else if (pos == TPosition::C2PIPE2GM) {
        hard = Hardware::FIXBUF;






    } else if (pos == TPosition::CO1) {
        hard = Hardware::L0C;
    } else if (pos == TPosition::SHM) {
        hard = Hardware::L1;
    } else if (pos == TPosition::TSCM) {
        hard = Hardware::L1;
    }
    return hard;
}

[aicore] constexpr TPosition GetPosition(TPosition srcPos, TPosition dstPos)
{

                                                                       ;
                                                                          ;





    if ((dstPos == TPosition::GM) || (dstPos == TPosition::CO2)) {
        return srcPos;
    }

    return dstPos;
}

[aicore] constexpr Hardware GetBufferPos(TPosition srcPos, TPosition dstPos)
{

                                                                       ;
                                                                          ;





    if ((dstPos == TPosition::GM) || (dstPos == TPosition::CO2)) {
        return GetPhyType(srcPos);
    }

    return GetPhyType(dstPos);
}

[aicore] constexpr TPosition GetBufferLogicPos(TPosition pos, bool isSrc)
{
                                ;
                                     ;
                                 ;
    if (pos == TPosition::A1) {
        return isSrc ? TPosition::GM : TPosition::A1;
    } else if (pos == TPosition::B1) {
        return isSrc ? TPosition::GM : TPosition::B1;
    } else if (pos == TPosition::C1) {
        return isSrc ? TPosition::GM : TPosition::C1;
    } else if (pos == TPosition::A2) {
        return isSrc ? TPosition::A1 : TPosition::A2;
    } else if (pos == TPosition::B2) {
        return isSrc ? TPosition::B1 : TPosition::B2;
    } else if (pos == TPosition::C2) {
        return isSrc ? TPosition::C1 : TPosition::C2;
    } else if (pos == TPosition::CO1) {
        return isSrc ? TPosition::CO1 : TPosition::CO2;
    } else if (pos == TPosition::CO2) {
        return isSrc ? TPosition::CO2 : TPosition::GM;
    } else if (pos == TPosition::VECIN) {
        return isSrc ? TPosition::GM : TPosition::VECIN;
    } else if (pos == TPosition::VECOUT) {
        return isSrc ? TPosition::VECOUT : TPosition::GM;
    } else if (pos == TPosition::SPM) {
        return isSrc ? TPosition::VECOUT : TPosition::GM;
    } else if (pos == TPosition::C2PIPE2GM) {
        return isSrc ? TPosition::B1 : TPosition::C2PIPE2GM;
    }
    return TPosition::MAX;
}

[aicore] constexpr HardEvent GetQueEvt(Hardware src, Hardware dst, bool fwdDirect, bool nd2nz = false,
                                         bool nz2nd = false)
{
    (void)(nz2nd);

                                                            ;
                                ;
                                ;
    if (src == Hardware::GM) {
                                   ;
                                    ;
                                     ;
                                       ;
        if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE2_V : HardEvent::V_MTE2;
        } else if (dst == Hardware::L1) {






            (void)(nd2nz);

            return fwdDirect ? HardEvent::MTE2_MTE1 : HardEvent::MTE1_MTE2;
        } else if (dst == Hardware::L0A) {
            return fwdDirect ? HardEvent::MTE2_M : HardEvent::M_MTE2;
        } else if (dst == Hardware::L0B) {
            return fwdDirect ? HardEvent::MTE2_M : HardEvent::M_MTE2;
        }
    } else if (src == Hardware::UB) {
                                    ;
                                    ;
                                     ;
                                       ;
        if (dst == Hardware::GM) {
            return fwdDirect ? HardEvent::V_MTE3 : HardEvent::MTE3_V;
        } else if (dst == Hardware::L1) {
            return fwdDirect ? HardEvent::MTE3_MTE1 : HardEvent::MTE1_MTE3;
        } else if (dst == Hardware::L0C) {
            return fwdDirect ? HardEvent::V_V : HardEvent::MAX;
        } else if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE2_MTE3 : HardEvent::MTE3_MTE2;
        }
    } else if (src == Hardware::L1) {
                                   ;
                                   ;
                                    ;




        if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE1_V : HardEvent::V_MTE1;
        } else if (dst == Hardware::L0A) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        } else if (dst == Hardware::L0B) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        } else if (dst == Hardware::FIXBUF) {
            return fwdDirect ? HardEvent::MTE1_FIX : HardEvent::FIX_MTE1;
        } else if (dst == Hardware::BIAS) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        }
    } else if (src == Hardware::L0A) {
                                    ;
        return fwdDirect ? HardEvent::M_V : HardEvent::V_M;
    } else if (src == Hardware::L0B) {
                                    ;
        return fwdDirect ? HardEvent::M_V : HardEvent::V_M;






    } else if (src == Hardware::L0C) {
                                   ;
        return fwdDirect ? HardEvent::M_FIX : HardEvent::FIX_M;
    }
# 485 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
    return HardEvent::MAX;
}


template <MemDsbT arg>
[aicore] __inline__ __attribute__((always_inline)) void DataSyncBarrierImpl()
{
    dsb((mem_dsb_t)arg);
}

template <HardEvent event, MemoryT memT, bool isVirtual>
[aicore] __inline__ __attribute__((always_inline)) void HSetFlagImpl(int32_t eventID)
{

                                                                                                                  ;
    static_assert(((int32_t)memT >= 0 && memT <= MemoryT::BIAS && memT != MemoryT::UB && memT != MemoryT::L1),
                  "memT only support L0A, L0B, L0C, BIAS.");

    event_t e = static_cast<event_t>(eventID);

    switch (event) {
        case HardEvent::MTE1_M:

                                                                         ;
            hset_flag(PIPE_MTE1, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_MTE1:

                                                                         ;
            hset_flag(PIPE_M, PIPE_MTE1, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_FIX:
                                                                                     ;
            hset_flag(PIPE_M, PIPE_FIX, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::FIX_M:
                                                                                     ;
            hset_flag(PIPE_FIX, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        default:
                                                                                                           ;
            break;
    }
}

template <HardEvent event, MemoryT memT, bool isVirtual>
[aicore] __inline__ __attribute__((always_inline)) void HWaitFlagImpl(int32_t eventID)
{

                                                                                                                  ;
    static_assert(((int32_t)memT >= 0 && memT <= MemoryT::BIAS && memT != MemoryT::UB && memT != MemoryT::L1),
                  "memT only support L0A, L0B, L0C, BIAS.");

    event_t e = static_cast<event_t>(eventID);

    switch (event) {
        case HardEvent::MTE1_M:

                                                                         ;
            hwait_flag(PIPE_MTE1, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_MTE1:

                                                                         ;
            hwait_flag(PIPE_M, PIPE_MTE1, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_FIX:
                                                                                     ;
            hwait_flag(PIPE_M, PIPE_FIX, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::FIX_M:
                                                                                     ;
            hwait_flag(PIPE_FIX, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        default:
                                                                                                           ;
            break;
    }
}


template <HardEvent event>
[aicore] __inline__ __attribute__((always_inline)) void SetFlagImpl(int32_t eventID)
{

                                                                                                                  ;
    event_t e = static_cast<event_t>(eventID);
    switch (event) {
        case HardEvent::MTE2_MTE1:
            set_flag(PIPE_MTE2, PIPE_MTE1, e);
            break;
        case HardEvent::MTE1_MTE2:
            set_flag(PIPE_MTE1, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_MTE3:
            set_flag(PIPE_MTE2, PIPE_MTE3, e);
            break;
        case HardEvent::MTE3_MTE2:
            set_flag(PIPE_MTE3, PIPE_MTE2, e);
            break;
        case HardEvent::MTE1_M:
            set_flag(PIPE_MTE1, PIPE_M, e);
            break;
        case HardEvent::M_MTE1:
            set_flag(PIPE_M, PIPE_MTE1, e);
            break;
        case HardEvent::MTE2_V:
            set_flag(PIPE_MTE2, PIPE_V, e);
            break;
        case HardEvent::V_MTE2:
            set_flag(PIPE_V, PIPE_MTE2, e);
            break;
        case HardEvent::MTE3_V:
            set_flag(PIPE_MTE3, PIPE_V, e);
            break;
        case HardEvent::V_MTE3:
            set_flag(PIPE_V, PIPE_MTE3, e);
            break;
        case HardEvent::M_V:
            set_flag(PIPE_M, PIPE_V, e);
            break;
        case HardEvent::M_S:
            set_flag(PIPE_M, PIPE_S, e);
            break;
        case HardEvent::V_M:
            set_flag(PIPE_V, PIPE_M, e);
            break;
        case HardEvent::S_V:
            set_flag(PIPE_S, PIPE_V, e);
            break;
        case HardEvent::V_S:
            set_flag(PIPE_V, PIPE_S, e);
            break;

        case HardEvent::V_V:
            pipe_barrier(PIPE_V);
            return;

        case HardEvent::MTE3_MTE1:
            set_flag(PIPE_MTE3, PIPE_MTE1, e);
            break;
        case HardEvent::MTE1_MTE3:
            set_flag(PIPE_MTE1, PIPE_MTE3, e);
            break;
        case HardEvent::MTE1_V:
            set_flag(PIPE_MTE1, PIPE_V, e);
            break;
        case HardEvent::MTE2_M:
            set_flag(PIPE_MTE2, PIPE_M, e);
            break;
        case HardEvent::M_MTE2:
            set_flag(PIPE_M, PIPE_MTE2, e);
            break;
        case HardEvent::S_MTE2:
            set_flag(PIPE_S, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_S:
            set_flag(PIPE_MTE2, PIPE_S, e);
            break;
        case HardEvent::V_MTE1:
            set_flag(PIPE_V, PIPE_MTE1, e);
            break;
        case HardEvent::S_MTE3:
            set_flag(PIPE_S, PIPE_MTE3, e);
            break;
        case HardEvent::MTE3_S:
            set_flag(PIPE_MTE3, PIPE_S, e);
            break;

        case HardEvent::M_FIX:
            set_flag(PIPE_M, PIPE_FIX, e);
            break;
        case HardEvent::FIX_M:
            set_flag(PIPE_FIX, PIPE_M, e);
            break;
        case HardEvent::FIX_MTE3:
            set_flag(PIPE_FIX, PIPE_MTE3, e);
            break;
        case HardEvent::FIX_MTE2:
            set_flag(PIPE_FIX, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_FIX:
            set_flag(PIPE_MTE2, PIPE_FIX, e);
            break;
        case HardEvent::FIX_S:
            set_flag(PIPE_FIX, PIPE_S, e);
            break;
        case HardEvent::MTE1_FIX:
            set_flag(PIPE_MTE1, PIPE_FIX, e);
            break;
        case HardEvent::FIX_MTE1:
            set_flag(PIPE_FIX, PIPE_MTE1, e);
            break;
        case HardEvent::FIX_FIX:
            pipe_barrier(PIPE_FIX);
            break;

        case HardEvent::MAX:
            break;
        default:
                                                                                                               ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void WaitFlagImpl(const HardEvent event, int32_t eventID)
{

                                                                                                                  ;
    event_t e = static_cast<event_t>(eventID);
    switch (event) {
# 723 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
        case HardEvent::MTE2_V:
            wait_flag(PIPE_MTE2, PIPE_V, e);
            break;
        case HardEvent::V_MTE2:
            wait_flag(PIPE_V, PIPE_MTE2, e);
            break;
        case HardEvent::MTE3_V:
            wait_flag(PIPE_MTE3, PIPE_V, e);
            break;
        case HardEvent::V_MTE3:
            wait_flag(PIPE_V, PIPE_MTE3, e);
            break;
# 751 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_event.h"
        case HardEvent::FIX_M:
            wait_flag(PIPE_FIX, PIPE_M, e);
            break;
        case HardEvent::M_FIX:
            wait_flag(PIPE_M, PIPE_FIX, e);
            break;
        case HardEvent::MTE2_FIX:
            wait_flag(PIPE_MTE2, PIPE_FIX, e);
            break;
        case HardEvent::FIX_MTE2:
            wait_flag(PIPE_FIX, PIPE_MTE2, e);
            break;
        case HardEvent::FIX_S:
            wait_flag(PIPE_FIX, PIPE_S, e);
            break;
        case HardEvent::FIX_MTE3:
            wait_flag(PIPE_FIX, PIPE_MTE3, e);
            break;
        case HardEvent::MTE1_FIX:
            wait_flag(PIPE_MTE1, PIPE_FIX, e);
            break;
        case HardEvent::FIX_MTE1:
            wait_flag(PIPE_FIX, PIPE_MTE1, e);
            break;
        case HardEvent::FIX_FIX:
            pipe_barrier(PIPE_FIX);
            break;

        case HardEvent::MTE3_MTE2:
            wait_flag(PIPE_MTE3, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_MTE3:
            wait_flag(PIPE_MTE2, PIPE_MTE3, e);
            break;
        case HardEvent::S_MTE2:
            wait_flag(PIPE_S, PIPE_MTE2, e);
            break;
        case HardEvent::MTE2_S:
            wait_flag(PIPE_MTE2, PIPE_S, e);
            break;
        case HardEvent::S_MTE3:
            wait_flag(PIPE_S, PIPE_MTE3, e);
            break;
        case HardEvent::MTE3_S:
            wait_flag(PIPE_MTE3, PIPE_S, e);
            break;
        case HardEvent::M_S:
            wait_flag(PIPE_M, PIPE_S, e);
            break;
        case HardEvent::S_V:
            wait_flag(PIPE_S, PIPE_V, e);
            break;
        case HardEvent::V_S:
            wait_flag(PIPE_V, PIPE_S, e);
            break;
        case HardEvent::V_V:
            return;
        case HardEvent::MAX:
            break;
        default:
            break;
    }
    return;
}
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h" 2
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
enum KernelMetaType : uint8_t {
    KERNEL_TYPE_AIV_ONLY,
    KERNEL_TYPE_AIC_ONLY,
    KERNEL_TYPE_MIX_AIV_1_0,
    KERNEL_TYPE_MIX_AIC_1_0,
    KERNEL_TYPE_MIX_AIC_1_1,
    KERNEL_TYPE_MIX_AIC_1_2,
    KERNEL_TYPE_AICORE,
    KERNEL_TYPE_VECTORCORE,
    KERNEL_TYPE_MIX_AICORE,
    KERNEL_TYPE_MIX_VECTOR_CORE,
    KERNEL_TYPE_MAX,
};

enum KernelType {
    K_TYPE_AICORE = 1,
    K_TYPE_AIC = 2,
    K_TYPE_AIV = 3,
    K_TYPE_MIX_AIC_MAIN = 4,
    K_TYPE_MIX_AIV_MAIN = 5,
    K_TYPE_AIC_ROLLBACK = 6,
    K_TYPE_AIV_ROLLBACK = 7,
    K_TYPE_MAX
};

struct BaseTlv {
    unsigned short type;
    unsigned short len;
};

enum FuncMetaType {
    F_TYPE_KTYPE = 1,
    F_TYPE_CROSS_CORE_SYNC = 2,
    F_TYPE_MIX_TASK_RATION = 3,
    F_TYPE_MAX
};

enum CrossCoreSyncType {
    C_TYPE_USE_SYNC = 1,
    C_TYPE_MAX
};

struct OpSystemRunCfg {
    uint64_t l2Cacheoffset;
};





[aicore] __inline__ __attribute__((always_inline)) void GetCannVersion(__attribute__((cce_global)) char*& versionStr, uint64_t& version, uint64_t& timeStamp)
{

    versionStr = const_cast<__attribute__((cce_global)) char*>("8.0.RC3");





    timeStamp = static_cast<uint64_t>(20241026152044625);





    version = static_cast<uint64_t>(((9 * 100000000) + (1 * 1000000) + ((4 * 100) + 5000)));



}


namespace AscendC {
template <typename U>
[aicore] __inline__ __attribute__((always_inline)) static auto IsLite(int) -> typename U::LiteType;
template <typename U>
[aicore] __inline__ __attribute__((always_inline)) static auto IsLite(void*) -> U;

enum class CacheMode {
    CACHE_MODE_DISABLE = 0,
    CACHE_MODE_NORMAL = 1,
    CACHE_MODE_LAST = 2,
    CACHE_MODE_PERSISTENT = 4
};

enum class CacheRwMode {
    READ = 1,
    WRITE = 2,
    RW = 3
};

template<class T, CacheRwMode rwMode = CacheRwMode::RW>
[aicore] __inline__ __attribute__((cce_global)) T* L2CacheAlter(__attribute__((cce_global)) T* addr, CacheMode mode)
{






    return addr;
}
}

struct FunMetaKType {
    BaseTlv head;
    unsigned int ktype;
};

struct FunMetaCrossCoreType {
    BaseTlv head;
    unsigned int usedCrossCoreSync;
};

struct FunMetaMixCoreType {
    BaseTlv head;
    unsigned short taskRation0;
    unsigned short taskRation1;
};

struct FunLevelKType {
    struct FunMetaKType ktypeMeta;
};

struct FunLevelCrossCoreType {
    struct FunMetaKType ktypeMeta;
    struct FunMetaCrossCoreType crossCoreType;
};

struct FunLevelMixCoreType {
    struct FunMetaKType ktypeMeta;
    struct FunMetaMixCoreType mixCoreType;
};
# 209 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
namespace AscendC {
constexpr int32_t MIX = 0;
constexpr int32_t AIC = 1;
constexpr int32_t AIV = 2;
constexpr size_t DUMP_UINTSIZE = (1024 * 1024);
}
# 225 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
constexpr int32_t g_coreType = AscendC::AIV;
# 262 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
constexpr bool g_gm_overflow_check = false;


namespace AscendC {
# 280 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
[aicore] __inline__ __attribute__((always_inline)) uint32_t DivCeil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t AlignUp(uint32_t a, uint32_t b)
{
    return DivCeil(a, b) * b;
}

[aicore] constexpr __inline__ __attribute__((always_inline)) uint32_t ConstCeil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t Ceil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] __inline__ __attribute__((always_inline)) int32_t CeilDivision(int32_t num1, int32_t num2)
{
    if (num2 == 0) {
        return 0;
    }
    return (num1 + num2 - 1) / num2;
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void OOMCheckTensorListRange(__attribute__((cce_global)) T *gmInputAddr, const int inputSize)
{
# 321 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
}

[aicore] static __inline__ __attribute__((always_inline)) bool OOMCheckAddrInTensorList(uint64_t index, uintptr_t gmAddrConvert,
    uintptr_t& inputOutputAddr, uint64_t& inputOutputLen)
{
# 354 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
    (void)index;
    (void)gmAddrConvert;
    (void)inputOutputAddr;
    (void)inputOutputLen;
    return false;
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void OOMCheckAddrRange(__attribute__((cce_global)) T* gmAddr, const uint64_t gmSize)
{
# 374 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void OOMAddAddrForL2Cache(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) T* oriAddr)
{
# 402 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
}

[aicore] static __inline__ __attribute__((always_inline)) void OOMInit()
{



}

struct TQueConfig {
    bool nd2nz = false;
    bool nz2nd = false;
    bool scmBlockGroup = false;
    uint32_t bufferLen = 0;
    uint32_t bufferNumber = 0;
    uint32_t consumerSize = 0;
    TPosition consumer[8] = {};
};

[aicore] constexpr TQueConfig GetTQueConfig(bool nd2nzIn, bool nz2ndIn, bool scmBlockGroupIn,
    uint32_t bufferLenIn, uint32_t bufferNumberIn, uint32_t consumerSizeIn, const TPosition consumerIn[])
{
    return {
        .nd2nz = nd2nzIn,
        .nz2nd = nz2ndIn,
        .scmBlockGroup = scmBlockGroupIn,
        .bufferLen = bufferLenIn,
        .bufferNumber = bufferNumberIn,
        .consumerSize = consumerSizeIn,
        .consumer = {consumerIn[0], consumerIn[1], consumerIn[2], consumerIn[3],
            consumerIn[4], consumerIn[5], consumerIn[6], consumerIn[7]}
    };
}

[aicore] constexpr TQueConfig GetTQueConfig(const int32_t mask)
{
    return {
        .nd2nz = static_cast<bool>(static_cast<uint32_t>(mask) & 0x1u),
        .nz2nd = static_cast<bool>((static_cast<uint32_t>(mask) & 0x2u) >> 1),
        .scmBlockGroup = static_cast<bool>((static_cast<uint32_t>(mask) & 0x4u) >> 2),
        .bufferLen = 0,
        .bufferNumber = 0,
        .consumerSize = 0,
        .consumer = {TPosition::MAX, TPosition::MAX, TPosition::MAX, TPosition::MAX,
            TPosition::MAX, TPosition::MAX, TPosition::MAX, TPosition::MAX}
    };
}

[aicore] constexpr TQueConfig GetTQueConfig(const TQueConfig* conf)
{
    return {
        .nd2nz = conf->nd2nz,
        .nz2nd = conf->nz2nd,
        .scmBlockGroup = conf->scmBlockGroup,
        .bufferLen = conf->bufferLen,
        .bufferNumber = conf->bufferNumber,
        .consumerSize = conf->consumerSize,
        .consumer = {conf->consumer[0], conf->consumer[1], conf->consumer[2], conf->consumer[3],
            conf->consumer[4], conf->consumer[5], conf->consumer[6], conf->consumer[7]}
    };
}

template <bool b> struct BoolInst {
    using Type = BoolInst<b>;
    static constexpr bool value = b;
};

using TrueType = BoolInst<true>;
using FalseType = BoolInst<false>;

template <typename T, typename U> struct IsSameType : public FalseType {};

template <typename T> struct IsSameType<T, T> : public TrueType {};

template <typename... Arg>
struct Tuple {};

template <typename T, typename U, typename... Args>
[aicore] constexpr bool SupportType()
{
    if constexpr (sizeof...(Args) > 0) {
        return IsSameType<T, U>::value || SupportType<T, Args...>();
    }
    return IsSameType<T, U>::value;
}

template <typename T, int U, int... Args> [aicore] constexpr bool SupportBytes()
{
    if constexpr (sizeof...(Args) > 0) {
        return sizeof(T) == U || SupportBytes<T, Args...>();
    }
    return sizeof(T) == U;
}

const int32_t DEFAULT_BLK_NUM = 8;
const int32_t POWER_MASK_NUM = 8;
const int32_t HALF_FACTOR = 2;
const int32_t DOUBLE_FACTOR = 2;
const int32_t DEFAULT_BLK_STRIDE = 1;
const uint8_t DEFAULT_REPEAT_STRIDE = 8;
const uint8_t HALF_DEFAULT_REPEAT_STRIDE = 4;
const uint8_t ONE_FOURTH_DEFAULT_REPEAT_STRIDE = 2;
const uint64_t FULL_MASK = 0xffffffffffffffff;
const uint64_t CONST_MASK_VALUE = 0x8000000000000000;
const uint16_t MAX_HALF_MASK_LEN = 64;
const int32_t DEFAULT_C0_SIZE = 32;
const int32_t DEFAULT_BLOCK_SIZE = 256;
const int32_t MAX_REPEAT_TIMES = 255;
const int32_t MIN_REPEAT_TIMES = 0;
const bool DEFAULT_REPEAT_STRIDE_MODE = 0;
const bool STRIDE_SIZE_MODE = 0;
const int32_t ONE_BYTE_BIT_SIZE = 8;
const int32_t ONE_DUMP_BACKUP_SIZE = 1024;
const int32_t DUMP_UB_SIZE = 256;
const int32_t DUMP_EXC_FLAG = 7;
const uint32_t TOTAL_L0A_SIZE = 64 * 1024;
const uint32_t TOTAL_L0B_SIZE = 64 * 1024;
const uint32_t TMP_UB_SIZE = 8 * 1024;
const uint32_t MAX_SLICE_SIZE = 6 * 256;
const uint32_t F32_INF = 0x7f800000;
const uint32_t F32_NEG_INF = 0xff800000;
const uint32_t F32_NAN = 0x7fc00000;

const uint16_t VALUE_512 = 512;
const uint16_t UINT12_MAX = 4095;
const uint16_t UINT15_MAX = 32767;


const uint32_t BLOCK_INFO_LEN_POS = 0;
const uint32_t BLOCK_INFO_CORE_POS = 1;
const uint32_t BLOCK_INFO_BLOCKNUM_POS = 2;
const uint32_t BLOCK_INFO_DUMPOFFSET_POS = 3;
const uint32_t BLOCK_INFO_MAGIC_POS = 4;
const uint32_t BLOCK_INFO_RSV_POS = 5;
const uint32_t BLOCK_INFO_DUMP_ADDR = 6;
const uint32_t BLOCK_INFO_MAGIC_NUM = 0x5aa5bccd;

const uint32_t DUMP_META_TYPE_POS = 0;
const uint32_t DUMP_META_LEN_POS = 4;
const uint16_t DUMP_META_BLOCK_DIM_POS = 8;
const uint8_t DUMP_META_CORE_TYPE_POS = 10;
const uint8_t DUMP_META_TASK_RATION = 11;
const uint32_t DUMP_META_RSV_POS = 12;

const uint32_t DUMP_MESSAGE_HEAD_TYPE_POS = 0;
const uint32_t DUMP_MESSAGE_HEAD_LEN_POS = 1;
const uint32_t DUMP_MESSAGE_HEAD_ADDR_POS = 2;
const uint32_t DUMP_MESSAGE_HEAD_DATA_TYPE_POS = 3;
const uint32_t DUMP_MESSAGE_HEAD_DESC_POS = 4;
const uint32_t DUMP_MESSAGE_HEAD_BUFFERID_POS = 5;
const uint32_t DUMP_MESSAGE_HEAD_POSITION_POS = 6;
const uint32_t DUMP_MESSAGE_HEAD_RSV_POS = 7;
const uint32_t DUMP_SCALAR_POS = 8;
const uint32_t DUMP_CORE_COUNT = 75;
const uint32_t DUMP_WORKSPACE_SIZE = DUMP_CORE_COUNT * (1024 * 1024);

const uint32_t DUMP_SHAPE_MESSAGE_HEAD_TYPE_POS = 0;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_LEN_POS = 1;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_DIM_POS = 2;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_SHAPE_START_POS = 3;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_RSV_POS = 11;
const uint32_t DUMP_SHAPE_MESSAGE_TL_LEN = 8;

constexpr int32_t CTRL_46_BIT = 46;
constexpr int32_t CTRL_47_BIT = 47;
constexpr int32_t CTRL_48_BIT = 48;
constexpr int32_t CTRL_53_BIT = 53;


constexpr uint32_t TENSOR_TENSOR_FLOAT_POWER_FACTOR = 4;
constexpr uint32_t TENSOR_TENSOR_INT_POWER_FACTOR = 6;
constexpr uint32_t TENSOR_TENSOR_HALF_POWER_FACTOR = 7;
constexpr uint32_t TENSOR_SCALAR_FLOAT_POWER_FACTOR = 5;
constexpr uint32_t TENSOR_SCALAR_INT_POWER_FACTOR = 7;
constexpr uint32_t TENSOR_SCALAR_HALF_POWER_FACTOR = 7;
constexpr uint32_t POWER_TWO = 2;
constexpr uint32_t POWER_THREE = 3;
constexpr uint32_t POWER_INT32_BITS = 32;


constexpr uint32_t INT4_TWO = 2;
constexpr uint32_t INT4_BIT_NUM = 4;


constexpr int32_t DEQ_SHIFT_LEFT_17_BIT = 131072;
constexpr float DEQ_SHIFT_RIGHT_17_BIT = 1.0 / DEQ_SHIFT_LEFT_17_BIT;
constexpr int8_t ADDDEQRELU_MASK_MODE_ONE = 1;
constexpr int8_t ADDDEQRELU_MASK_MODE_TWO = 2;
# 599 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
const int32_t TOTAL_VEC_LOCAL_SIZE = 184 * 1024;
const uint32_t TOTAL_UB_SIZE = 192 * 1024;
const uint32_t TMP_UB_OFFSET = 184 * 1024;
const uint32_t TOTAL_L1_SIZE = 512 * 1024 - 128;
const uint32_t SINGLE_MSG_SIZE = 64;
const uint32_t CACHE_LINE_SIZE = 64;
const uint32_t TOTAL_L0C_SIZE = 128 * 1024;
# 627 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
const uint8_t PAD_SIZE = 4;
const uint8_t MRG_SORT_ELEMENT_LEN = 4;
const uint8_t DEFAULT_DATA_COPY_NBURST = 1;
const uint8_t DEFAULT_DATA_COPY_STRIDE = 0;
const int32_t BLOCK_CUBE = 16;
const int32_t CUBE_MAX_SIZE = 256;
const int32_t BYTE_PER_FRACTAL = 512;
const int32_t SRC_BURST_LEN_SIZE_ELE = 16;
const int32_t SRC_GAP_SIZE_BYTE = 32;
const int32_t DST_BURST_LEN_SIZE_ELE = 256;
const int32_t VREDUCE_PER_REP_OUTPUT = 2;
const uint16_t ONE_BLK_SIZE = 32;
const uint16_t ONE_PARAM_SIZE = 8;
const uint16_t AIV_CORE_NUM = 50;
const uint16_t DUMP_MSG_HEAD_SIZE = 24;
const int32_t ONE_REPEAT_BYTE_SIZE = 256;
const int32_t FULL_MASK_LEN = 128;
const int32_t HLAF_MASK_LEN = 64;
const int32_t DEFAULT_REDUCE_DST_REP_SRIDE = 1;
const uint8_t B64_BYTE_SIZE = 8;
const uint8_t B32_BYTE_SIZE = 4;
const uint8_t B16_BYTE_SIZE = 2;
const uint8_t B8_BYTE_SIZE = 1;
const uint8_t B32_DATA_NUM_PER_BLOCK = 8;
const uint8_t B16_DATA_NUM_PER_BLOCK = 16;
const int32_t B16_DATA_NUM_PER_REPEAT = 128;
const int32_t B32_DATA_NUM_PER_REPEAT = 64;
const int32_t BLOCK_STRIDE_POS_IN_SM = 16;
const int32_t PLD_BUFFER_SIZE = 2;
const uint8_t FIXPIPE_DEQ_TENSOR_SIZE = 16;
const uint8_t SET_DATA_EXP_ZERO = 0;
const uint8_t SET_DATA_EXP_ONE = 1;
const uint8_t SET_DATA_EXP_TWO = 2;
const uint8_t SET_DATA_EXP_THREE = 3;
const uint8_t VDEQ_TENSOR_SIZE = 16;






constexpr size_t RESERVED_WORKSPACE = 16 * 1024 * 1024;






const int32_t NCHW_CONV_ADDR_LIST_SIZE = 16;
const int32_t VA_REG_ARRAY_LEN = 8;
const uint8_t CONV2D_IMG_SIZE = 2;
const uint8_t CONV2D_KERNEL_SIZE = 2;
const uint8_t CONV2D_STRIDE = 2;
const uint8_t CONV2D_PAD = 4;
const uint8_t CONV2D_DILATION = 2;
const int32_t K_MAX_DIM = 8;

const uint32_t TWO_OF_STACK_BUFFER = 2;
const uint32_t THREE_OF_STACK_BUFFER = 3;
const uint32_t HALF_REPEAT_SIZE = ONE_REPEAT_BYTE_SIZE / B16_BYTE_SIZE;
const uint32_t FLOAT_REPEAT_SIZE = ONE_REPEAT_BYTE_SIZE / B32_BYTE_SIZE;
const uint32_t ONE_REPEAT_FLOAT_SIZE = ONE_REPEAT_BYTE_SIZE / B32_BYTE_SIZE;
const uint32_t ONE_REPEAT_HALF_SIZE = ONE_REPEAT_BYTE_SIZE / B16_BYTE_SIZE;
const uint32_t MAX_REPEAT_FLOAT_SIZE = ONE_REPEAT_FLOAT_SIZE * MAX_REPEAT_TIMES;
const uint32_t MAX_REPEAT_HALF_SIZE = ONE_REPEAT_HALF_SIZE * MAX_REPEAT_TIMES;
const uint32_t ONE_BLK_HALF_NUM = ONE_BLK_SIZE / B16_BYTE_SIZE;
const uint32_t ONE_BLK_FLOAT_NUM = ONE_BLK_SIZE / B32_BYTE_SIZE;
const uint32_t BRCB_BROADCAST_NUMBER = 8;
const uint32_t BRCB_MAX_REPEAT_SIZE = BRCB_BROADCAST_NUMBER * MAX_REPEAT_TIMES;
const int32_t MIN_BLOCK_LEN = 1;
const uint32_t PAIR_REDUCE_REPEAT_STRIDE_LEN = 128;
const uint32_t PAIR_REDUCE_SUM_MERGES = 2;
const uint32_t TWO_HUNDRED_FIFTY_TWO_REPEAT = 252;
const uint32_t TWO_HUNDRED_FIFTY_TWO_REPEAT_BYTE_SIZE = TWO_HUNDRED_FIFTY_TWO_REPEAT * ONE_REPEAT_BYTE_SIZE;
const uint32_t REDUCEV2_MODE_SEVEN = 7;
const uint32_t DROPOUT_MODE_BYTE_MISALIGN = 1;
const uint32_t DROPOUT_MODE_BYTE_ALIGN = 2;
const uint32_t DROPOUT_MODE_BIT_ALIGN = 3;
const uint32_t DROPOUT_MODE_BIT_MISALIGN = 4;
const uint32_t REDUCEV2_MODE_ONE = 1;
const uint32_t REDUCEV2_MODE_TWO = 2;
const uint32_t REDUCEV2_MODE_THREE = 3;


const int32_t B8_TMP_ELE_LEN = 1024;
const int32_t B16_TMP_ELE_LEN = 256;
const int32_t B32_TMP_ELE_LEN = 128;
const int32_t B8_TRANS_LEN = 1024;
const int32_t B8_TRANS_FRACTAL = 512;
const int32_t B8_TRANS_ROW = 32;
const int32_t B8_COPY_COL = 32;


const uint64_t LOAD_M_START_POSITION = 48;
const uint64_t LOAD_K_START_POSITION = 32;
const uint64_t LOAD_M_EXTENSION = 16;
const uint64_t LOAD_DILATION_FILTER_H = 40;
const uint64_t LOAD_DILATION_FILTER_W = 32;
const uint64_t LOAD_FILTER_H = 24;
const uint64_t LOAD_FILTER_W = 16;
const uint64_t LOAD_STRIDE_H = 8;
# 836 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
template <bool condition, class T1, class T2>
struct Conditional {
    using type = T1;
};

template <class T1, class T2>
struct Conditional<false, T1, T2> {
    using type = T2;
};

template <int bitNum, bool sign = true>
struct IntegerSubType {
    static int const kBits = bitNum;
    static bool const kSigned = sign;

    using T = typename Conditional<kSigned, int8_t, uint8_t>::type;
    using Storage = uint8_t;

    static Storage const mask = Storage(((static_cast<uint64_t>(1)) << static_cast<uint32_t>(kBits)) - 1);
    Storage storage;
    [aicore] __inline__ __attribute__((always_inline)) IntegerSubType() = default;

    [aicore] __inline__ __attribute__((always_inline)) IntegerSubType(uint32_t value)
        : storage(reinterpret_cast<Storage const &>(value) & mask) {}

    [aicore] __inline__ __attribute__((always_inline)) IntegerSubType(int32_t value)
        : storage(reinterpret_cast<Storage const &>(value) & mask) {}

    [aicore] __inline__ __attribute__((always_inline)) operator T() const
    {
        if (kSigned && ((storage & Storage(static_cast<uint64_t>(1) << static_cast<uint32_t>(kBits - 1))) != 0)) {

            return T(storage) | ~T(mask);
        }
        return T(storage);
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator == (IntegerSubType const &rhs) const
    {
        return storage == rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator != (IntegerSubType const &rhs) const
    {
        return storage != rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator > (IntegerSubType const &rhs) const
    {
        bool lhsIsNeg = (this->storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(this->kBits - 1)));
        bool rhsIsNeg = (rhs.storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(rhs.kBits - 1)));
        if (kSigned && (lhsIsNeg != rhsIsNeg)) {
            return (!lhsIsNeg) && rhsIsNeg;
        }
        return this->storage > rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator >= (IntegerSubType const &rhs) const
    {
        bool lhsIsNeg = (this->storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(this->kBits - 1)));
        bool rhsIsNeg = (rhs.storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(rhs.kBits - 1)));
        if (kSigned && (lhsIsNeg != rhsIsNeg)) {
            return (!lhsIsNeg) && rhsIsNeg;
        }
        return storage >= rhs.storage;
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator < (IntegerSubType const &rhs) const
    {
        return !(*this >= rhs);
    }

    [aicore] __inline__ __attribute__((always_inline)) bool operator <= (IntegerSubType const &rhs) const
    {
        return !(*this > rhs);
    }
};

using int4b_t = IntegerSubType<INT4_BIT_NUM, true>;

template <typename T> struct SizeOfBits {};
template <>
struct SizeOfBits<int4b_t> {
    static int const value = INT4_BIT_NUM;
};

[aicore] __inline__ __attribute__((always_inline)) bool CheckCastOverlappingHigh(const uint64_t dstAddr, const uint64_t srcAddr,
    const uint32_t dstTypeSize, const uint32_t srcTypeSize, const uint32_t calCount)
{
    uint64_t addrLow = dstAddr > srcAddr ? srcAddr : dstAddr;
    uint64_t addrHigh = dstAddr > srcAddr ? dstAddr : srcAddr;
    uint64_t needSizeLow = dstAddr > srcAddr ? calCount * srcTypeSize : calCount * dstTypeSize;

    if ((srcTypeSize < dstTypeSize) && (srcAddr >= AlignUp(dstAddr + calCount * srcTypeSize, ONE_BLK_SIZE))) {
        return true;
    }
    if (dstTypeSize > srcTypeSize && srcAddr == dstAddr) {
        return false;
    }
    if ((needSizeLow > static_cast<uint64_t>(ONE_REPEAT_BYTE_SIZE)) && (srcAddr != dstAddr) &&
        ((addrLow + needSizeLow > addrHigh))) {
        return false;
    }
    return true;
}
# 1107 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
class MaskSetter {
public:
    static MaskSetter& Instance()
    {
        static MaskSetter instance;
        return instance;
    };

    void SetMask(bool setMask)
    {
        isSetMask = setMask;
    }

    bool GetMask() const
    {
        return isSetMask;
    }

private:
    MaskSetter(){};
    ~MaskSetter(){};
    bool isSetMask = true;
};

class Int4Setter {
public:
    static Int4Setter& Instance()
    {
        static Int4Setter instance;
        return instance;
    };

    void SetInt4()
    {
        isInt4 = true;
    }

    void SetDstInt4()
    {
        isDstInt4 = true;
    }

    void SetSrcInt4()
    {
        isSrcInt4 = true;
    }

    void ResetInt4()
    {
        isInt4 = false;
    }

    void ResetDstSrcInt4()
    {
        isDstInt4 = false;
        isSrcInt4 = false;
    }

    bool GetInt4() const
    {
        return isInt4;
    }

    bool GetDstInt4() const
    {
        return isDstInt4;
    }

    bool GetSrcInt4() const
    {
        return isSrcInt4;
    }

private:
    Int4Setter(){};
    ~Int4Setter(){};

    bool isInt4 = false;
    bool isDstInt4 = false;
    bool isSrcInt4 = false;
};

union NotNumUnion {
    [aicore] NotNumUnion() {}
    float f;
    uint32_t i;
};

enum class TShapeType : uint8_t {
    DEFAULT,
    NHWC,
    NC1HWC0,
    NHC,
    NCHT,
    ND,
    FRACTAL_NZ,
    HNC,
    HCNT,
    NDHWC,
    FRACTAL_Z_3D,
    NDHC,
    DCHNT,
    FRACTAL_Z,
    NCDHW,
    NDC1HWC0,
    NCDH,
    NDCHT,
    NCHW,
    NCH,
    HWCN,
    HCN,
    CHNT,
    DHWCN,
    DHCN
};

enum class GatherMaskMode : uint8_t {
    VERSION_V1 = 0,
    VERSION_V2 = 1
};

const GatherMaskMode defaultGahterMaskMode = GatherMaskMode::VERSION_V2;




enum class CMPMODE : uint8_t {
    LT = 0,
    GT,
    EQ,
    LE,
    GE,
    NE,
};

enum class RoundMode : uint8_t {
    CAST_NONE = 0,
    CAST_RINT,
    CAST_FLOOR,
    CAST_CEIL,
    CAST_ROUND,
    CAST_TRUNC,
    CAST_ODD,
};

enum class SELMODE : uint8_t {
    VSEL_CMPMASK_SPR = 0,
    VSEL_TENSOR_SCALAR_MODE,
    VSEL_TENSOR_TENSOR_MODE,
};

enum class BlockMode : uint8_t {
    BLOCK_MODE_NORMAL = 0,
    BLOCK_MODE_MATRIX,
    BLOCK_MODE_VECTOR,
    BLOCK_MODE_SMALL_CHANNEL,
    BLOCK_MODE_DEPTHWISE,
};

enum class DeqScale : uint8_t {
    DEQ_NONE = 0,
    DEQ,
    VDEQ,
    DEQ8,
    VDEQ8,
    DEQ16,
    VDEQ16,
};

enum class ReduceMode : uint8_t {
    REDUCE_MAX = 0,
    REDUCE_MIN,
    REDUCE_SUM,
};

enum class ReduceOrder : uint8_t {
    ORDER_VALUE_INDEX = 0,
    ORDER_INDEX_VALUE,
    ORDER_ONLY_VALUE,
    ORDER_ONLY_INDEX,
};

enum class DumpType : uint8_t {
    DUMP_DEFAULT = 0,
    DUMP_SCALAR,
    DUMP_TENSOR,
    DUMP_SHAPE,
    DUMP_ASSERT,
    DUMP_META,
};

enum class CLAMPMODE {
    CLAMP_MAX = 0,
    CLAMP_MIN,
};

enum class FmatrixMode : uint8_t {
    FMATRIX_LEFT = 0,
    FMATRIX_RIGHT = 1,
};


enum class PcieCtrl : uint64_t {
    WR = 0,
    RD
};

enum class DeQuantMode : uint8_t {
    DEQUANT_WITH_SINGLE_ROW = 0,
    DEQUANT_WITH_MULTI_ROW,
};

struct CheckLocalMemoryIAParam {
    [aicore] CheckLocalMemoryIAParam()
    {
        enableBit = 0;
        startAddr = 0;
        endAddr = 0;
        isScalarRead = false;
        isScalarWrite = false;
        isVectorRead = false;
        isVectorWrite = false;
        isMteRead = false;
        isMteWrite = false;
        isEnable = false;
    }

    [aicore] CheckLocalMemoryIAParam(const uint8_t enableBitIn, const uint32_t startAddrIn, const uint32_t endAddrIn,
        const bool isScalarReadIn, const bool isScalarWriteIn, const bool isVectorReadIn, const bool isVectorWriteIn,
        const bool isMteReadIn, const bool isMteWriteIn, const bool isEnableIn)
    {
        enableBit = enableBitIn;
        startAddr = startAddrIn;
        endAddr = endAddrIn;
        isScalarRead = isScalarReadIn;
        isScalarWrite = isScalarWriteIn;
        isVectorRead = isVectorReadIn;
        isVectorWrite = isVectorWriteIn;
        isMteRead = isMteReadIn;
        isMteWrite = isMteWriteIn;
        isEnable = isEnableIn;
    }

    uint8_t enableBit = 0;
    uint32_t startAddr = 0;
    uint32_t endAddr = 0;
    bool isScalarRead = false;
    bool isScalarWrite = false;
    bool isVectorRead = false;
    bool isVectorWrite = false;
    bool isMteRead = false;
    bool isMteWrite = false;
    bool isEnable = false;
    uint32_t reserved = 0;
};

struct ReduceRepeatParams {
    [aicore] ReduceRepeatParams()
    {
        highMask = FULL_MASK;
        lowMask = FULL_MASK;
        repeatTimes = 0;
        dstRepStride = DEFAULT_REDUCE_DST_REP_SRIDE;
        srcBlkStride = DEFAULT_BLK_STRIDE;
        srcRepStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] ReduceRepeatParams(const int32_t mask, const int32_t repeatTimesIn, const int32_t dstRepStrideIn,
        const int32_t srcBlkStrideIn, const int32_t srcRepStrideIn)
    {




        if (mask == HLAF_MASK_LEN) {
            highMask = 0;
            lowMask = FULL_MASK;
        } else if (mask == HLAF_MASK_LEN * DOUBLE_FACTOR) {
            highMask = FULL_MASK;
            lowMask = FULL_MASK;
        } else {
            highMask = (mask > HLAF_MASK_LEN) ?
                (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(mask - HLAF_MASK_LEN)) - 1) :
                0;
            lowMask =
                (mask > HLAF_MASK_LEN) ? FULL_MASK : (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(mask)) - 1);
        }

        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcBlkStride = srcBlkStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    [aicore] ReduceRepeatParams(const uint64_t mask[2], const int32_t repeatTimesIn, const int32_t dstRepStrideIn,
        const int32_t srcBlkStrideIn, const int32_t srcRepStrideIn)
    {




        highMask = mask[1];
        lowMask = mask[0];

        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcBlkStride = srcBlkStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    uint64_t bitMask[2] = {0, 0};
    int32_t normalMask = 0;
    int32_t maskMode = 0;
    int32_t repeatTimes = 0;
    int32_t dstRepStride = 0;
    int32_t srcBlkStride = 0;
    int32_t srcRepStride = 0;
};

struct DumpMessageHead {
    [aicore] DumpMessageHead()
    {
        type = 0;
        lenth = 0;
        addr = 0;
        dataType = 0;
        desc = 0;
        bufferId = 0;
        position = 0;
        rsv = 0;
    }

    [aicore] DumpMessageHead(uint32_t typeIn, uint32_t lenthIn, uint32_t addrIn, uint32_t dataTypeIn, uint32_t descIn,
        uint32_t bufferIdIn, uint32_t positionIn, uint32_t rsvIn)
    {
        type = typeIn;
        lenth = lenthIn;
        addr = addrIn;
        dataType = dataTypeIn;
        desc = descIn;
        bufferId = bufferIdIn;
        position = positionIn;
        rsv = rsvIn;
    }

    uint32_t type = 0;
    uint32_t lenth = 0;
    uint32_t addr = 0;
    uint32_t dataType = 0;
    uint32_t desc = 0;
    uint32_t bufferId = 0;
    uint32_t position = 0;
    uint32_t rsv = 0;
};

struct DumpShapeMessageHead {
    [aicore] DumpShapeMessageHead()
    {
        dim = 0;
        rsv = 0;
        for (uint32_t idx = 0; idx < 8; ++idx) {
            shape[idx] = 0;
        }
    }

    [aicore] DumpShapeMessageHead(uint32_t dimIn, uint32_t shapeIn[], uint32_t rsvIn = 0)
    {


          ;
        dim = dimIn;
        rsv = rsvIn;
        for (uint32_t idx = 0; idx < 8; ++idx) {
            if (idx < dim) {
                shape[idx] = shapeIn[idx];
            } else {
                shape[idx] = 0;
            }
        }
    }

    uint32_t dim = 0;
    uint32_t shape[8];
    uint32_t rsv = 0;
};

struct UnaryRepeatParams {
    [aicore] UnaryRepeatParams()
    {
        blockNumber = DEFAULT_BLK_NUM;
        dstBlkStride = DEFAULT_BLK_STRIDE;
        srcBlkStride = DEFAULT_BLK_STRIDE;
        dstRepStride = DEFAULT_REPEAT_STRIDE;
        srcRepStride = DEFAULT_REPEAT_STRIDE;
        halfBlock = false;
    }

    [aicore] UnaryRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t srcBlkStrideIn,
        const uint8_t dstRepStrideIn, const uint8_t srcRepStrideIn)
    {
        dstBlkStride = dstBlkStrideIn;
        srcBlkStride = srcBlkStrideIn;
        dstRepStride = dstRepStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    [aicore] UnaryRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t srcBlkStrideIn,
        const uint8_t dstRepStrideIn, const uint8_t srcRepStrideIn, const bool halfBlockIn)
    {
        dstBlkStride = dstBlkStrideIn;
        srcBlkStride = srcBlkStrideIn;
        dstRepStride = dstRepStrideIn;
        srcRepStride = srcRepStrideIn;
        halfBlock = halfBlockIn;
    }

    uint32_t blockNumber = 0;
    uint16_t dstBlkStride = 0;
    uint16_t srcBlkStride = 0;
    uint8_t dstRepStride = 0;
    uint8_t srcRepStride = 0;

    bool repeatStrideMode = false;
    bool strideSizeMode = false;
    bool halfBlock = false;
};

struct ProposalIntriParams {
    [aicore] ProposalIntriParams()
    {
        repeat = 0;
        modeNumber = 0;
    }

    [aicore] ProposalIntriParams(const int32_t repeatTimes, const int32_t modeNumberIn)
    {
        repeat = repeatTimes;
        modeNumber = modeNumberIn;
    }

    int32_t repeat = 0;
    int32_t modeNumber = 0;
};

struct BlockInfo {
    [aicore] BlockInfo()
    {
        len = 0;
        core = 0;
        blockNum = 0;
        dumpOffset = 0;
        magic = 0;
        rsv = 0;
        dumpAddr = 0;
    }
    [aicore] BlockInfo(uint64_t dumpAddrIn, uint32_t lenIn, uint32_t coreIn, uint32_t blockNumIn,
        uint32_t dumpOffsetIn, uint32_t magicIn, uint32_t rsvIn)
    {
        len = lenIn;
        core = coreIn;
        blockNum = blockNumIn;
        dumpOffset = dumpOffsetIn;
        magic = magicIn;
        rsv = rsvIn;
        dumpAddr = dumpAddrIn;
    }
    uint32_t len = 0;
    uint32_t core = 0;
    uint32_t blockNum = 0;
    uint32_t dumpOffset = 0;
    uint32_t magic = 0;
    uint32_t rsv = 0;
    uint64_t dumpAddr = 0;
};

struct DumpMeta {
    uint32_t typeId = static_cast<uint32_t>(DumpType::DUMP_META);
    uint32_t len = 8;
    uint16_t blockDim = 0;
    uint8_t coreType = 0;
    uint8_t taskRation = 0;
    uint32_t rsv = 0;
};
struct BinaryRepeatParams {
    [aicore] BinaryRepeatParams()
    {
        blockNumber = DEFAULT_BLK_NUM;
        dstBlkStride = DEFAULT_BLK_STRIDE;
        src0BlkStride = DEFAULT_BLK_STRIDE;
        src1BlkStride = DEFAULT_BLK_STRIDE;
        dstRepStride = DEFAULT_REPEAT_STRIDE;
        src0RepStride = DEFAULT_REPEAT_STRIDE;
        src1RepStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] BinaryRepeatParams(const uint8_t dstBlkStrideIn, const uint8_t src0BlkStrideIn,
        const uint8_t src1BlkStrideIn, const uint8_t dstRepStrideIn, const uint8_t src0RepStrideIn,
        const uint8_t src1RepStrideIn)
    {
        dstBlkStride = dstBlkStrideIn;
        src0BlkStride = src0BlkStrideIn;
        src1BlkStride = src1BlkStrideIn;
        dstRepStride = dstRepStrideIn;
        src0RepStride = src0RepStrideIn;
        src1RepStride = src1RepStrideIn;
    }

    uint32_t blockNumber = 0;
    uint8_t dstBlkStride = 0;
    uint8_t src0BlkStride = 0;
    uint8_t src1BlkStride = 0;
    uint8_t dstRepStride = 0;
    uint8_t src0RepStride = 0;
    uint8_t src1RepStride = 0;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};

struct VdeqInfo {
    [aicore] VdeqInfo() {}
    [aicore] VdeqInfo(const float vdeqScaleIn[VDEQ_TENSOR_SIZE], const int16_t vdeqOffsetIn[VDEQ_TENSOR_SIZE],
        const bool vdeqSignModeIn[VDEQ_TENSOR_SIZE])
    {
        for (int32_t i = 0; i < VDEQ_TENSOR_SIZE; ++i) {
            vdeqScale[i] = vdeqScaleIn[i];
            vdeqOffset[i] = vdeqOffsetIn[i];
            vdeqSignMode[i] = vdeqSignModeIn[i];
        }
    }

    float vdeqScale[VDEQ_TENSOR_SIZE] = { 0 };
    int16_t vdeqOffset[VDEQ_TENSOR_SIZE] = { 0 };
    bool vdeqSignMode[VDEQ_TENSOR_SIZE] = { 0 };
};

struct GatherRepeatParams {
    [aicore] GatherRepeatParams()
    {
        blockNumber = DEFAULT_BLK_NUM;
        dstBlkStride = DEFAULT_BLK_STRIDE;
        src0BlkStride = DEFAULT_BLK_STRIDE;
        src1BlkStride = DEFAULT_BLK_STRIDE;
        dstRepStride = DEFAULT_REPEAT_STRIDE;
        src0RepStride = DEFAULT_REPEAT_STRIDE;
        src1RepStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] GatherRepeatParams(const uint8_t dstBlkStrideIn, const uint8_t dstRepStrideIn)
    {
        dstBlkStride = dstBlkStrideIn;
        dstRepStride = dstRepStrideIn;
    }

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;

    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};

struct BrcbRepeatParams {
    [aicore] BrcbRepeatParams()
    {
        blockNumber = DEFAULT_BLK_NUM;
        dstBlkStride = DEFAULT_BLK_STRIDE;
        src0BlkStride = DEFAULT_BLK_STRIDE;
        src1BlkStride = DEFAULT_BLK_STRIDE;
        dstRepStride = DEFAULT_REPEAT_STRIDE;
        src0RepStride = DEFAULT_REPEAT_STRIDE;
        src1RepStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] BrcbRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t dstRepStrideIn)
    {
        dstBlkStride = dstBlkStrideIn;
        dstRepStride = dstRepStrideIn;
    }

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint16_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;

    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};


using LoadData2dParams = struct LoadData2DParams;
struct LoadData2DParams {
    [aicore] LoadData2DParams()
    {
        startIndex = 0;
        repeatTimes = 0;
        srcStride = 0;
        sid = 0;
        dstGap = 0;
        ifTranspose = false;
        addrMode = 0;
    }

    [aicore] LoadData2DParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn, const uint16_t srcStrideIn,
        const uint8_t sidIn, const uint16_t dstGapIn, const bool ifTransposeIn, const uint8_t addrModeIn)
    {
        startIndex = startIndexIn;
        repeatTimes = repeatTimesIn;
        srcStride = srcStrideIn;
        sid = sidIn;
        dstGap = dstGapIn;
        ifTranspose = ifTransposeIn;
        addrMode = addrModeIn;
    }

    uint16_t startIndex = 0;
    uint16_t dstGap = 0;
    uint16_t srcStride = 0;
    bool ifTranspose = 0;
    uint8_t repeatTimes = 0;

    uint8_t sid = 0;
    uint8_t addrMode = 0;
};

struct LoadData2DParamsV2 {
    [aicore] LoadData2DParamsV2()
    {
        mStartPosition = 0;
        kStartPosition = 0;
        mStep = 0;
        kStep = 0;
        srcStride = 0;
        dstStride = 0;
        ifTranspose = false;
        sid = 0;
    }

    [aicore] LoadData2DParamsV2(const uint32_t mStartPositionIn, const uint32_t kStartPositionIn,
        const uint16_t mStepIn, const uint16_t kStepIn, const int32_t srcStrideIn, const uint16_t dstStrideIn,
        const bool ifTransposeIn, const uint8_t sidIn)
    {
        mStartPosition = mStartPositionIn;
        kStartPosition = kStartPositionIn;
        mStep = mStepIn;
        kStep = kStepIn;
        srcStride = srcStrideIn;
        dstStride = dstStrideIn;
        ifTranspose = ifTransposeIn;
        sid = sidIn;
    }

    uint32_t mStartPosition = 0;
    uint32_t kStartPosition = 0;
    uint16_t mStep = 0;
    uint16_t kStep = 0;
    int32_t srcStride = 0;
    uint16_t dstStride = 0;
    bool ifTranspose = false;
    uint8_t sid = 0;
};

struct LoadData2dTransposeParams {
    [aicore] LoadData2dTransposeParams()
    {
        startIndex = 0;
        repeatTimes = 0;
        srcStride = 0;
        dstGap = 0;
        dstFracGap = 0;
        addrMode = 0;
    }

    [aicore] LoadData2dTransposeParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn, const uint8_t addrModeIn)
    {
        startIndex = startIndexIn;
        repeatTimes = repeatTimesIn;
        srcStride = srcStrideIn;
        dstGap = dstGapIn;
        dstFracGap = dstfracGapIn;
        addrMode = addrModeIn;
    }

    [aicore] LoadData2dTransposeParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn)
    {
        startIndex = startIndexIn;
        repeatTimes = repeatTimesIn;
        srcStride = srcStrideIn;
        dstGap = dstGapIn;
        dstFracGap = dstfracGapIn;
    }

    uint16_t startIndex = 0;
    uint8_t repeatTimes = 0;
    uint16_t srcStride = 0;
    uint16_t dstGap = 0;
    uint16_t dstFracGap = 0;
    uint8_t addrMode = 0;
};

struct LoadData2dTransposeParamsV2 {
    [aicore] LoadData2dTransposeParamsV2()
    {
        startIndex = 0;
        repeatTimes = 0;
        srcStride = 0;
        dstGap = 0;
        dstFracGap = 0;
        srcFracGap = 0;
        addrMode = 0;
    }

    [aicore] LoadData2dTransposeParamsV2(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn,
        const uint16_t srcFracGapIn)
    {
        startIndex = startIndexIn;
        repeatTimes = repeatTimesIn;
        srcStride = srcStrideIn;
        dstGap = dstGapIn;
        dstFracGap = dstfracGapIn;
        srcFracGap = srcFracGapIn;
    }

    [aicore] LoadData2dTransposeParamsV2(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn,
        const uint16_t srcFracGapIn, const uint8_t addrModeIn)
    {
        startIndex = startIndexIn;
        repeatTimes = repeatTimesIn;
        srcStride = srcStrideIn;
        dstGap = dstGapIn;
        dstFracGap = dstfracGapIn;
        srcFracGap = srcFracGapIn;
        addrMode = addrModeIn;
    }

    uint16_t startIndex = 0;
    uint8_t repeatTimes = 0;
    uint16_t srcStride = 0;
    uint16_t dstGap = 0;
    uint16_t dstFracGap = 0;
    uint16_t srcFracGap = 0;
    uint8_t addrMode = 0;
};

template <typename T>
struct LoadData3DParamsV1 {
    [aicore] LoadData3DParamsV1()
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = 0;
        }
        l1H = 0;
        l1W = 0;
        c1Index = 0;
        fetchFilterW = 0;
        fetchFilterH = 0;
        leftTopW = 0;
        leftTopH = 0;
        strideW = 0;
        strideH = 0;
        filterW = 0;
        filterH = 0;
        dilationFilterW = 0;
        dilationFilterH = 0;
        jumpStride = 0;
        repeatMode = 0;
        repeatTime = 0;
        cSize = 0;
        padValue = 0;
    }

    [aicore] LoadData3DParamsV1(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t c1IndexIn, const uint8_t fetchFilterWIn, const uint8_t fetchFilterHIn, const int16_t leftTopWIn,
        const int16_t leftTopHIn, const uint8_t strideWIn, const uint8_t strideHIn, const uint8_t filterWIn,
        const uint8_t filterHIn, const uint8_t dilationFilterWIn, const uint8_t dilationFilterHIn,
        const uint8_t jumpStrideIn, const uint8_t repeatModeIn, const uint8_t repeatTimeIn, const uint8_t cSizeIn,
        const T padValueIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
        l1H = l1HIn;
        l1W = l1WIn;
        c1Index = c1IndexIn;
        fetchFilterW = fetchFilterWIn;
        fetchFilterH = fetchFilterHIn;
        leftTopW = leftTopWIn;
        leftTopH = leftTopHIn;
        strideW = strideWIn;
        strideH = strideHIn;
        filterW = filterWIn;
        filterH = filterHIn;
        dilationFilterW = dilationFilterWIn;
        dilationFilterH = dilationFilterHIn;
        jumpStride = jumpStrideIn;
        repeatMode = repeatModeIn;
        repeatTime = repeatTimeIn;
        cSize = cSizeIn;
        padValue = padValueIn;
    }

    uint8_t padList[PAD_SIZE] = {0};
    uint8_t strideW = 0;
    uint8_t strideH = 0;
    uint8_t filterW = 0;
    uint8_t filterH = 0;
    uint8_t dilationFilterW = 0;
    uint8_t dilationFilterH = 0;
    uint8_t jumpStride = 0;
    uint8_t repeatMode = 0;
    uint8_t repeatTime = 0;
    uint8_t cSize = 0;
    T padValue = 0;
    uint8_t fetchFilterW = 0;
    uint8_t fetchFilterH = 0;
    uint16_t l1H = 0;
    uint16_t l1W = 0;
    uint16_t c1Index = 0;
    int16_t leftTopW = 0;
    int16_t leftTopH = 0;
};

template <typename T>
struct LoadData3DParamsV2 {
    [aicore] LoadData3DParamsV2()
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = 0;
        }
        l1H = 0;
        l1W = 0;
        channelSize = 0;
        kExtension = 0;
        mExtension = 0;
        kStartPt = 0;
        mStartPt = 0;
        strideW = 1;
        strideH = 1;
        filterW = 1;
        filterH = 1;
        dilationFilterW = 1;
        dilationFilterH = 1;
        enTranspose = false;
        enSmallK = false;
        padValue = 0;
        filterSizeW = false;
        filterSizeH = false;
        fMatrixCtrl = false;
    }

    [aicore] LoadData3DParamsV2(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t channelSizeIn, const uint16_t kExtensionIn, const uint16_t mExtensionIn,
        const uint16_t kStartPtIn, const uint16_t mStartPtIn, const uint8_t strideWIn, const uint8_t strideHIn,
        const uint8_t filterWIn, const uint8_t filterHIn, const uint8_t dilationFilterWIn,
        const uint8_t dilationFilterHIn, const bool enTransposeIn, const bool enSmallKIn, const T padValueIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
        l1H = l1HIn;
        l1W = l1WIn;
        channelSize = channelSizeIn;
        kExtension = kExtensionIn;
        mExtension = mExtensionIn;
        kStartPt = kStartPtIn;
        mStartPt = mStartPtIn;
        strideW = strideWIn;
        strideH = strideHIn;
        filterW = filterWIn;
        filterH = filterHIn;
        dilationFilterW = dilationFilterWIn;
        dilationFilterH = dilationFilterHIn;
        enTranspose = enTransposeIn;
        enSmallK = enSmallKIn;
        padValue = padValueIn;
        filterSizeW = false;
        filterSizeH = false;
        fMatrixCtrl = false;
    }

    [aicore] LoadData3DParamsV2(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t channelSizeIn, const uint16_t kExtensionIn, const uint16_t mExtensionIn,
        const uint16_t kStartPtIn, const uint16_t mStartPtIn, const uint8_t strideWIn, const uint8_t strideHIn,
        const uint8_t filterWIn, const uint8_t filterHIn, const uint8_t dilationFilterWIn,
        const uint8_t dilationFilterHIn, const bool enTransposeIn, const bool enSmallKIn, const T padValueIn,
        const bool filterSizeWIn, const bool filterSizeHIn, const bool fMatrixCtrlIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
        l1H = l1HIn;
        l1W = l1WIn;
        channelSize = channelSizeIn;
        kExtension = kExtensionIn;
        mExtension = mExtensionIn;
        kStartPt = kStartPtIn;
        mStartPt = mStartPtIn;
        strideW = strideWIn;
        strideH = strideHIn;
        filterW = filterWIn;
        filterH = filterHIn;
        dilationFilterW = dilationFilterWIn;
        dilationFilterH = dilationFilterHIn;
        enTranspose = enTransposeIn;
        enSmallK = enSmallKIn;
        padValue = padValueIn;
        filterSizeW = filterSizeWIn;
        filterSizeH = filterSizeHIn;
        fMatrixCtrl = fMatrixCtrlIn;
    }

    uint8_t padList[PAD_SIZE] = {0};
    uint16_t l1H = 0;
    uint16_t l1W = 0;
    uint16_t channelSize = 0;
    uint16_t kExtension = 0;
    uint16_t mExtension = 0;
    uint16_t kStartPt = 0;
    uint16_t mStartPt = 0;

    uint8_t strideW = 1;
    uint8_t strideH = 1;
    uint8_t filterW = 1;
    uint8_t filterH = 1;
    uint8_t dilationFilterW = 1;
    uint8_t dilationFilterH = 1;
    bool enTranspose = false;
    bool enSmallK = false;
    T padValue = 0;
    bool filterSizeW = false;
    bool filterSizeH = false;
    bool fMatrixCtrl = false;
};

struct LoadData3DParamsV2Pro {
    [aicore] LoadData3DParamsV2Pro()
    {
        channelSize = 0;
        enTranspose = false;
        enSmallK = false;
        filterSizeW = false;
        filterSizeH = false;
        fMatrixCtrl = false;
        extConfig = 0;
        filterConfig = 0X10101010101;
    }

    [aicore] LoadData3DParamsV2Pro(const uint16_t channelSizeIn, const bool enTransposeIn, const bool enSmallKIn,
        const bool filterSizeWIn, const bool filterSizeHIn, const bool fMatrixCtrlIn, const uint64_t extConfigIn,
        const uint64_t filterConfigIn)
    {
        channelSize = channelSizeIn;
        enTranspose = enTransposeIn;
        enSmallK = enSmallKIn;
        filterSizeW = filterSizeWIn;
        filterSizeH = filterSizeHIn;
        fMatrixCtrl = fMatrixCtrlIn;
        extConfig = extConfigIn;
        filterConfig = filterConfigIn;
    }

    uint16_t channelSize = 0;
    bool enTranspose = false;
    bool enSmallK = false;
    bool filterSizeW = false;
    bool filterSizeH = false;
    bool fMatrixCtrl = false;
    uint64_t extConfig = 0;
    uint64_t filterConfig = 0X10101010101;
};

template <typename U>
struct InitConstValueParams {
    [aicore] InitConstValueParams()
    {
        repeatTimes = 0;
        blockNum = 0;
        dstGap = 0;
        initValue = 0;
    }

    [aicore] InitConstValueParams(const uint16_t repeatTimesIn,
        const uint16_t blockNumIn, const uint16_t dstGapIn, const U initValueIn)
    {
        repeatTimes = repeatTimesIn;
        blockNum = blockNumIn;
        dstGap = dstGapIn;
        initValue = initValueIn;
    }

    [aicore] InitConstValueParams(const uint16_t repeatTimesIn, const U initValueIn)
    {
        repeatTimes = repeatTimesIn;
        initValue = initValueIn;
    }

    uint16_t repeatTimes = 0;
    uint16_t blockNum = 0;
    uint16_t dstGap = 0;
    U initValue = 0;
};

struct LoadImageToLocalParams {
    [aicore] LoadImageToLocalParams()
    {
        horizSize = 0;
        vertSize = 0;
        horizStartPos = 0;
        vertStartPos = 0;
        srcHorizSize = 0;
        topPadSize = 0;
        botPadSize = 0;
        leftPadSize = 0;
        rightPadSize = 0;
    }

    [aicore] LoadImageToLocalParams(const uint16_t horizSizeIn, const uint16_t vertSizeIn,
        const uint16_t horizStartPosIn, const uint16_t vertStartPosIn, const uint16_t srcHorizSizeIn,
        const uint8_t topPadSizeIn, const uint8_t botPadSizeIn, const uint16_t leftPadSizeIn,
        const uint16_t rightPadSizeIn)
    {
        horizSize = horizSizeIn;
        vertSize = vertSizeIn;
        horizStartPos = horizStartPosIn;
        vertStartPos = vertStartPosIn;
        srcHorizSize = srcHorizSizeIn;
        topPadSize = topPadSizeIn;
        botPadSize = botPadSizeIn;
        leftPadSize = leftPadSizeIn;
        rightPadSize = rightPadSizeIn;
    }

    uint16_t horizSize = 0;
    uint16_t vertSize = 0;
    uint16_t horizStartPos = 0;
    uint16_t vertStartPos = 0;
    uint16_t srcHorizSize = 0;
    uint8_t topPadSize = 0;
    uint8_t botPadSize = 0;
    uint16_t leftPadSize = 0;
    uint16_t rightPadSize = 0;
    uint8_t sid = 0;
};
# 2188 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
struct LoadDataRepeatParam {
    [aicore] LoadDataRepeatParam()
    {
        repeatStride = 0;
        repeatTime = 1;
        repeatMode = 0;
        reserved = 0;
    }

    [aicore] LoadDataRepeatParam(const uint16_t repeatStrideIn, const uint8_t repeatTimeIn,
        const uint8_t repeatModeIn)
    {
        repeatStride = repeatStrideIn;
        repeatTime = repeatTimeIn;
        repeatMode = repeatModeIn;
    }

    uint16_t repeatStride = 0;
    uint8_t repeatTime = 1;
    uint8_t repeatMode = 0;
    uint8_t reserved = 0;
};


enum class LoopMode : uint8_t {
    MODE_NM = 0,
    MODE_MN = 1,
    MODE_KM = 2,
    MODE_KN = 3
};

struct GemmTiling {
    [aicore] GemmTiling()
    {
        mIterNum = 1;
        nIterNum = 1;
        kIterNum = 1;
        loopMode = LoopMode::MODE_NM;
    }

    const uint32_t blockSize = 16;
    LoopMode loopMode = LoopMode::MODE_NM;
    uint32_t mNum = 0;
    uint32_t nNum = 0;
    uint32_t kNum = 0;
    uint32_t roundM = 0;
    uint32_t roundN = 0;
    uint32_t roundK = 0;
    uint32_t c0Size = 32;
    uint32_t dtypeSize = 1;
    uint32_t mBlockNum = 0;
    uint32_t nBlockNum = 0;
    uint32_t kBlockNum = 0;
    uint32_t mIterNum = 0;
    uint32_t nIterNum = 0;
    uint32_t kIterNum = 0;
    uint32_t mTileBlock = 0;
    uint32_t nTileBlock = 0;
    uint32_t kTileBlock = 0;
    uint32_t kTailBlock = 0;
    uint32_t mTailBlock = 0;
    uint32_t nTailBlock = 0;
    bool kHasTail = false;
    bool mHasTail = false;
    bool nHasTail = false;
    bool kHasTailEle = false;
    uint32_t kTailEle = 0;
};

struct Conv2dParams {
    [aicore] Conv2dParams() {}

    [aicore] Conv2dParams(const uint32_t imgShapeIn[CONV2D_IMG_SIZE],
        const uint32_t kernelShapeIn[CONV2D_KERNEL_SIZE], const uint32_t strideIn[CONV2D_STRIDE], const uint32_t cinIn,
        const uint32_t coutIn, const uint32_t padListIn[CONV2D_PAD], const uint32_t dilationIn[CONV2D_DILATION],
        const uint32_t initYIn, const bool partialSumIn)
    {
        for (int32_t i = 0; i < CONV2D_IMG_SIZE; ++i) {
            imgShape[i] = imgShapeIn[i];
        }
        for (int32_t i = 0; i < CONV2D_KERNEL_SIZE; ++i) {
            kernelShape[i] = kernelShapeIn[i];
        }
        for (int32_t i = 0; i < CONV2D_STRIDE; ++i) {
            stride[i] = strideIn[i];
        }
        cin = cinIn;
        cout = coutIn;
        for (int32_t i = 0; i < CONV2D_PAD; ++i) {
            padList[i] = padListIn[i];
        }
        for (int32_t i = 0; i < CONV2D_DILATION; ++i) {
            dilation[i] = dilationIn[i];
        }
        initY = initYIn;
        partialSum = partialSumIn;
    }

    uint32_t imgShape[CONV2D_IMG_SIZE] = { 0 };
    uint32_t kernelShape[CONV2D_KERNEL_SIZE] = { 0 };
    uint32_t stride[CONV2D_STRIDE] = { 0 };
    uint32_t cin = 0;
    uint32_t cout = 0;
    uint32_t padList[CONV2D_PAD] = { 0 };
    uint32_t dilation[CONV2D_DILATION] = { 0 };
    uint32_t initY = 0;
    bool partialSum = false;
};

struct Conv2dTilling {
    const uint32_t blockSize = 16;
    LoopMode loopMode = LoopMode::MODE_NM;

    uint32_t c0Size = 32;
    uint32_t dTypeSize = 1;

    uint32_t strideH = 0;
    uint32_t strideW = 0;
    uint32_t dilationH = 0;
    uint32_t dilationW = 0;
    uint32_t hi = 0;
    uint32_t wi = 0;
    uint32_t ho = 0;
    uint32_t wo = 0;

    uint32_t height = 0;
    uint32_t width = 0;

    uint32_t howo = 0;

    uint32_t mNum = 0;
    uint32_t nNum = 0;
    uint32_t kNum = 0;

    uint32_t mBlockNum = 0;
    uint32_t kBlockNum = 0;
    uint32_t nBlockNum = 0;

    uint32_t roundM = 0;
    uint32_t roundN = 0;
    uint32_t roundK = 0;

    uint32_t mTileBlock = 0;
    uint32_t nTileBlock = 0;
    uint32_t kTileBlock = 0;

    uint32_t mIterNum = 0;
    uint32_t nIterNum = 0;
    uint32_t kIterNum = 0;

    uint32_t mTileNums = 0;

    bool mHasTail = false;
    bool nHasTail = false;
    bool kHasTail = false;

    uint32_t kTailBlock = 0;
    uint32_t mTailBlock = 0;
    uint32_t nTailBlock = 0;

    uint32_t mTailNums = 0;
};

struct MmadParams {
    [aicore] MmadParams()
    {
        m = 0;
        n = 0;
        k = 0;
        unitFlag = 0;
        kDirectionAlign = false;
        cmatrixSource = false;
        cmatrixInitVal = true;
    }
    [aicore] MmadParams(const uint16_t mIn, const uint16_t nIn, const uint16_t kIn, const bool isBiasIn,
        const int32_t fmOffsetIn, const bool enSsparseIn, const bool enWinogradAIn, const bool enWinogradBIn)
    {
        m = mIn;
        n = nIn;
        k = kIn;
        isBias = isBiasIn;
        fmOffset = fmOffsetIn;
        enSsparse = enSsparseIn;
        enWinogradA = enWinogradAIn;
        enWinogradB = enWinogradBIn;
    }

    [aicore] MmadParams(const uint16_t mIn, const uint16_t nIn, const uint16_t kIn, const uint8_t unitFlagIn,
        const bool cmatrixSourceIn, const bool cmatrixInitValIn)
    {
        m = mIn;
        n = nIn;
        k = kIn;
        unitFlag = unitFlagIn;
        cmatrixSource = cmatrixSourceIn;
        cmatrixInitVal = cmatrixInitValIn;
    }

    uint16_t m = 0;
    uint16_t n = 0;
    uint16_t k = 0;


    bool isBias = false;

    int32_t fmOffset = 0;

    bool enSsparse = false;

    bool enWinogradA = false;

    bool enWinogradB = false;
    uint8_t unitFlag = 0;
    bool kDirectionAlign = false;

    bool cmatrixSource = false;

    bool cmatrixInitVal = true;
};


struct MatMulInfo {
    const uint16_t m{ 0 };
    const uint16_t n{ 0 };
    const uint16_t k{ 0 };
    const bool isInitOut{ false };
    const bool isBias{ false };
};

struct DropOutShapeInfo {
    [aicore] DropOutShapeInfo(){};
    uint32_t firstAxis = 0;
    uint32_t srcLastAxis = 0;
    uint32_t maskLastAxis = 0;
};

struct SelectWithBytesMaskShapeInfo {
    [aicore] SelectWithBytesMaskShapeInfo(){};
    uint32_t firstAxis = 0;
    uint32_t srcLastAxis = 0;
    uint32_t maskLastAxis = 0;
};

template <typename T> class LocalTensor;
template <typename T> class GlobalTensor;

template <typename T> struct LayerNormParams {
    [aicore] LayerNormParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct BatchNormParams {
    [aicore] BatchNormParams(){};
    float firstDimValueBack = 1.0;
    uint8_t srcRepeatStride = 1;
    uint32_t srcOffset = 1;
    uint32_t basicLoop = 0;
    uint32_t brcRepeatTimes = 0;
    uint32_t oriBloop = 0;
    uint32_t oriBTail = 0;
    uint32_t oriBTmpLoopOffset = 0;
    uint32_t oriBTmpTailOffset = 0;
    uint32_t oriBOutLoopOffset = 0;
    uint32_t oriBOutTailOffset = 0;
    uint32_t reduceAddLoop = 0;
    uint32_t reduceAddTail = 0;
    uint32_t reduceAddTailOffset = 0;

    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct DeepNormParams {
    [aicore] DeepNormParams(){};
    float lastDimValueBack = 1.0;

    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct ExpParams {
    [aicore] ExpParams() {};
    uint32_t inputSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t fourthTmpStartPos = 0;
    uint32_t loopNum = 0;
    uint32_t tailSize = 0;
    uint32_t tailPos = 0;
    uint32_t curDataLength = 0;
    uint32_t expandLevel = 0;

    LocalTensor<T> tempTensorFloorX;
    LocalTensor<T> tempTensorFloorXPow;
    LocalTensor<T> tempTensorRes;
    LocalTensor<T> tempTensorIntPart;
};

template <typename T> struct AntiquantParams {
    [aicore] AntiquantParams() {};
    LocalTensor<T> tempTensorOffset;
    LocalTensor<T> tempTensorScale;
    LocalTensor<T> tempTensorInput;
};

template <typename T, typename U>struct DropOutParams {
    [aicore] DropOutParams() {};
    uint32_t dataSize = 0;
    uint32_t stackBufferSize = 0;
    uint32_t repeatTimes = 1;

    uint32_t maxRepeatSize = 0;
    uint32_t oneRepeatSize = 0;

    uint32_t currentSize = 0;
    uint32_t repeatRounding = 0;
    uint32_t repeatRemaining = 0;
    uint32_t repeatTail = 0;

    LocalTensor<T> firstLocal;
    LocalTensor<U> secondLocal;
};

template <typename T, typename U> struct PowerFParams {
    [aicore] PowerFParams(){};
    LocalTensor<T> tmpTensor1;
    LocalTensor<T> tmpTensor2;
    LocalTensor<T> tmpTensor3;
    LocalTensor<U> tmpMask1;
    LocalTensor<U> tmpMask2;
    LocalTensor<U> tmpMask3;
    LocalTensor<U> finiteIntegerYMask;
};

template <typename T, typename U> struct PowerIParams {
    [aicore] PowerIParams(){};
    float expIterateSum;

    LocalTensor<T> expUBIterate;
    LocalTensor<T> oriAbsExp;
    LocalTensor<T> recordExpNode;
    LocalTensor<T> tmpTensor1;
    LocalTensor<T> tmpTensor2;
    LocalTensor<U> negMask;
    LocalTensor<U> mask;
    LocalTensor<U> tmpScalar;
};

template <typename T> struct GeluParams {
    [aicore] GeluParams(){};
    uint32_t repeatTimes = 1;

    uint32_t currentSize = 0;
    uint32_t repeatRounding = 0;
    uint32_t repeatRemaining = 0;
    uint32_t tail = 0;

    uint32_t maxRepeatSize = 0;
    uint32_t oneRepeatSize = 0;

    uint32_t dataSize = 0;
    uint32_t stackSize = 0;
    uint32_t tmpBufferSize = 0;
    LocalTensor<T> sharedTmpBuffer;

    LocalTensor<T> tempTensorConv;
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
};

template <typename T> struct TanhParams {
    [aicore] TanhParams(){};
    uint32_t repeatTimes = 1;
    uint32_t calCount = 0;
    uint32_t stackSize = 0;
    uint32_t tmpBufferSize = 0;
    LocalTensor<T> sharedTmpBuffer;

    LocalTensor<T> tempTensorConv;
    LocalTensor<T> tmpClip;
};

template <typename T> struct AscendDequantParams {
    [aicore] AscendDequantParams(){};
    uint64_t tmpSize;

    LocalTensor<T> tmpAddrA;
    LocalTensor<T> tmpAddrB;
};

template <typename T> struct MrgSortSrcList {
    [aicore] MrgSortSrcList() {}

    [aicore] MrgSortSrcList(const LocalTensor<T>& src1In, const LocalTensor<T>& src2In, const LocalTensor<T>& src3In,
        const LocalTensor<T>& src4In)
    {
        src1 = src1In[0];
        src2 = src2In[0];
        src3 = src3In[0];
        src4 = src4In[0];
    }

    LocalTensor<T> src1;
    LocalTensor<T> src2;
    LocalTensor<T> src3;
    LocalTensor<T> src4;
};

struct MrgSort4Info {
    [aicore] MrgSort4Info() {}
    [aicore] MrgSort4Info(const uint16_t elementLengthsIn[MRG_SORT_ELEMENT_LEN], const bool ifExhaustedSuspensionIn,
        const uint16_t validBitIn, const uint16_t repeatTimesIn)
    {
        for (int32_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
            elementLengths[i] = elementLengthsIn[i];
        }
        ifExhaustedSuspension = ifExhaustedSuspensionIn;
        validBit = validBitIn;
        repeatTimes = repeatTimesIn;
    }

    uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = { 0 };
    bool ifExhaustedSuspension = false;
    uint16_t validBit = 0;
    uint8_t repeatTimes = 1;
};

struct DataCopyParams {
    [aicore] DataCopyParams()
    {
        blockCount = DEFAULT_DATA_COPY_NBURST;
        blockLen = 0;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        dstStride = DEFAULT_DATA_COPY_STRIDE;
    }

    [aicore] DataCopyParams(const uint16_t count, const uint16_t len, const uint16_t srcStrideIn,
        const uint16_t dstStrideIn)
    {
        blockCount = count;
        blockLen = len;
        srcStride = srcStrideIn;
        dstStride = dstStrideIn;
    }

    uint16_t blockCount = 0;
    uint16_t blockLen = 0;
    uint16_t srcStride = 0;
    uint16_t dstStride = 0;
};

struct SliceInfo {
    [aicore] SliceInfo()
    {
        startIndex = 0;
        endIndex = ONE_BLK_SIZE - 1;
        stride = 0;
        burstLen = ONE_BLK_SIZE;
        shapeValue = 0;
    }

    [aicore] SliceInfo(const uint32_t startIndexIn, const uint32_t endIndexIn, const uint32_t strideIn,
        const uint32_t burstLenIn, const uint32_t shapeValueIn = 0)
    {
        startIndex = startIndexIn;
        endIndex = endIndexIn;
        stride = strideIn;
        burstLen = burstLenIn;
        shapeValue = shapeValueIn;
    }

    uint32_t startIndex = 0;
    uint32_t endIndex = 0;
    uint32_t stride = 0;
    uint32_t burstLen = 0;
    uint32_t shapeValue = 0;
};

template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t GetScalarBitcodeValue(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        uint64_t output;
    } data;

    data.input = scalarValue;
    return data.output;
}

template <typename T, typename U> [aicore] __inline__ __attribute__((always_inline)) U GetScalarBitcodeValue(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        U output;
    } data;

    data.input = scalarValue;
    return static_cast<U>(data.output);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) half GetScalarBitcodeToHalf(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        half output;
    } data;

    data.input = scalarValue;
    return data.output;
}

struct DataCopyExtParams {
    [aicore] DataCopyExtParams()
    {
        blockCount = DEFAULT_DATA_COPY_NBURST;
        blockLen = 0;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        dstStride = DEFAULT_DATA_COPY_STRIDE;
        rsv = 0;
    }

    [aicore] DataCopyExtParams(const uint16_t count, const uint32_t len, const uint32_t srcStrideIn,
        const uint32_t dstStrideIn, const uint32_t rsvIn)
    {
        blockCount = count;
        blockLen = len;
        srcStride = srcStrideIn;
        dstStride = dstStrideIn;
        rsv = rsvIn;
    }

    uint16_t blockCount = 0;
    uint32_t blockLen = 0;
    uint32_t srcStride = 0;
    uint32_t dstStride = 0;
    uint32_t rsv = 0;
};

template <typename T> struct DataCopyPadExtParams {
    [aicore] DataCopyPadExtParams()
    {
        isPad = false;
        leftPadding = 0;
        rightPadding = 0;
        paddingValue = 0;
    }
    [aicore] DataCopyPadExtParams(const bool isPadValue, const uint8_t leftPadValue, const uint8_t rightPadValue,
        T padValue)
    {
        isPad = isPadValue;
        leftPadding = leftPadValue;
        rightPadding = rightPadValue;
        paddingValue = padValue;
    }
    bool isPad = false;
    uint8_t leftPadding = 0;
    uint8_t rightPadding = 0;
    T paddingValue = 0;
};

struct DataCopyPadParams {
    [aicore] DataCopyPadParams()
    {
        isPad = false;
        leftPadding = 0;
        rightPadding = 0;
        paddingValue = 0;
    }
    [aicore] DataCopyPadParams(const bool isPadValue, const uint8_t leftPadValue, const uint8_t rightPadValue,
        const uint64_t padValue)
    {
        isPad = isPadValue;
        leftPadding = leftPadValue;
        rightPadding = rightPadValue;
        paddingValue = padValue;
    }
    bool isPad = false;
    uint8_t leftPadding = 0;
    uint8_t rightPadding = 0;
    uint64_t paddingValue = 0;
};

struct Nd2NzParams {
    [aicore] Nd2NzParams()
    {
        ndNum = 0;
        nValue = 0;
        dValue = 0;
        srcNdMatrixStride = 0;
        srcDValue = 0;
        dstNzC0Stride = 0;
        dstNzNStride = 0;
        dstNzMatrixStride = 0;
    }

    [aicore] Nd2NzParams(const uint16_t ndNumIn, const uint16_t nValueIn, const uint16_t dValueIn,
        const uint16_t srcNdMatrixStrideIn, const uint16_t srcDValueIn, const uint16_t dstNzC0StrideIn,
        const uint16_t dstNzNStrideIn, const uint16_t dstNzMatrixStrideIn)
    {
        ndNum = ndNumIn;
        nValue = nValueIn;
        dValue = dValueIn;
        srcNdMatrixStride = srcNdMatrixStrideIn;
        srcDValue = srcDValueIn;
        dstNzC0Stride = dstNzC0StrideIn;
        dstNzNStride = dstNzNStrideIn;
        dstNzMatrixStride = dstNzMatrixStrideIn;
    }

    uint16_t ndNum = 0;
    uint16_t nValue = 0;
    uint16_t dValue = 0;
    uint16_t srcNdMatrixStride = 0;
    uint16_t srcDValue = 0;
    uint16_t dstNzC0Stride = 0;
    uint16_t dstNzNStride = 0;
    uint16_t dstNzMatrixStride = 0;
};

struct Nz2NdParamsFull {
    [aicore] Nz2NdParamsFull()
    {
        ndNum = 1;
        nValue = 0;
        dValue = 0;
        srcNdMatrixStride = 1;
        srcNStride = 0;
        dstDStride = 0;
        dstNdMatrixStride = 1;
    }

    [aicore] Nz2NdParamsFull(const uint16_t ndNumIn, const uint16_t nValueIn, const uint16_t dValueIn,
        const uint16_t srcNdMatrixStrideIn, const uint16_t srcNStrideIn, const uint16_t dstDStrideIn,
        const uint16_t dstNdMatrixStrideIn)
    {
        ndNum = ndNumIn;
        nValue = nValueIn;
        dValue = dValueIn;
        srcNdMatrixStride = srcNdMatrixStrideIn;
        srcNStride = srcNStrideIn;
        dstDStride = dstDStrideIn;
        dstNdMatrixStride = dstNdMatrixStrideIn;
    }

    uint16_t ndNum = 1;
    uint16_t nValue = 0;
    uint16_t dValue = 0;
    uint16_t srcNdMatrixStride = 1;
    uint16_t srcNStride = 0;
    uint16_t dstDStride = 0;
    uint16_t dstNdMatrixStride = 1;
};

struct CopyRepeatParams {
    [aicore] CopyRepeatParams()
    {
        dstStride = DEFAULT_DATA_COPY_STRIDE;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        dstRepeatSize = DEFAULT_REPEAT_STRIDE;
        srcRepeatSize = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] CopyRepeatParams(const uint16_t dstStrideIn, const uint16_t srcStrideIn, uint16_t dstRepeatSizeIn,
        uint16_t srcRepeatSizeIn)
    {
        dstStride = dstStrideIn;
        srcStride = srcStrideIn;
        dstRepeatSize = dstRepeatSizeIn;
        srcRepeatSize = srcRepeatSizeIn;
    }

    uint16_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t dstRepeatSize = DEFAULT_REPEAT_STRIDE;
    uint16_t srcRepeatSize = DEFAULT_REPEAT_STRIDE;
};

struct DataCopyEnhancedParams {
    [aicore] DataCopyEnhancedParams()
    {
        blockMode = BlockMode::BLOCK_MODE_NORMAL;
        deqScale = DeqScale::DEQ_NONE;
        deqValue = 0;
        sidStoreMode = 0;
        isRelu = false;
        padMode = pad_t::PAD_NONE;
        padValue = 0;
    }

    [aicore] DataCopyEnhancedParams(const BlockMode blockModeIn, const DeqScale deqScaleIn, const uint64_t deqValueIn,
        const uint8_t sidStoreModeIn, const bool isReluIn, const pad_t padModeIn, const uint64_t padValueIn)
    {
        blockMode = blockModeIn;
        deqScale = deqScaleIn;
        deqValue = deqValueIn;
        sidStoreMode = sidStoreModeIn;
        isRelu = isReluIn;
        padMode = padModeIn;
        padValue = padValueIn;
    }

    BlockMode blockMode = BlockMode::BLOCK_MODE_NORMAL;
    DeqScale deqScale = DeqScale::DEQ_NONE;
    uint64_t deqValue = 0;
    uint8_t sidStoreMode = 0;
    bool isRelu = false;
    pad_t padMode = pad_t::PAD_NONE;
    uint64_t padValue = 0;
    uint64_t deqTensorAddr = 0;
};

struct DataCopyCO12DstParams {
    [aicore] DataCopyCO12DstParams()
    {
        sid = 0;
        nSize = 0;
        mSize = 0;
        dstStride = DEFAULT_DATA_COPY_STRIDE;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        unitFlag = 0;
        quantPre = QuantMode_t::NoQuant;
        reluPre = 0;
        channelSplit = false;
        nz2ndEn = false;
    }

    [aicore] DataCopyCO12DstParams(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint32_t dstStrideIn,
        const uint16_t srcStrideIn, const QuantMode_t quantPreIn, const uint8_t reluPreIn, const bool channelSplitIn,
        const bool nz2ndEnIn)
    {
        nSize = nSizeIn;
        mSize = mSizeIn;
        dstStride = dstStrideIn;
        srcStride = srcStrideIn;
        quantPre = quantPreIn;
        reluPre = reluPreIn;
        channelSplit = channelSplitIn;
        nz2ndEn = nz2ndEnIn;
    }

    uint8_t sid = 0;
    uint16_t nSize = 0;
    uint16_t mSize = 0;
    uint32_t dstStride = 0;
    uint16_t srcStride = 0;
    uint8_t unitFlag = 0;
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint8_t reluPre = 0;
    bool channelSplit = false;
    bool nz2ndEn = false;
};

struct QuantParams {
    [aicore] QuantParams() {}
    [aicore] QuantParams(const QuantMode_t quantPreIn) : quantPre(quantPreIn) {}
    [aicore] QuantParams(const QuantMode_t quantPreIn, const uint64_t deqScalarIn)
        : quantPre(quantPreIn), deqScalar(deqScalarIn) {}
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint64_t deqScalar;
};

struct Nz2NdParams {
    [aicore] Nz2NdParams()
    {
        nz2ndEn = false;
        ndNum = 1;
        srcNdStride = 0;
        dstNdStride = 0;
        originalNSize = 0;
    }

    [aicore] Nz2NdParams(const bool nz2ndEnIn, const uint16_t ndNumIn, const uint16_t srcNdStrideIn,
        const uint16_t dstNdStrideIn, const uint16_t originalNSizeIn)
    {
        nz2ndEn = nz2ndEnIn;
        ndNum = ndNumIn;
        srcNdStride = srcNdStrideIn;
        dstNdStride = dstNdStrideIn;
        originalNSize = originalNSizeIn;
    }

    bool nz2ndEn = false;
    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;
    uint16_t originalNSize = 0;
};
enum class CO2Layout : uint8_t {
    NZ = 0,
    ROW_MAJOR,
    COLUMN_MAJOR
};
struct FixpipeConfig {
    CO2Layout format;
};

constexpr FixpipeConfig CFG_NZ = {CO2Layout::NZ};
constexpr FixpipeConfig CFG_ROW_MAJOR = {CO2Layout::ROW_MAJOR};
constexpr FixpipeConfig CFG_COLUMN_MAJOR = {CO2Layout::COLUMN_MAJOR};


template <typename src_T = int32_t>
struct FixpipeParams {
    [aicore] FixpipeParams()
    {
        cburstNum = DEFAULT_DATA_COPY_NBURST;
        burstLen = 1;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        dstStride = DEFAULT_DATA_COPY_STRIDE;
        reluEn = false;
        unitFlag = 0;
    }

    [aicore] FixpipeParams(const uint16_t count, const uint16_t len, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn)
    {
        cburstNum = count;
        burstLen = len;
        dstStride = dstStrideIn;
        srcStride = srcStrideIn;
    }

    uint16_t cburstNum = 0;
    uint16_t burstLen = 0;
    uint32_t dstStride = 0;
    uint16_t srcStride = 0;

    QuantParams quantParams;
    bool reluEn = false;
    Nz2NdParams nz2ndParams;
    uint8_t unitFlag = 0;
};

struct FixpipeParamsV220 {
    [aicore] FixpipeParamsV220()
    {
        nSize = 0;
        mSize = 0;
        dstStride = 0;
        srcStride = 0;
        reluEn = false;
        unitFlag = 0;
    }

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn)
    {
        nSize = nSizeIn;
        mSize = mSizeIn;
        srcStride = srcStrideIn;
        dstStride = dstStrideIn;
        reluEn = reluEnIn;
    }

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn, const QuantMode_t quantPreIn, const int64_t deqScalarIn,
        const uint16_t ndNumIn, const uint16_t srcNdStrideIn, const uint16_t dstNdStrideIn, const uint8_t unitFlagIn)
    {
        nSize = nSizeIn;
        mSize = mSizeIn;
        srcStride = srcStrideIn;
        dstStride = dstStrideIn;
        reluEn = reluEnIn;
        quantPre = quantPreIn;
        deqScalar = deqScalarIn;
        ndNum = ndNumIn;
        srcNdStride = srcNdStrideIn;
        dstNdStride = dstNdStrideIn;
        unitFlag = unitFlagIn;
    }

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn, const QuantMode_t quantPreIn, const int64_t deqScalarIn,
        const uint16_t ndNumIn, const uint16_t srcNdStrideIn, const uint16_t dstNdStrideIn, const uint8_t unitFlagIn,
        const bool isChannelSplitIn)
    {
        nSize = nSizeIn;
        mSize = mSizeIn;
        srcStride = srcStrideIn;
        dstStride = dstStrideIn;
        reluEn = reluEnIn;
        quantPre = quantPreIn;
        deqScalar = deqScalarIn;
        ndNum = ndNumIn;
        srcNdStride = srcNdStrideIn;
        dstNdStride = dstNdStrideIn;
        unitFlag = unitFlagIn;
        isChannelSplit = isChannelSplitIn;
    }

    uint16_t nSize = 0;
    uint16_t mSize = 0;
    uint16_t srcStride = 0;
    uint32_t dstStride = 0;

    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint64_t deqScalar;

    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;
    bool reluEn = false;
    uint8_t unitFlag = 0;
    bool isChannelSplit = false;
};

using FixpipeParamsM300 = FixpipeParamsV220;

struct TransDataTo5HDParams {
    [aicore] TransDataTo5HDParams()
    {
        dstHighHalf = false;
        srcHighHalf = false;
        repeatTimes = 1;
        dstRepStride = 0;
        srcRepStride = 0;
    }

    [aicore] TransDataTo5HDParams(const bool dstHighHalfIn, const bool srcHighHalfIn, const uint8_t repeatTimesIn,
        const uint16_t dstRepStrideIn, const uint16_t srcRepStrideIn)
    {
        dstHighHalf = dstHighHalfIn;
        srcHighHalf = srcHighHalfIn;
        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    bool dstHighHalf = false;
    bool srcHighHalf = false;
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
};

enum class TransposeType : uint8_t {

    TRANSPOSE_TYPE_NONE,


    TRANSPOSE_NZ2ND_0213,


    TRANSPOSE_NZ2NZ_0213,


    TRANSPOSE_NZ2NZ_012_WITH_N,


    TRANSPOSE_NZ2ND_012_WITH_N,

    TRANSPOSE_NZ2ND_012_WITHOUT_N,


    TRANSPOSE_NZ2NZ_012_WITHOUT_N,
    TRANSPOSE_ND2ND_ONLY,
    TRANSPOSE_ND_UB_GM,
    TRANSPOSE_GRAD_ND_UB_GM,
    TRANSPOSE_ND2ND_B16,
    TRANSPOSE_NCHW2NHWC,
    TRANSPOSE_NHWC2NCHW
};

struct ConfusionTranspose0213Tiling {
    [aicore] ConfusionTranspose0213Tiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeA1 = 0;
        alignA3 = 0;
        alignA2 = 0;
        widthTiling = 0;
        newPopSize = 0;
        newPopH = 0;
        needSize = 0;
        mainBlocks = 0;
        tailSize = 0;
        alignA2MulAlignA3 = 0;
        batchOffset = 0;
        alignA3MulA1 = 0;
        shapeA1BlockCube = 0;
        mainOffset = 0;
    }

    [aicore] ConfusionTranspose0213Tiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeA1In,
        uint32_t alignA3In, uint32_t alignA2In, uint32_t widthTilingIn, uint32_t newPopSizeIn, uint32_t newPopHIn,
        uint32_t needSizeIn, uint32_t mainBlocksIn, uint32_t tailSizeIn, uint32_t alignA2MulAlignA3In,
        uint32_t batchOffsetIn, uint32_t alignA3MulA1In, uint32_t shapeA1BlockCubeIn, uint32_t mainOffsetIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeA1 = shapeA1In;
        alignA3 = alignA3In;
        alignA2 = alignA2In;
        widthTiling = widthTilingIn;
        newPopSize = newPopSizeIn;
        newPopH = newPopHIn;
        needSize = needSizeIn;
        mainBlocks = mainBlocksIn;
        tailSize = tailSizeIn;
        alignA2MulAlignA3 = alignA2MulAlignA3In;
        batchOffset = batchOffsetIn;
        alignA3MulA1 = alignA3MulA1In;
        shapeA1BlockCube = shapeA1BlockCubeIn;
        mainOffset = mainOffsetIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeA1 = 0;
    uint32_t alignA3 = 0;
    uint32_t alignA2 = 0;
    uint32_t widthTiling = 0;
    uint32_t newPopSize = 0;
    uint32_t newPopH = 0;
    uint32_t needSize = 0;
    uint32_t mainBlocks = 0;
    uint32_t tailSize = 0;
    uint32_t alignA2MulAlignA3 = 0;
    uint32_t batchOffset = 0;
    uint32_t alignA3MulA1 = 0;
    uint32_t shapeA1BlockCube = 0;
    uint32_t mainOffset = 0;
};

struct ConfusionTranspose2NZ012NTiling {
    [aicore] ConfusionTranspose2NZ012NTiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        blockNum = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        alignH = 0;
        alignS = 0;
        hnDivBlockNum = 0;
        alignHnDiv = 0;
        gap = 0;
        alignsBlockCube = 0;
        prehBlockNum = 0;
        dstBatchOffset = 0;
        srcBatchOffset = 0;
    }

    [aicore] ConfusionTranspose2NZ012NTiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn,
        uint32_t hnDivIn, uint32_t blockNumIn, uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn,
        uint32_t alignHIn, uint32_t alignSIn, uint32_t hnDivBlockNumIn, uint32_t alignHnDivIn, uint32_t gapIn,
        uint32_t alignsBlockCubeIn, uint32_t prehBlockNumIn, uint32_t dstBatchOffsetIn, uint32_t srcBatchOffsetIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        blockNum = blockNumIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        alignH = alignHIn;
        alignS = alignSIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignHnDiv = alignHnDivIn;
        gap = gapIn;
        alignsBlockCube = alignsBlockCubeIn;
        prehBlockNum = prehBlockNumIn;
        dstBatchOffset = dstBatchOffsetIn;
        srcBatchOffset = srcBatchOffsetIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t blockNum = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t alignH = 0;
    uint32_t alignS = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignHnDiv = 0;
    uint32_t gap = 0;
    uint32_t alignsBlockCube = 0;
    uint32_t prehBlockNum = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t srcBatchOffset = 0;
};
struct ConfusionTranspose2ND012NTiling {
    [aicore] ConfusionTranspose2ND012NTiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        hnDivBlockNum = 0;
        alignHnDiv = 0;
        gap = 0;
        alignsCube = 0;
        prehBlockNum = 0;
        alignsMulAlignHnDiv = 0;
        alignHnDivCube = 0;
        alignHnDivBlockSize = 0;
        dstBatchOffset = 0;
        srcBatchOffset = 0;
        blockNum = 0;
    }

    [aicore] ConfusionTranspose2ND012NTiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn,
        uint32_t hnDivIn, uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn, uint32_t hnDivBlockNumIn,
        uint32_t alignHnDivIn, uint32_t gapIn, uint32_t alignsCubeIn, uint32_t prehBlockNumIn,
        uint32_t alignsMulAlignHnDivIn, uint32_t alignHnDivCubeIn, uint32_t alignHnDivBlockSizeIn,
        uint32_t dstBatchOffsetIn, uint32_t srcBatchOffsetIn, uint32_t blockNumIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignHnDiv = alignHnDivIn;
        gap = gapIn;
        alignsCube = alignsCubeIn;
        prehBlockNum = prehBlockNumIn;
        alignsMulAlignHnDiv = alignsMulAlignHnDivIn;
        alignHnDivCube = alignHnDivCubeIn;
        alignHnDivBlockSize = alignHnDivBlockSizeIn;
        dstBatchOffset = dstBatchOffsetIn;
        srcBatchOffset = srcBatchOffsetIn;
        blockNum = blockNumIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignHnDiv = 0;
    uint32_t gap = 0;
    uint32_t alignsCube = 0;
    uint32_t prehBlockNum = 0;
    uint32_t alignsMulAlignHnDiv = 0;
    uint32_t alignHnDivCube = 0;
    uint32_t alignHnDivBlockSize = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t srcBatchOffset = 0;
    uint32_t blockNum = 0;
};

struct ConfusionTranspose012Tiling {
    [aicore] ConfusionTranspose012Tiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        hnDivBlockNum = 0;
        alignH = 0;
        alignsCube = 0;
        alignhBlockCube = 0;
        blockSizeMulAlignH = 0;
        srcBatchOffset = 0;
        dstBatchOffset = 0;
        blockNum = 0;
    }

    [aicore] ConfusionTranspose012Tiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn, uint32_t hnDivIn,
        uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn, uint32_t hnDivBlockNumIn, uint32_t alignHIn,
        uint32_t alignsCubeIn, uint32_t alignhBlockCubeIn, uint32_t blockSizeMulAlignHIn, uint32_t srcBatchOffsetIn,
        uint32_t dstBatchOffsetIn, uint32_t blockNumIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignH = alignHIn;
        alignsCube = alignsCubeIn;
        alignhBlockCube = alignhBlockCubeIn;
        blockSizeMulAlignH = blockSizeMulAlignHIn;
        srcBatchOffset = srcBatchOffsetIn;
        dstBatchOffset = dstBatchOffsetIn;
        blockNum = blockNumIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignH = 0;
    uint32_t alignsCube = 0;
    uint32_t alignhBlockCube = 0;
    uint32_t blockSizeMulAlignH = 0;
    uint32_t srcBatchOffset = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t blockNum = 0;
};
struct ConfusionTransposeOnlyTiling {
    [aicore] ConfusionTransposeOnlyTiling()
    {
        blockSize = 0;
        height = 0;
        width = 0;
        highBlock = 0;
        stride = 0;
        repeat = 0;
    }

    [aicore] ConfusionTransposeOnlyTiling(uint32_t blockSizeIn, uint32_t heightIn, uint32_t widthIn,
        uint32_t highBlockIn, uint32_t strideIn, uint32_t repeatIn)
    {
        blockSize = blockSizeIn;
        height = heightIn;
        width = widthIn;
        highBlock = highBlockIn;
        stride = strideIn;
        repeat = repeatIn;
    }
    uint32_t blockSize = 0;
    uint32_t height = 0;
    uint32_t width = 0;
    uint32_t highBlock = 0;
    uint32_t stride = 0;
    uint32_t repeat = 0;
};

struct TransposeParamsExt {
    [aicore] TransposeParamsExt()
    {
        nSize = 0;
        cSize = 0;
        hSize = 0;
        wSize = 0;
        transposeType = TransposeType::TRANSPOSE_ND2ND_B16;
    }

    [aicore] TransposeParamsExt(const uint16_t nSizeIn, const uint16_t cSizeIn, const uint16_t hSizeIn,
        const uint16_t wSizeIn, const TransposeType transposeTypeIn)
    {
        nSize = nSizeIn;
        cSize = cSizeIn;
        hSize = hSizeIn;
        wSize = wSizeIn;
        transposeType = transposeTypeIn;
    }

    uint16_t nSize = 0;
    uint16_t cSize = 0;
    uint16_t hSize = 0;
    uint16_t wSize = 0;
    TransposeType transposeType = TransposeType::TRANSPOSE_ND2ND_B16;
};

struct GatherMaskParams {
    [aicore] GatherMaskParams()
    {
        src0BlockStride = DEFAULT_BLK_STRIDE;
        repeatTimes = 0;
        src0RepeatStride = DEFAULT_REPEAT_STRIDE;
        src1RepeatStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] GatherMaskParams(const uint8_t src0BlockStrideIn, const uint16_t repeatTimesIn,
        const uint16_t src0RepeatStrideIn, const uint8_t src1RepeatStrideIn)
    {
        src0BlockStride = src0BlockStrideIn;
        repeatTimes = repeatTimesIn;
        src0RepeatStride = src0RepeatStrideIn;
        src1RepeatStride = src1RepeatStrideIn;
    }

    uint8_t src0BlockStride = DEFAULT_BLK_STRIDE;
    uint16_t repeatTimes = 0;
    uint16_t src0RepeatStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepeatStride = DEFAULT_REPEAT_STRIDE;
};

struct IntriInfo {
    uint32_t c0Count{ 0 };
    uint32_t repeat{ 0 };
    uint32_t repeatRounding{ 0 };
    uint32_t repeatRemaining{ 0 };
    uint32_t tail{ 0 };
};

enum class DataFormat : uint8_t {
    ND = 0,
    NZ,
    NCHW,
    NC1HWC0,
    NHWC,
};

struct PadParams {
    [aicore] PadParams()
    {
        leftPad = 0;
        rightPad = 0;
        padValue = 0;
    }

    [aicore] PadParams(const uint16_t leftPadIn, const uint16_t rightPadIn, const int32_t padValueIn)
    {
        leftPad = leftPadIn;
        rightPad = rightPadIn;
        padValue = padValueIn;
    }

    uint16_t leftPad = 0;
    uint16_t rightPad = 0;
    int32_t padValue = 0;
};

struct UnPadParams {
    [aicore] UnPadParams()
    {
        leftPad = 0;
        rightPad = 0;
    }

    [aicore] UnPadParams(const uint16_t leftPadIn, const uint16_t rightPadIn)
    {
        leftPad = leftPadIn;
        rightPad = rightPadIn;
    }

    uint16_t leftPad = 0;
    uint16_t rightPad = 0;
};



constexpr int32_t AIPP_OFFSET_CSC_ENABLE = 63;
constexpr int32_t AIPP_OFFSET_CH1 = 16;
constexpr int32_t AIPP_OFFSET_CH2 = 32;
constexpr int32_t AIPP_OFFSET_CH3 = 48;
constexpr int32_t AIPP_OFFSET_SWAP_RB = 16;
constexpr int32_t AIPP_OFFSET_SWAP_UV = 17;
constexpr int32_t AIPP_OFFSET_SWAP_AX = 18;
constexpr int32_t AIPP_OFFSET_FORMAT = 19;
constexpr int32_t AIPP_OFFSET_SINGLE_LINE = 24;
constexpr int32_t AIPP_OFFSET_PADDING_MODE = 27;
constexpr int32_t AIPP_OFFSET_CPADDING_MODE = 40;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH0 = 16;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH1 = 24;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH2 = 32;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH0 = 40;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH1 = 48;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH2 = 56;
constexpr int32_t AIPP_OFFSET_DTC_CH1 = 32;
constexpr int32_t AIPP_OFFSET_DTC_ROUND_MODE = 34;

enum class AippInputFormat : uint8_t {
    YUV420SP_U8 = 0,
    XRGB8888_U8 = 1,
    RGB888_U8 = 4,
    YUV400_U8 = 9,
};

template <typename U>
struct AippPaddingParams {
    uint32_t paddingMode{ 0 };
    U paddingValueCh0{ 0 };
    U paddingValueCh1{ 0 };
    U paddingValueCh2{ 0 };
    U paddingValueCh3{ 0 };
};

struct AippSwapParams {
    bool isSwapRB{ false };
    bool isSwapUV{ false };
    bool isSwapAX{ false };
};

struct AippSingleLineParams {
    bool isSingleLineCopy{ false };
};

struct AippDataTypeConvParams {
    uint8_t dtcMeanCh0{ 0 };
    uint8_t dtcMeanCh1{ 0 };
    uint8_t dtcMeanCh2{ 0 };
    half dtcMinCh0{ 0 };
    half dtcMinCh1{ 0 };
    half dtcMinCh2{ 0 };
    half dtcVarCh0{ 1.0 };
    half dtcVarCh1{ 1.0 };
    half dtcVarCh2{ 1.0 };
    uint32_t dtcRoundMode{ 0 };
};

template <typename U>
struct AippChannelPaddingParams {
    uint32_t cPaddingMode;
    U cPaddingValue;
};

struct AippColorSpaceConvParams {
    bool isEnableCsc{ false };
    int16_t cscMatrixR0C0{ 0 };
    int16_t cscMatrixR0C1{ 0 };
    int16_t cscMatrixR0C2{ 0 };
    int16_t cscMatrixR1C0{ 0 };
    int16_t cscMatrixR1C1{ 0 };
    int16_t cscMatrixR1C2{ 0 };
    int16_t cscMatrixR2C0{ 0 };
    int16_t cscMatrixR2C1{ 0 };
    int16_t cscMatrixR2C2{ 0 };
    uint8_t cscBiasIn0{ 0 };
    uint8_t cscBiasIn1{ 0 };
    uint8_t cscBiasIn2{ 0 };
    uint8_t cscBiasOut0{ 0 };
    uint8_t cscBiasOut1{ 0 };
    uint8_t cscBiasOut2{ 0 };
};

template <typename U>
struct AippParams {
    AippPaddingParams<U> paddingParams;
    AippSwapParams swapParams;
    AippSingleLineParams singleLineParams;
    AippDataTypeConvParams dtcParams;
    AippChannelPaddingParams<U> cPaddingParams;
    AippColorSpaceConvParams cscParams;
};



class AscendCUtils {
public:
    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetBitSize(int32_t byteSize)
    {
        return byteSize * ONE_BYTE_BIT_SIZE;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetC0Size()
    {
        return DEFAULT_C0_SIZE;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetC0Count(const int32_t dtypeSize)
    {
                                                                                                 ;
        return GetC0Size() / dtypeSize;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int32_t GetDefaultBlockNum()
    {
        return DEFAULT_BLK_NUM;
    }

    [aicore] static __inline__ __attribute__((always_inline)) int64_t GetRsvdCnt()
    {
        return get_rsvd_cnt();
    }

    template <typename T, bool isSetMask = true>
    [aicore] static __inline__ __attribute__((always_inline)) void SetMask(const uint64_t& maskHigh, const uint64_t& maskLow)
    {
        if constexpr (!isSetMask) {
            return;
        }
        if constexpr(g_coreType != AscendC::AIC) {
            set_vector_mask(maskHigh, maskLow);
        }
    }

    template <typename T, bool isSetMask = true> [aicore] static __inline__ __attribute__((always_inline)) void SetMask(int32_t len)
    {
        if constexpr (!isSetMask) {
            return;
        }

        int32_t typeLen = 0;
        if constexpr (IsSameType<T, int4b_t>::value) {
            typeLen = DEFAULT_BLOCK_SIZE * INT4_TWO;
        } else {
            typeLen = DEFAULT_BLOCK_SIZE / sizeof(T);
        }
        constexpr int32_t halfTypeLen = 64;
        constexpr int32_t lenCoeff = 2;
        if (len == halfTypeLen) {
            SetMask<T>(0, FULL_MASK);
            return;
        } else if (len == typeLen || len >= halfTypeLen * lenCoeff) {
            SetMask<T>(FULL_MASK, FULL_MASK);
            return;
        }
        SetMask<T>(static_cast<uint64_t>(
            (len > halfTypeLen) ? (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(len - halfTypeLen)) - 1) : 0),
            static_cast<uint64_t>(
            (len > halfTypeLen) ? FULL_MASK : (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(len)) - 1)));
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void SetMaskCount()
    {
        set_mask_count();
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void SetMaskNorm()
    {
        set_mask_norm();
    }


    [aicore] static __inline__ __attribute__((always_inline)) void SetOverflow(uint64_t ctrlValue)
    {


        if (ctrlValue == 1) {
            set_ctrl(sbitset1(get_ctrl(), CTRL_48_BIT));
        } else {
            set_ctrl(sbitset0(get_ctrl(), CTRL_48_BIT));
        }
    }
# 3748 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
    template <bool isSetMask = true> [aicore] static __inline__ __attribute__((always_inline)) void ResetMask()
    {
        if constexpr (!isSetMask) {
            return;
        }
        if constexpr(g_coreType != AscendC::AIC) {
            set_vector_mask(FULL_MASK, FULL_MASK);
        }
    }

    template <bool isInt4 = false>
    [aicore] __inline__ __attribute__((always_inline)) static IntriInfo CalIntriInfo(
        const uint32_t dtypeSize, const uint32_t calCount, uint32_t repStride = DEFAULT_BLK_NUM)
    {
        IntriInfo retIntriInfo;
        retIntriInfo.c0Count = GetC0Count(dtypeSize);
        if constexpr (isInt4) {
            retIntriInfo.c0Count = GetC0Size() * INT4_TWO;
        }
        uint32_t repeatCount = repStride * retIntriInfo.c0Count;
        retIntriInfo.repeat = calCount / repeatCount;
        retIntriInfo.tail = calCount % repeatCount;
        retIntriInfo.repeatRounding = retIntriInfo.repeat / MAX_REPEAT_TIMES;
        retIntriInfo.repeatRemaining = retIntriInfo.repeat % MAX_REPEAT_TIMES;

        return retIntriInfo;
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) __attribute__((cce_unif_buff)) T* GetTemporaryBufferAddr(const int32_t bufferOffset, const int32_t bufferSize)
    {
# 3794 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
        (void)bufferSize;
        __attribute__((cce_unif_buff)) T* addr = reinterpret_cast<__attribute__((cce_unif_buff)) T*>((uint64_t)(0) + bufferOffset);

        return addr;
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void FreeTemporaryBuffer(__attribute__((cce_unif_buff)) T* addr)
    {
        (void)addr;
    }


    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) __attribute__((cce_fixpipe_buff)) T* GetTemporaryFbBufferAddr(const int32_t bufferOffset, const int32_t bufferSize)
    {
# 3820 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
        (void)bufferSize;
        __attribute__((cce_fixpipe_buff)) T* addr = reinterpret_cast<__attribute__((cce_fixpipe_buff)) T*>((uint64_t)(0) + bufferOffset);

        return addr;
    }

    template <typename T> [aicore] static __inline__ __attribute__((always_inline)) void FreeTemporaryFbBuffer(__attribute__((cce_fixpipe_buff)) T* addr)
    {
        (void)addr;
    }


    [aicore] static __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const DataCopyParams& intriParams, const bool& isSrc,
        const bool& isMovAlignIntri)
    {
        uint16_t stride = intriParams.dstStride;
        uint16_t burstLenUnit = 32;
        uint16_t strideUnit = 32;
        if (isSrc) {
            stride = intriParams.srcStride;
        }
        if (isMovAlignIntri) {
            burstLenUnit = 1;
            strideUnit = 1;
        }
        if (intriParams.blockLen == 0) {
            return 0;
        }
        uint64_t gmLen = static_cast<uint64_t>(intriParams.blockCount) * intriParams.blockLen * burstLenUnit +
            (intriParams.blockCount - 1) * stride * strideUnit;
        return gmLen;
    }

    [aicore] static __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const DataCopyExtParams& intriParams, const bool& isSrc,
        const bool& isMovAlignIntri)
    {
        uint16_t stride = intriParams.dstStride;
        uint16_t burstLenUnit = 32;
        uint16_t strideUnit = 32;
        if (isSrc) {
            stride = intriParams.srcStride;
        }
        if (isMovAlignIntri) {
            burstLenUnit = 1;
            strideUnit = 1;
        }
        if (intriParams.blockLen == 0) {
            return 0;
        }
        uint64_t gmLen = static_cast<uint64_t>(intriParams.blockCount) * intriParams.blockLen * burstLenUnit +
            (intriParams.blockCount - 1) * stride * strideUnit;
        return gmLen;
    }

    [aicore] static __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const uint64_t& srcEleSize, const Nd2NzParams& intriParams)
    {
        uint64_t gmLen = (static_cast<uint64_t>(intriParams.ndNum) - 1) * srcEleSize * intriParams.srcNdMatrixStride +
            (intriParams.nValue - 1) * intriParams.srcDValue * srcEleSize + intriParams.dValue * srcEleSize;
        return gmLen;
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflow(__attribute__((cce_global)) T* gmAddr, const bool& isSrc,
        const uint64_t& gmLen)
    {
# 3926 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflowNormal(__attribute__((cce_global)) T *gmAddr, __attribute__((cce_global)) uint8_t *workSpace,
        const bool isSrc, const bool isMovAlignIntri, const DataCopyParams& intriParams)
    {
        (void)(workSpace);
        uint64_t gmLen = GetGMLen(intriParams, isSrc, isMovAlignIntri);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflowNormal(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace,
        const bool isSrc, const bool isMovAlignIntri, const DataCopyExtParams& intriParams)
    {
        (void)(workSpace);
        uint64_t gmLen = GetGMLen(intriParams, isSrc, isMovAlignIntri);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }

    template <typename T>
    [aicore] static __inline__ __attribute__((always_inline)) void CheckGmMemOverflowNd2Nz(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace,
        const bool isSrc, const Nd2NzParams& intriParams)
    {
        (void)(workSpace);
        uint64_t srcEleSize = sizeof(T);
        uint64_t gmLen = GetGMLen(srcEleSize, intriParams);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }
};
# 4091 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_utils.h"
constexpr uint32_t BF16_TO_FP32_MAN_LEN = 16;
[aicore] __inline__ __attribute__((always_inline)) bfloat16_t ToBfloat16(const float& fVal)
{
    float fNum = fVal;
    union ToBfloat16Union {
        [aicore] ToBfloat16Union() {}
        uint16_t val;
        bfloat16_t bNum;
    } bf16Union;
    union FloattoInt32Union {
        [aicore] FloattoInt32Union() {}
        float ftmp;
        uint32_t uret;
    } int32Union;
    int32Union.ftmp = fNum;
    bf16Union.val = int32Union.uret >> BF16_TO_FP32_MAN_LEN;
    return bf16Union.bNum;
}

[aicore] __inline__ __attribute__((always_inline)) float ToFloat(const bfloat16_t& bVal)
{
    bfloat16_t bNum = bVal;
    union ToFloatUnion {
        [aicore] ToFloatUnion() {}
        uint32_t val;
        float fNum;
    } floatUnion;
    union ToUint16Union {
        [aicore] ToUint16Union() {}
        bfloat16_t uret;
        uint16_t num;
    } u16Union;
    u16Union.uret = bNum;
    floatUnion.val = u16Union.num << BF16_TO_FP32_MAN_LEN;
    return floatUnion.fNum;
}

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h"
namespace AscendC {
constexpr uint64_t MASK_PLACEHOLDER = 0;
constexpr uint64_t MASK_PLACEHOLDER_LIST[2] = {0, 0};

enum class MaskMode : uint8_t {
    NORMAL = 0,
    COUNTER
};

template <typename T, MaskMode mode>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMaskImpl(const uint64_t maskHigh, const uint64_t maskLow)
{
    if constexpr(g_coreType != AscendC::AIC) {
        set_vector_mask(maskHigh, maskLow);
    }
}

template <typename T, MaskMode mode>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMaskImpl(int32_t len)
{
    if constexpr (mode == MaskMode::COUNTER) {
        SetVectorMaskImpl<T, mode>(0, len);
        return;
    }
    AscendCUtils::SetMask<T>(len);
}

[aicore] __inline__ __attribute__((always_inline)) void ResetMaskImpl()
{
    if constexpr(g_coreType != AscendC::AIC) {
        set_vector_mask(FULL_MASK, FULL_MASK);
    }
}

template <pipe_t pipe> [aicore] __inline__ __attribute__((always_inline)) void PipeBarrierImpl()
{
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h"
    pipe_barrier(pipe);
}

enum class CacheLine : uint64_t {
    SINGLE_CACHE_LINE = 0,
    ENTIRE_DATA_CACHE
};

enum class DcciDst : uint64_t {
    CACHELINE_ALL = 0,
    CACHELINE_UB,
    CACHELINE_OUT,
    CACHELINE_ATOMIC
};


template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DcciGMImpl(__attribute__((cce_global)) T* dst)
{
    dcci(static_cast<__attribute__((cce_global)) void *>(dst), static_cast<uint64_t>(entireType), static_cast<uint64_t>(dcciDst));
}

template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DcciUBImpl(__attribute__((cce_unif_buff)) T* dst)
{
    dcci(static_cast<__attribute__((cce_unif_buff)) void *>(dst), static_cast<uint64_t>(entireType), static_cast<uint64_t>(dcciDst));
}



template <typename T, CacheLine entireType>
[aicore] __inline__ __attribute__((always_inline)) void DcciGMImpl(__attribute__((cce_global)) T* dst)
{
    dcci(static_cast<__attribute__((cce_global)) void *>(dst), static_cast<uint64_t>(entireType));
}


[aicore] __inline__ __attribute__((always_inline)) void SetMaskCountImpl()
{
    set_mask_count();
}

[aicore] __inline__ __attribute__((always_inline)) void SetMaskNormImpl()
{
    set_mask_norm();
}

[aicore] __inline__ __attribute__((always_inline)) void SetLreluMode(bool lreluMode)
{
    if (lreluMode) {
        set_ctrl(sbitset1(get_ctrl(), LEAKY_RELU_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), LEAKY_RELU_MODE_BIT));
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32ModeImpl(bool hf32Mode)
{
    if (hf32Mode) {
        set_ctrl(sbitset1(get_ctrl(), HF32_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), HF32_MODE_BIT));
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32TransModeImpl(bool hf32TransMode)
{
    if (hf32TransMode) {
        set_ctrl(sbitset1(get_ctrl(), HF32_TRANS_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), HF32_TRANS_MODE_BIT));
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetMMLayoutTransformImpl(bool mmLayoutMode)
{
    if (mmLayoutMode) {
        set_ctrl(sbitset1(get_ctrl(), MM_LAYOUT_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), MM_LAYOUT_MODE_BIT));
    }
}

template <bool castMode>
[aicore] __inline__ __attribute__((always_inline)) void SetCastOverflowModeImpl()
{
    if constexpr (castMode) {
        set_ctrl(sbitset1(get_ctrl(), CAST_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), CAST_MODE_BIT));
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl0(__attribute__((cce_global)) T* src0)
{
    uint64_t aippConfig0 = reinterpret_cast<uint64_t>(src0) & 0xffffffffffff;

    set_aipp_spr_0(aippConfig0);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl1(__attribute__((cce_global)) T* src1, AippParams<U>& config)
{
    uint64_t aippConfig1 = reinterpret_cast<uint64_t>(src1) & 0xffffffffffff;

    if (config.cscParams.isEnableCsc) {
        aippConfig1 |= static_cast<uint64_t>(1) << AIPP_OFFSET_CSC_ENABLE;
    }

    set_aipp_spr_1(aippConfig1);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl2(AippParams<U>& config)
{
    uint16_t cscMatrixR0C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C0);
    uint16_t cscMatrixR0C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C1);
    uint16_t cscMatrixR0C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C2);
    uint16_t cscMatrixR1C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C0);

    uint64_t aippConfig2 = static_cast<uint64_t>(cscMatrixR0C0);
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR0C1) << AIPP_OFFSET_CH1;
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR0C2) << AIPP_OFFSET_CH2;
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR1C0) << AIPP_OFFSET_CH3;

    set_aipp_spr_2(aippConfig2);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl3(AippParams<U>& config)
{
    uint16_t cscMatrixR1C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C1);
    uint16_t cscMatrixR1C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C2);
    uint16_t cscMatrixR2C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C0);
    uint16_t cscMatrixR2C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C1);

    uint64_t aippConfig3 = static_cast<uint64_t>(cscMatrixR1C1);
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR1C2) << AIPP_OFFSET_CH1;
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR2C0) << AIPP_OFFSET_CH2;
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR2C1) << AIPP_OFFSET_CH3;

    set_aipp_spr_3(aippConfig3);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl4(AippParams<U>& config)
{
    uint16_t cscMatrixR2C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C2);
    uint8_t cscBiasOut0 = GetScalarBitcodeValue(config.cscParams.cscBiasOut0);
    uint8_t cscBiasOut1 = GetScalarBitcodeValue(config.cscParams.cscBiasOut1);
    uint8_t cscBiasOut2 = GetScalarBitcodeValue(config.cscParams.cscBiasOut2);
    uint8_t cscBiasIn0 = GetScalarBitcodeValue(config.cscParams.cscBiasIn0);
    uint8_t cscBiasIn1 = GetScalarBitcodeValue(config.cscParams.cscBiasIn1);
    uint8_t cscBiasIn2 = GetScalarBitcodeValue(config.cscParams.cscBiasIn2);

    uint64_t aippConfig4 = static_cast<uint64_t>(cscMatrixR2C2);
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut0) << AIPP_OFFSET_CSC_OUT_CH0;
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut1) << AIPP_OFFSET_CSC_OUT_CH1;
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut2) << AIPP_OFFSET_CSC_OUT_CH2;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn0) << AIPP_OFFSET_CSC_IN_CH0;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn1) << AIPP_OFFSET_CSC_IN_CH1;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn2) << AIPP_OFFSET_CSC_IN_CH2;

    set_aipp_spr_4(aippConfig4);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl5(AippParams<U>& config)
{



    uint8_t dtcMeanCh0 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh0);
    uint8_t dtcMeanCh1 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh1);
    uint8_t dtcMeanCh2 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh2);

    uint64_t aippConfig5 = static_cast<uint64_t>(dtcMeanCh0);
    aippConfig5 |= static_cast<uint64_t>(dtcMeanCh1) << AIPP_OFFSET_CH1;
    aippConfig5 |= static_cast<uint64_t>(dtcMeanCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_5(aippConfig5);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl6(AippParams<U>& config)
{



    uint16_t dtcMinCh0 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh0);
    uint16_t dtcMinCh1 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh1);
    uint16_t dtcMinCh2 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh2);

    uint64_t aippConfig6 = static_cast<uint64_t>(dtcMinCh0);
    aippConfig6 |= static_cast<uint64_t>(dtcMinCh1) << AIPP_OFFSET_CH1;
    aippConfig6 |= static_cast<uint64_t>(dtcMinCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_6(aippConfig6);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl7(AippParams<U>& config)
{



    uint16_t dtcVarCh0 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh0);
    uint16_t dtcVarCh1 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh1);
    uint16_t dtcVarCh2 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh2);

    uint64_t aippConfig7 = static_cast<uint64_t>(dtcVarCh0);
    aippConfig7 |= static_cast<uint64_t>(dtcVarCh1) << AIPP_OFFSET_CH1;
    aippConfig7 |= static_cast<uint64_t>(dtcVarCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_7(aippConfig7);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl8(AippParams<U>& config)
{
    uint64_t aippConfig8 = 0;
    if constexpr(IsSameType<U, int8_t>::value || IsSameType<U, uint8_t>::value) {
        uint8_t paddingValueCh0 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh0);
        uint8_t paddingValueCh1 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh1);
        uint8_t paddingValueCh2 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh2);
        uint8_t paddingValueCh3 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh3);

        aippConfig8 |= static_cast<uint64_t>(paddingValueCh0);
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh1) << AIPP_OFFSET_CH1;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh2) << AIPP_OFFSET_CH2;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh3) << AIPP_OFFSET_CH3;
    } else {
        uint16_t paddingValueCh0 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh0);
        uint16_t paddingValueCh1 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh1);
        uint16_t paddingValueCh2 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh2);
        uint16_t paddingValueCh3 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh3);

        aippConfig8 |= static_cast<uint64_t>(paddingValueCh0);
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh1) << AIPP_OFFSET_CH1;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh2) << AIPP_OFFSET_CH2;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh3) << AIPP_OFFSET_CH3;
    }

    set_aipp_spr_8(aippConfig8);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl9(AippInputFormat format, AippParams<U>& config)
{
    uint64_t aippConfig9 = 0;

    if constexpr(IsSameType<U, int8_t>::value || IsSameType<U, uint8_t>::value) {
        uint8_t cPaddingValue = GetScalarBitcodeValue(config.cPaddingParams.cPaddingValue);
        aippConfig9 |= static_cast<uint64_t>(cPaddingValue);
    } else {
        uint16_t cPaddingValue = GetScalarBitcodeValue(config.cPaddingParams.cPaddingValue);
        aippConfig9 |= static_cast<uint64_t>(cPaddingValue);
    }

    if (config.swapParams.isSwapRB) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_RB;
    }
    if (config.swapParams.isSwapUV) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_UV;
    }
    if (config.swapParams.isSwapAX) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_AX;
    }

    aippConfig9 |= (static_cast<uint64_t>(format) & 0x1f) << AIPP_OFFSET_FORMAT;

    if (config.singleLineParams.isSingleLineCopy) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SINGLE_LINE;
    }

    aippConfig9 |= (static_cast<uint64_t>(config.paddingParams.paddingMode) & 0x3) << AIPP_OFFSET_PADDING_MODE;





    aippConfig9 |= (static_cast<uint64_t>(config.cPaddingParams.cPaddingMode) & 0x1) << AIPP_OFFSET_CPADDING_MODE;

    set_aipp_spr_9(aippConfig9);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl18(AippParams<U>& config)
{

    return;

    float dtcVarCh0f = static_cast<float>(config.dtcParams.dtcVarCh0);
    float dtcVarCh1f = static_cast<float>(config.dtcParams.dtcVarCh1);
    uint32_t dtcVarCh0 = GetScalarBitcodeValue(dtcVarCh0f);
    uint32_t dtcVarCh1 = GetScalarBitcodeValue(dtcVarCh1f);

    uint64_t aippConfig18 = static_cast<uint64_t>(dtcVarCh0);
    aippConfig18 |= static_cast<uint64_t>(dtcVarCh1) << AIPP_OFFSET_DTC_CH1;

    set_aipp_spr_18(aippConfig18);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl19(AippParams<U>& config)
{

    return;

    float dtcVarCh2f = static_cast<float>(config.dtcParams.dtcVarCh2);
    uint32_t dtcVarCh2 = GetScalarBitcodeValue(dtcVarCh2f);
    uint64_t aippConfig19 = static_cast<uint64_t>(dtcVarCh2);
    set_aipp_spr_19(aippConfig19);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl20(AippParams<U>& config)
{

    return;

    float dtcMeanCh0f = static_cast<float>(config.dtcParams.dtcMeanCh0 * 1.0f);
    float dtcMeanCh1f = static_cast<float>(config.dtcParams.dtcMeanCh1 * 1.0f);

    uint32_t dtcMeanCh0 = GetScalarBitcodeValue(dtcMeanCh0f);
    uint32_t dtcMeanCh1 = GetScalarBitcodeValue(dtcMeanCh1f);

    uint64_t aippConfig20 = static_cast<uint64_t>(dtcMeanCh0);
    aippConfig20 |= static_cast<uint64_t>(dtcMeanCh1) << AIPP_OFFSET_DTC_CH1;

    set_aipp_spr_20(aippConfig20);
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl21(AippParams<U>& config)
{

    return;

    float dtcMeanCh2f = static_cast<float>(config.dtcParams.dtcMeanCh2 * 1.0f);
    uint32_t dtcMeanCh2 = GetScalarBitcodeValue(dtcMeanCh2f);
    uint64_t aippConfig21 = static_cast<uint64_t>(dtcMeanCh2);
    set_aipp_spr_21(aippConfig21);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl(__attribute__((cce_global)) T* src0, __attribute__((cce_global)) T* src1,
    AippInputFormat format, AippParams<U>& config)
{

    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
# 440 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_reg.h"
    SetAippFunctionsImpl0<T>(src0);
    SetAippFunctionsImpl1<T, U>(src1, config);
    SetAippFunctionsImpl2<U>(config);
    SetAippFunctionsImpl3<U>(config);
    SetAippFunctionsImpl4<U>(config);
    SetAippFunctionsImpl5<U>(config);
    SetAippFunctionsImpl6<U>(config);
    SetAippFunctionsImpl7<U>(config);
    SetAippFunctionsImpl8<U>(config);
    SetAippFunctionsImpl9<U>(format, config);

}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctionsImpl(__attribute__((cce_global)) T* src0, AippInputFormat format, AippParams<U> config)
{

    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }

    SetAippFunctionsImpl(src0, reinterpret_cast<__attribute__((cce_global)) T*>(0), format, config);
}


}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_process_lock.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h" 2

namespace AscendC {
class TPipe;
class KfcCommClient;

[aicore] __inline__ __attribute__((always_inline)) constexpr int16_t GetDataBlockSizeInBytes()
{
    return ONE_BLK_SIZE;
}
}





[[block_local]] __inline__ AscendC::TPipe* g_vecTPipePtr;
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_common.h"
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_sysWorkspaceReserved;
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_dumpWorkspaceReserved;
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_hcclContextReserved[2];




[aicore] __inline__ __attribute__((always_inline)) AscendC::TPipe* GetTPipePtr()
{




    return g_vecTPipePtr;






}


namespace AscendC {





template <typename T>
struct TensorTrait {
    using LiteType = T;
};

template <typename T>
using PrimT = decltype(IsLite<T>(0));

template <typename T, MaskMode mode = MaskMode::NORMAL>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMask(const uint64_t maskHigh, const uint64_t maskLow)
{





    SetVectorMaskImpl<T, mode>(maskHigh, maskLow);
}

template <typename T, MaskMode mode = MaskMode::NORMAL>
[aicore] static __inline__ __attribute__((always_inline)) void SetVectorMask(int32_t len)
{



    SetVectorMaskImpl<T, mode>(len);
}

[aicore] __inline__ __attribute__((always_inline)) void ResetMask()
{



    ResetMaskImpl();
}


template <MemDsbT arg0>
[aicore] __inline__ __attribute__((always_inline)) void DataSyncBarrier()
{
    DataSyncBarrierImpl<arg0>();
}


template <HardEvent event> [aicore] __inline__ __attribute__((always_inline)) void SetFlag(int32_t eventID)
{
    SetFlagImpl<event>(eventID);
}

template <HardEvent event> [aicore] __inline__ __attribute__((always_inline)) void WaitFlag(int32_t eventID)
{
    WaitFlagImpl(event, eventID);
}

template <pipe_t pipe> [aicore] __inline__ __attribute__((always_inline)) void PipeBarrier()
{
    PipeBarrierImpl<pipe>();
}


template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DataCacheCleanAndInvalid(const GlobalTensor<T>& dstTensor)
{
    DcciGMImpl<T, entireType, dcciDst>(const_cast<__attribute__((cce_global)) T*>(dstTensor.GetPhyAddr()));
}

template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] __inline__ __attribute__((always_inline)) void DataCacheCleanAndInvalid(const LocalTensor<T>& dstTensor)
{
    DcciUBImpl<T, entireType, dcciDst>(const_cast<__attribute__((cce_unif_buff)) T*>(dstTensor.GetPhyAddr()));
}



template <typename T, CacheLine entireType>
[aicore] __inline__ __attribute__((always_inline)) void DataCacheCleanAndInvalid(const GlobalTensor<T>& dstTensor)
{
    DcciGMImpl<T, entireType>(const_cast<__attribute__((cce_global)) T*>(dstTensor.GetPhyAddr()));
}


[aicore] __inline__ __attribute__((always_inline)) void SetMaskCount()
{
    SetMaskCountImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void SetMaskNorm()
{
    SetMaskNormImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32Mode(bool hf32Mode)
{
    SetHF32ModeImpl(hf32Mode);
}

[aicore] __inline__ __attribute__((always_inline)) void SetHF32TransMode(bool hf32TransMode)
{
    SetHF32TransModeImpl(hf32TransMode);
}

[aicore] __inline__ __attribute__((always_inline)) void SetMMLayoutTransform(bool mmLayoutMode)
{
    SetMMLayoutTransformImpl(mmLayoutMode);
}

template <uint32_t index>
[aicore] __inline__ __attribute__((always_inline)) void SetHcclContext(__attribute__((cce_global)) uint8_t* context)
{
    if constexpr (index > 1) {
        return;
    }
    g_hcclContextReserved[index] = context;
}

template <uint32_t index>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetHcclContext(void)
{
    if constexpr (index > 1) {
        return nullptr;
    }
    return g_hcclContextReserved[index];
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctions(const GlobalTensor<T>& src0, AippInputFormat format, AippParams<U> config)
{
    SetAippFunctionsImpl<T, U>(const_cast<__attribute__((cce_global)) T*>(src0.GetPhyAddr()), format, config);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SetAippFunctions(const GlobalTensor<T>& src0, const GlobalTensor<T>& src1,
    AippInputFormat format, AippParams<U> config)
{
    SetAippFunctionsImpl<T, U>(const_cast<__attribute__((cce_global)) T*>(src0.GetPhyAddr()),
        const_cast<__attribute__((cce_global)) T*>(src1.GetPhyAddr()), format, config);
}

}

[[deprecated("NOTICE: SetDumpWorkSpacePtr has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) SetDumpWorkSpacePtr(__attribute__((cce_global)) uint8_t* workspace)
{
    return g_dumpWorkspaceReserved = workspace;
}
[[deprecated("NOTICE: GetDumpWorkSpacePtr has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetDumpWorkSpacePtr()
{
    return g_dumpWorkspaceReserved;
}




[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetSysWorkSpacePtr()
{
    return g_sysWorkspaceReserved;
}
[[deprecated(
    "NOTICE: SetSysWorkSpacePtr has been deprecated and will be removed in the next version.")]]
[aicore] __inline__ __attribute__((always_inline)) void SetSysWorkSpacePtr(__attribute__((cce_global)) uint8_t* workspace)
{
    g_sysWorkspaceReserved = workspace;
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmp_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmp_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    uint8_t repeat, const BinaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmpv_lt(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GT: {
            vcmpv_gt(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::EQ: {
            vcmpv_eq(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::LE: {
            vcmpv_le(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GE: {
            vcmpv_ge(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::NE: {
            vcmpv_ne(dst, src0, src1, repeat, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VcmpIntrinsicsImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const BinaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmp_lt(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GT: {
            vcmp_gt(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::EQ: {
            vcmp_eq(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::LE: {
            vcmp_le(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GE: {
            vcmp_ge(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::NE: {
            vcmp_ne(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void VcmpvIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int32_t* src0, __attribute__((cce_unif_buff)) int32_t* src1,
    CMPMODE cmpMode, uint8_t repeat, const BinaryRepeatParams& repeatParams)
{


                                                                                               ;
    vcmpv_eq(dst, src0, src1, repeat, repeatParams.dstBlkStride,
        repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
        repeatParams.src0RepStride, repeatParams.src1RepStride);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareCompute(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        struct BinaryRepeatParams repeatParams;
        uint32_t sumRepeat = calCount * sizeof(T) / ONE_REPEAT_BYTE_SIZE;
        constexpr uint32_t repeatNormal = 252;
        uint32_t repeatRound = sumRepeat / repeatNormal;
        uint32_t repeatTail = sumRepeat % repeatNormal;
        uint32_t srcOffset = repeatNormal * ONE_REPEAT_BYTE_SIZE / sizeof(T);
        uint32_t dstOffset = srcOffset / ONE_BYTE_BIT_SIZE;

        for (uint32_t i = 0; i < repeatRound; ++i) {
            VcmpvImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + i * dstOffset,
                src0 + i * srcOffset,
                src1 + i * srcOffset, cmpMode, MASK_PLACEHOLDER, repeatNormal,
                repeatParams);
        }
        VcmpvImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + repeatRound * dstOffset,
            src0 + repeatRound * srcOffset,
            src1 + repeatRound * srcOffset, cmpMode, MASK_PLACEHOLDER, repeatTail,
            repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask[], uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                ;
        VcmpvIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask[], const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);

                                                                             ;
        VcmpIntrinsicsImpl(src0, src1, cmpMode, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                ;
        VcmpvIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);

                                                                             ;
        VcmpIntrinsicsImpl(src0, src1, cmpMode, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                ;
        CompareCompute(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode, calCount);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmpvs_lt(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::GT: {
            vcmpvs_gt(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::EQ: {
            vcmpvs_eq(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::LE: {
            vcmpvs_le(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::GE: {
            vcmpvs_ge(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::NE: {
            vcmpvs_ne(dst, src0, src1, repeat,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void VcmpvsIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int32_t* src0, int32_t src1,
    CMPMODE cmpMode, uint8_t repeat, const UnaryRepeatParams& repeatParams)
{


                                                                                                     ;
    vcmpvs_eq(dst, src0, src1, repeat,
        static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalarCompute(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        struct UnaryRepeatParams repeatParams;
        uint32_t sumRepeat = calCount * sizeof(T) / ONE_REPEAT_BYTE_SIZE;
        constexpr uint32_t repeatNormal = 252;
        uint32_t repeatRound = sumRepeat / repeatNormal;
        uint32_t repeatTail = sumRepeat % repeatNormal;
        uint32_t srcOffset = repeatNormal * ONE_REPEAT_BYTE_SIZE / sizeof(T);
        uint32_t dstOffset = srcOffset / ONE_BYTE_BIT_SIZE;
        for (uint32_t i = 0; i < repeatRound; ++i) {
            VcmpvsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + i * dstOffset,
                src0 + i * srcOffset,
                src1, cmpMode, MASK_PLACEHOLDER, repeatNormal,
                repeatParams);
        }
        VcmpvsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + repeatRound * dstOffset,
            src0 + repeatRound * srcOffset,
            src1, cmpMode, MASK_PLACEHOLDER, repeatTail,
            repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                       ;
        VcmpvsIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint64_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                       ;
        VcmpvsIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                       ;
        CompareScalarCompute(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode, calCount);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetCmpMaskImpl(__attribute__((cce_unif_buff)) T* dst)
{
    get_cmpmask(dst);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetCmpMaskImpl(__attribute__((cce_unif_buff)) T* src)
{
    set_cmpmask(src);
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Add, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vadd(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vadd(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Sub, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vsub(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SubIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SubIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vsub(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MulIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Mul, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmul(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vmul(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DivIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Div, current api support dtype combination "
        "is src and dst both: half / float.");
    vdiv(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        DivIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        DivIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vdiv(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Max, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmax(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MaxIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MaxIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vmax(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Min, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmin(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MinIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MinIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vmin(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AndIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in And, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vand(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AndIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AndIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vand((__attribute__((cce_unif_buff)) int16_t*)dst, (__attribute__((cce_unif_buff)) int16_t*)src0, (__attribute__((cce_unif_buff)) int16_t*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void OrIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in Or, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vor(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
        repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        OrIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        OrIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vor((__attribute__((cce_unif_buff)) int16_t*)dst, (__attribute__((cce_unif_buff)) int16_t*)src0, (__attribute__((cce_unif_buff)) int16_t*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, half, float>(), "Failed to check dtype in AddRelu, current api support dtype "
        "combination is src and dst both: int16_t / half / float.");
    vaddrelu(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vaddrelu((__attribute__((cce_unif_buff)) T*)dst, (__attribute__((cce_unif_buff)) T*)src0, (__attribute__((cce_unif_buff)) T*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




[[block_local]] __inline__ half gDeqValue;
struct AddDeqReluParams {
    [aicore] AddDeqReluParams(){};

    uint32_t needTmpSize = 0;
    uint32_t calcSize = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;
    uint32_t dstOffset = 0;
    uint32_t tailSrc0Offset = 0;
    uint32_t tailSrc1Offset = 0;
    uint32_t tailDstOffset = 0;
    uint64_t mask1;
    uint64_t mask2[2];
    uint8_t maskMode = 0;
    uint16_t mainBlock = 0;
    uint16_t tailSize = 0;

    uint8_t repeat = 0;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
};

[aicore] __inline__ __attribute__((always_inline)) void SetAddDeqReluMaskCal(AddDeqReluParams &params)
{
    if (params.maskMode == ADDDEQRELU_MASK_MODE_ONE) {
        AscendCUtils::SetMask<half>(params.mask1);
    } else if (params.maskMode == ADDDEQRELU_MASK_MODE_TWO) {
        AscendCUtils::SetMask<half>(params.mask2[1], params.mask2[0]);
    }
}

template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluComput(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    __attribute__((cce_unif_buff)) int32_t *tmpBuffer, AddDeqReluParams &params)
{

    vadd(tmpBuffer, src0, src1, params.repeat, DEFAULT_BLK_STRIDE, params.src0BlkStride,
        params.src1BlkStride, DEFAULT_REPEAT_STRIDE, params.src0RepStride, params.src1RepStride);
    pipe_barrier(PIPE_V);

    __attribute__((cce_unif_buff)) float *src0FloatTmp = reinterpret_cast<__attribute__((cce_unif_buff)) float *>(src0);
    vconv_s322f32(src0FloatTmp, tmpBuffer, params.repeat, static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(DEQ_SHIFT_RIGHT_17_BIT), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(gDeqValue), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(DEQ_SHIFT_LEFT_17_BIT), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    if constexpr (isSetMask) {
        SetAddDeqReluMaskCal(params);
    }
    __attribute__((cce_unif_buff)) half *src1HalfTmp = reinterpret_cast<__attribute__((cce_unif_buff)) half *>(src1);
    vconv_f322f16(src1HalfTmp, src0FloatTmp, params.repeat, 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    pipe_barrier(PIPE_V);

    __attribute__((cce_unif_buff)) half *tmpBufferHalf = reinterpret_cast<__attribute__((cce_unif_buff)) half *>(tmpBuffer);
    if (params.maskMode != 0) {
        set_mask_count();
        set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
    }
    vector_dup(tmpBufferHalf, static_cast<half>(0), 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), 1,
        DEFAULT_REPEAT_STRIDE, 0);
    if (params.maskMode != 0) {
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
    pipe_barrier(PIPE_V);

    if constexpr (isSetMask) {
        SetAddDeqReluMaskCal(params);
    }
    if (params.maskMode == 0) {
        vmax(dst, tmpBufferHalf, src1HalfTmp, params.repeat, params.dstBlkStride, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, params.dstRepStride, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    } else {
        vmax(dst, tmpBufferHalf, src1HalfTmp, params.repeat, params.dstBlkStride, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, params.dstRepStride, HALF_DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    }
    pipe_barrier(PIPE_V);
}

[aicore] __inline__ __attribute__((always_inline)) void GetAddDeqReluParamCal(AddDeqReluParams &params, uint8_t repeatTimes)
{
    if (params.needTmpSize <= TMP_UB_SIZE / sizeof(int32_t)) {
        params.calcSize = params.needTmpSize;
        if (params.maskMode != 0) {
            params.repeat = repeatTimes;
        }
    } else {
        params.calcSize = TMP_UB_SIZE / sizeof(int32_t);
        if (params.maskMode != 0) {
            params.repeat = params.calcSize / B32_DATA_NUM_PER_REPEAT;
        }
    }
    if (params.maskMode == 0) {
        params.repeat = repeatTimes;
    }
    params.mainBlock = params.needTmpSize / params.calcSize;
    params.tailSize = params.needTmpSize % params.calcSize;
    if (params.maskMode == 0) {
        params.src0Offset = params.calcSize;
        params.src1Offset = params.calcSize;
        params.dstOffset = params.calcSize;
    } else {
        params.src0Offset = params.repeat * params.src0RepStride * B32_DATA_NUM_PER_BLOCK;
        params.src1Offset = params.repeat * params.src1RepStride * B32_DATA_NUM_PER_BLOCK;
        params.dstOffset = params.repeat * params.dstRepStride * B16_DATA_NUM_PER_BLOCK;
    }
    params.tailSrc0Offset = params.mainBlock * params.src0Offset;
    params.tailSrc1Offset = params.mainBlock * params.src1Offset;
    params.tailDstOffset = params.mainBlock * params.dstOffset;
}

[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluImpl(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const int32_t &calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.needTmpSize = calCount;
        GetAddDeqReluParamCal(params, 1);
        __attribute__((cce_unif_buff)) int32_t *tmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        set_mask_count();
        set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<false>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, tmpBuffer, params);
        }
        if (params.tailSize != 0) {
            params.calcSize = params.tailSize;
            set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
            AddDeqReluComput<false>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, tmpBuffer, params);
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluImpl(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.maskMode = ADDDEQRELU_MASK_MODE_ONE;
        params.mask1 = mask;
        params.needTmpSize = repeatTimes * DEFAULT_BLOCK_SIZE / sizeof(int32_t);

        params.dstBlkStride = repeatParams.dstBlkStride;
        params.src0BlkStride = repeatParams.src0BlkStride;
        params.src1BlkStride = repeatParams.src1BlkStride;
        params.dstRepStride = repeatParams.dstRepStride;
        params.src0RepStride = repeatParams.src0RepStride;
        params.src1RepStride = repeatParams.src1RepStride;
        GetAddDeqReluParamCal(params, repeatTimes);
        __attribute__((cce_unif_buff)) int32_t *tmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        if constexpr (isSetMask) {
            AscendCUtils::SetMask<int32_t>(mask);
        }
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<isSetMask>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, tmpBuffer, params);
        }
        if (params.tailSize != 0) {
            if constexpr (isSetMask) {
                AscendCUtils::SetMask<int32_t>(mask);
            }
            params.repeat = repeatTimes - params.repeat * params.mainBlock;
            AddDeqReluComput<isSetMask>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, tmpBuffer, params);
        }
    }
}

template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqReluImpl(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.maskMode = ADDDEQRELU_MASK_MODE_TWO;
        params.mask2[0] = mask[0];
        params.mask2[1] = mask[1];
        params.needTmpSize = repeatTimes * DEFAULT_BLOCK_SIZE / sizeof(int32_t);

        params.dstBlkStride = repeatParams.dstBlkStride;
        params.src0BlkStride = repeatParams.src0BlkStride;
        params.src1BlkStride = repeatParams.src1BlkStride;
        params.dstRepStride = repeatParams.dstRepStride;
        params.src0RepStride = repeatParams.src0RepStride;
        params.src1RepStride = repeatParams.src1RepStride;

        GetAddDeqReluParamCal(params, repeatTimes);
        __attribute__((cce_unif_buff)) int32_t *tmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        if constexpr (isSetMask) {
            AscendCUtils::SetMask<int32_t>(mask[1], mask[0]);
        }
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<isSetMask>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, tmpBuffer, params);
        }
        if (params.tailSize != 0) {
            if constexpr (isSetMask) {
                AscendCUtils::SetMask<int32_t>(mask[1], mask[0]);
            }
            params.repeat = repeatTimes - params.repeat * params.mainBlock;
            AddDeqReluComput<isSetMask>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, tmpBuffer, params);
        }
    }
}



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in FusedMulAdd, current api support dtype "
        "combination is src and dst both: half / float.");
    vmadd(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        FusedMulAddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        FusedMulAddIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vmadd(dst, src0, src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<T, U>, Tuple<half, half>, Tuple<float, float>, Tuple<float, half>>(), "Failed to "
        "check dtype in MulAddDst, current api support dtype combination is src: half, dst: half / float; src: float, "
        "dst: float.");
    vmla(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulAddDstIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulAddDstIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(T) == sizeof(U)) {
            vmla(dst, src0, src1, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        } else {
            vmla(dst, src0, src1, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in FusedMulAddRelu, current api support dtype "
        "combination is src and dst both: half / float.");
    vmaddrelu(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        FusedMulAddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        FusedMulAddReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vmaddrelu(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, half, float>(), "Failed to check dtype in SubRelu, current api support dtype "
        "combination is src and dst both: int16_t / half / float.");
    vsubrelu(dst, src0, src1, repeatTimes, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vsubrelu(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SubReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SubReluIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_symbol_override_impl.h" 2





#pragma begin_pipe(V)
namespace AscendC {
template <typename T> class LocalTensor;


template <typename T> class SymbolOverrideAdd {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAdd(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        AddImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideSub {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideSub(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        SubImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideMul {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideMul(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        MulImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideDiv {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideDiv(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        DivImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};


template <typename T> class SymbolOverrideAnd {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAnd(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        AndImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};

template <typename T> class SymbolOverrideOr {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideOr(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor)
    {}

    [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<T> &dstTensor) const
    {




        OrImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1Tensor_.GetPhyAddr(), dstTensor.GetSize());
    }

private:
    const LocalTensor<T> &src0Tensor_;
    const LocalTensor<T> &src1Tensor_;
};


template <typename T> class SymbolOverrideCompare {
public:
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare(const LocalTensor<T> &src0Tensor, const LocalTensor<T> &src1Tensor,
        CMPMODE cmpMode)
        : src0Tensor_(src0Tensor), src1Tensor_(src1Tensor), cmpMode_(cmpMode)
    {}

    template <typename U> [aicore] __inline__ __attribute__((always_inline)) void Process(const LocalTensor<U> &dstTensor) const
    {




        VcmpvImpl((__attribute__((cce_unif_buff)) U *)dstTensor.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)this->src0Tensor_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) T *)this->src1Tensor_.GetPhyAddr(), cmpMode_, this->src0Tensor_.GetSize());
    }

private:
    const LocalTensor<T> src0Tensor_;
    const LocalTensor<T> src1Tensor_;
    CMPMODE cmpMode_;
};
}
#pragma end_pipe
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_base.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_base.h"
namespace AscendC {
using TBufHandle = uint8_t*;
using TEventID = int8_t;
using TTagType = int32_t;

enum class TBufState : uint8_t {
    FREE = 0,
    OCCUPIED,
    ENQUE,
    DEQUE,
};

struct TBufType {
    TBufState state;
    HardEvent freeBufEvt;
    TEventID enQueEvtID;
    TEventID freeBufEvtID;
    uint32_t address;
    uint32_t dataLen;
    TTagType usertag;
                                      ;
};

struct TBuffAddr {
    uint32_t dataLen;
    uint32_t bufferAddr;
    TBufHandle bufferHandle;
    uint8_t logicPos;



};

template <typename T> class BaseLocalTensor {
public:
    using PrimType = PrimT<T>;
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_base.h"
    [aicore] __inline__ __attribute__((always_inline)) void SetAddr(const TBuffAddr& bufferAddr)
    {
        this->address_ = bufferAddr;
    }
    [[deprecated("NOTICE: InitBuffer has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
    [aicore] __inline__ __attribute__((always_inline)) void InitBuffer(const uint32_t bufferOffset, const uint32_t bufferSize)
    {
        this->address_.bufferAddr = (uint64_t)(0) + bufferOffset;
        if constexpr (IsSameType<PrimType, int4b_t>::value) {
            this->address_.dataLen = bufferSize * INT4_BIT_NUM / ONE_BYTE_BIT_SIZE;
        } else {
            this->address_.dataLen = bufferSize * sizeof(PrimType);
        }
    }

    [aicore] __inline__ __attribute__((always_inline)) TBufHandle GetBufferHandle() const
    {
        return address_.bufferHandle;
    }
public:
    TBuffAddr address_;
};

template <typename T> class BaseGlobalTensor {
public:
    using PrimType = PrimT<T>;
    [aicore] __inline__ __attribute__((always_inline)) void SetAddr(const uint64_t offset)
    {
        if constexpr (IsSameType<PrimType, int4b_t>::value) {
            address_ = address_ + offset / INT4_TWO;
            oriAddress_ = oriAddress_ + offset / INT4_TWO;
        } else {
            address_ = address_ + offset;
            oriAddress_ = oriAddress_ + offset;
        }
    }
public:
    __attribute__((cce_global)) PrimType* address_;
    __attribute__((cce_global)) PrimType* oriAddress_;
};

template <typename T> class BaseTensor {
};
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h" 2

namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h"
struct ShapeInfo {
public:
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo() {}
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[],
        const uint8_t inputOriginalShapeDim, const uint32_t inputOriginalShape[], const DataFormat inputFormat)
        : shapeDim(inputShapeDim), originalShapeDim(inputOriginalShapeDim), dataFormat(inputFormat)
    {




          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
        }
        for (int index = 0; index < originalShapeDim; ++index) {
            originalShape[index] = inputOriginalShape[index];
        }
    }
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[], const DataFormat inputFormat)
        : shapeDim(inputShapeDim), originalShapeDim(inputShapeDim), dataFormat(inputFormat)
    {



          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
            originalShape[index] = inputShape[index];
        }
    }

    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[])
        : shapeDim(inputShapeDim), originalShapeDim(inputShapeDim), dataFormat(DataFormat::ND)
    {



          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
            originalShape[index] = inputShape[index];
        }
    }
    uint8_t shapeDim;
    uint8_t originalShapeDim;
    uint32_t shape[8];
    uint32_t originalShape[8];
    DataFormat dataFormat;
};






template <typename U, typename T>
struct ShapeInfoParams {
    [aicore] ShapeInfoParams() {};
    using Params = ShapeInfo;
};
template <typename T>
struct ShapeInfoParams<TensorTrait<T>, T> {
    [aicore] ShapeInfoParams() {};
    using Params = int8_t;
};

[aicore] __inline__ __attribute__((always_inline)) uint64_t GetShapeSize(const ShapeInfo& shapeInfo)
{
    int shapeSize = 1;
    for (int index = 0; index < shapeInfo.shapeDim; ++index) {
        shapeSize *= shapeInfo.shape[index];
    }
    return shapeSize;
}

template <typename T> class LocalTensor : public BaseLocalTensor<T> {
public:
    using PrimType = PrimT<T>;
    [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T>() {};
# 139 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tensor.h"
    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetPhyAddr() const;
    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetPhyAddr(const uint32_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) PrimType GetValue(const uint32_t index) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) __attribute__((cce_unif_buff)) PrimType& operator()(const uint32_t offset) const;
    template <typename CAST_T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<CAST_T> ReinterpretCast() const;
    template <typename T1> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
        void SetValue(const uint32_t index, const T1 value) const;
    [aicore] __inline__ __attribute__((always_inline)) LocalTensor operator[](const uint32_t offset) const;

    template <typename T1>
    [[deprecated("NOTICE: SetAddrWithOffset has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
    [aicore] __inline__ __attribute__((always_inline)) void SetAddrWithOffset(LocalTensor<T1> &src, uint32_t offset);

    [aicore] __inline__ __attribute__((always_inline)) int32_t GetPosition() const;
    [aicore] __inline__ __attribute__((always_inline)) void SetSize(const uint32_t size);
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetSize() const;

    [[deprecated("NOTICE: GetLength has been deprecated and will be removed in the next version. Please do not use "
                 "it!")]]
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetLength() const;

    [[deprecated("NOTICE: SetBufferLen has been deprecated and will be removed in the next version. Please do not use "
                 "it!")]]
    [aicore] __inline__ __attribute__((always_inline)) void SetBufferLen(uint32_t dataLen);
    [aicore] __inline__ __attribute__((always_inline)) void SetUserTag(const TTagType tag);
    [aicore] __inline__ __attribute__((always_inline)) TTagType GetUserTag() const;

    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideAdd<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideSub<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideMul<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideDiv<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideOr<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideAnd<T>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideCompare<float>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) void operator = (const SymbolOverrideCompare<half>& symbolOverride);
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAdd<T> operator + (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideSub<T> operator - (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideMul<T> operator *(const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideDiv<T> operator / (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideOr<T> operator | (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAnd<T> operator & (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator < (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator > (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator != (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator == (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator <= (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T> operator >= (const LocalTensor<T>& src1Tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) void SetShapeInfo(const ShapeInfo& shapeInfo);
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo GetShapeInfo() const;

public:

    typename ShapeInfoParams<T, PrimType>::Params shapeInfo_;





private:



};

template <typename T> class GlobalTensor : public BaseGlobalTensor<T> {
public:
    using PrimType = PrimT<T>;
    [aicore] __inline__ __attribute__((always_inline)) GlobalTensor<T>() {}
    [aicore] __inline__ __attribute__((always_inline)) void SetGlobalBuffer(__attribute__((cce_global)) PrimType* buffer, uint64_t bufferSize);
    [aicore] __inline__ __attribute__((always_inline)) void SetGlobalBuffer(__attribute__((cce_global)) PrimType* buffer);
    [aicore] __inline__ __attribute__((always_inline)) const __attribute__((cce_global)) PrimType* GetPhyAddr() const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) PrimType* GetPhyAddr(const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) PrimType GetValue(const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) __attribute__((cce_global)) PrimType& operator()(const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) void SetValue(const uint64_t offset, PrimType value);

    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetSize() const;
    [aicore] __inline__ __attribute__((always_inline)) GlobalTensor operator[](const uint64_t offset) const;
    [aicore] __inline__ __attribute__((always_inline)) void SetShapeInfo(const ShapeInfo& shapeInfo);
    [aicore] __inline__ __attribute__((always_inline)) ShapeInfo GetShapeInfo() const;
    template<CacheRwMode rwMode = CacheRwMode::RW>
    [aicore] __inline__ __attribute__((always_inline)) void SetL2CacheHint(CacheMode mode);

public:

    uint64_t bufferSize_;

    typename ShapeInfoParams<T, PrimType>::Params shapeInfo_;

    CacheMode cacheMode_ = CacheMode::CACHE_MODE_NORMAL;
};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h" 2

namespace AscendC {
# 300 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t LocalTensor<T>::GetPhyAddr() const
{
    return GetPhyAddr(0);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t LocalTensor<T>::GetPhyAddr(const uint32_t offset) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        return this->address_.bufferAddr + offset / INT4_TWO;
    } else {
        return this->address_.bufferAddr + offset * sizeof(PrimType);
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    typename LocalTensor<T>::PrimType LocalTensor<T>::GetValue(const uint32_t index) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        LocalTensor<uint8_t> tmpLocalTensor = this->ReinterpretCast<uint8_t>();
        uint8_t val = tmpLocalTensor.GetValue(index / INT4_TWO);
        return static_cast<int4b_t>(val >> (INT4_BIT_NUM * (index % INT4_TWO)));
    } else {
        return *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(GetPhyAddr(index)));
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    __attribute__((cce_unif_buff)) typename LocalTensor<T>::PrimType& LocalTensor<T>::operator()(const uint32_t offset) const
{
    return *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(GetPhyAddr(offset)));
}

template <typename T>
template <typename CAST_T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<CAST_T> LocalTensor<T>::ReinterpretCast() const
{
    LocalTensor<CAST_T> tensorOut;
    tensorOut.address_.logicPos = static_cast<uint8_t>(this->GetPosition());
    tensorOut.address_.bufferHandle = this->GetBufferHandle();
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        tensorOut.address_.dataLen = this->GetSize() / INT4_TWO;
    } else {
        tensorOut.address_.dataLen = this->GetSize() * sizeof(PrimType);
    }
    tensorOut.address_.bufferAddr = this->address_.bufferAddr;
    nop_reinterpret_cast(this, &tensorOut);
    return tensorOut;
}

template <typename T>
template <typename T1> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    void LocalTensor<T>::SetValue(const uint32_t index, const T1 value) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        LocalTensor<uint8_t> tmpLocalTensor = this->ReinterpretCast<uint8_t>();
        uint8_t mask = (index % INT4_TWO == 0)? 0xf0 : 0xf;
        uint32_t idx = index / INT4_TWO;
        uint8_t val = tmpLocalTensor.GetValue(idx) & mask;
        uint8_t shift = (index % INT4_TWO == 0)? 0 : INT4_BIT_NUM;
        tmpLocalTensor.SetValue(idx, val + (value.storage << shift));
    } else {
        *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(static_cast<uint64_t>(this->address_.bufferAddr))
            + index) = static_cast<PrimType>(value);
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> LocalTensor<T>::operator[](const uint32_t offset) const
{
    LocalTensor retLocalTensor = *this;
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        retLocalTensor.address_.dataLen -= (offset / INT4_TWO);
        retLocalTensor.address_.bufferAddr = retLocalTensor.address_.bufferAddr + offset / INT4_TWO;
    } else {
        retLocalTensor.address_.dataLen -= (offset * sizeof(PrimType));
        retLocalTensor.address_.bufferAddr = retLocalTensor.address_.bufferAddr + offset * sizeof(PrimType);
    }
    return retLocalTensor;
}

template <typename T>
template <typename T1>
[[deprecated("NOTICE: SetAddrWithOffset has been deprecated and will be removed in the next version. "
    "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetAddrWithOffset(LocalTensor<T1> &src, uint32_t offset)
{
    this->address_ = src.address_;
    this->address_.bufferAddr += offset * sizeof(PrimT<T1>);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) int32_t LocalTensor<T>::GetPosition() const
{
    return this->address_.logicPos;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetSize(const uint32_t size)
{
# 400 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tensor_impl.h"
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        this->address_.dataLen = size / INT4_TWO;
    } else {
        this->address_.dataLen = size * sizeof(PrimType);
    }
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t LocalTensor<T>::GetSize() const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        return this->address_.dataLen * INT4_TWO;
    } else {
        return this->address_.dataLen / sizeof(PrimType);
    }
}

template <typename T>
[[deprecated("NOTICE: GetLength has been deprecated and will be removed in the next version. Please do not use "
                "it!")]]
[aicore] __inline__ __attribute__((always_inline)) uint32_t LocalTensor<T>::GetLength() const
{
    return this->address_.dataLen;
}

template <typename T>
[[deprecated("NOTICE: SetBufferLen has been deprecated and will be removed in the next version. Please do not use "
                "it!")]]
[aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetBufferLen(uint32_t dataLen)
{
    this->address_.dataLen = dataLen;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::SetUserTag(const TTagType tag)
{
    auto ptr = reinterpret_cast<TBufType*>(this->address_.bufferHandle);

                                                                            ;
    ptr->usertag = tag;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) TTagType LocalTensor<T>::GetUserTag() const
{
    auto ptr = reinterpret_cast<TBufType*>(this->address_.bufferHandle);

                                                                            ;
    return ptr->usertag;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideAdd<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideSub<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideMul<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideDiv<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideOr<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LocalTensor<T>::operator = (const SymbolOverrideAnd<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    LocalTensor<T>::operator = (const SymbolOverrideCompare<float>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    LocalTensor<T>::operator = (const SymbolOverrideCompare<half>& symbolOverride)
{
    symbolOverride.Process(*this);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAdd<T>
    LocalTensor<T>::operator + (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideAdd<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideSub<T>
    LocalTensor<T>::operator - (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideSub<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideMul<T>
    LocalTensor<T>::operator *(const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideMul<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideDiv<T>
    LocalTensor<T>::operator / (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideDiv<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideOr<T>
    LocalTensor<T>::operator | (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideOr<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideAnd<T>
    LocalTensor<T>::operator & (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideAnd<T>(*this, src1Tensor);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator < (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::LT);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator > (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::GT);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator != (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::NE);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator == (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::EQ);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator <= (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::LE);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) SymbolOverrideCompare<T>
    LocalTensor<T>::operator >= (const LocalTensor<T>& src1Tensor) const
{
    return SymbolOverrideCompare<T>(*this, src1Tensor, CMPMODE::GE);
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    LocalTensor<T>::SetShapeInfo(const ShapeInfo& shapeInfo)
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

        shapeInfo_ = shapeInfo;

}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) ShapeInfo LocalTensor<T>::GetShapeInfo() const
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

        return shapeInfo_;




}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void
    GlobalTensor<T>::SetGlobalBuffer(__attribute__((cce_global)) typename GlobalTensor<T>::PrimType* buffer, uint64_t bufferSize)
{
    if (this->cacheMode_ == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = buffer;
    } else {
        this->address_ = L2CacheAlter<PrimType, CacheRwMode::RW>(buffer, cacheMode_);
    }
    this->oriAddress_ = buffer;
    bufferSize_ = bufferSize;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GlobalTensor<T>::SetGlobalBuffer(__attribute__((cce_global)) typename GlobalTensor<T>::PrimType* buffer)
{
    if (this->cacheMode_ == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = buffer;
    } else {
        this->address_ = L2CacheAlter<PrimType, CacheRwMode::RW>(buffer, cacheMode_);
    }
    this->oriAddress_ = buffer;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline))
    const __attribute__((cce_global)) typename GlobalTensor<T>::PrimType* GlobalTensor<T>::GetPhyAddr() const
{
    return this->address_;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline))
    __attribute__((cce_global)) typename GlobalTensor<T>::PrimType* GlobalTensor<T>::GetPhyAddr(const uint64_t offset) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {

                                                                                               ;
        return this->address_ + offset / INT4_TWO;
    } else {
        return this->address_ + offset;
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    typename GlobalTensor<T>::PrimType GlobalTensor<T>::GetValue(const uint64_t offset) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        __attribute__((cce_global)) uint8_t *addr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(this->oriAddress_) + offset / INT4_TWO;
        return static_cast<PrimType>((*addr) >> (INT4_BIT_NUM * (offset % INT4_TWO)));
    } else {
        return this->oriAddress_[offset];
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S")))
    __attribute__((cce_global)) typename GlobalTensor<T>::PrimType& GlobalTensor<T>::operator()(const uint64_t offset) const
{
    return this->oriAddress_[offset];
}
template <typename T> [aicore] __inline__ __attribute__((always_inline))
    void GlobalTensor<T>::SetValue(const uint64_t offset, typename GlobalTensor<T>::PrimType value)
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        __attribute__((cce_global)) uint8_t *addr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(this->oriAddress_) + offset / INT4_TWO;
        uint8_t mask = (offset % INT4_TWO == 0)? 0xf0 : 0xf;

        uint8_t val = (*addr) & mask;
        uint8_t shift = (offset % INT4_TWO == 0)? 0 : INT4_BIT_NUM;
        *addr = val + (value.storage << shift);
    } else {
        this->oriAddress_[offset] = value;
    }
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint64_t GlobalTensor<T>::GetSize() const
{
    return bufferSize_;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) GlobalTensor<T> GlobalTensor<T>::operator[](const uint64_t offset) const
{
    GlobalTensor retLocalTensor = *this;
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        retLocalTensor.address_ = retLocalTensor.address_ + offset / INT4_TWO;
        retLocalTensor.oriAddress_ = retLocalTensor.oriAddress_ + offset / INT4_TWO;
    } else {
        retLocalTensor.address_ = retLocalTensor.address_ + offset;
        retLocalTensor.oriAddress_ = retLocalTensor.oriAddress_ + offset;
    }
    return retLocalTensor;
}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GlobalTensor<T>::SetShapeInfo(const ShapeInfo& shapeInfo)
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

    shapeInfo_ = shapeInfo;

}
template <typename T> [aicore] __inline__ __attribute__((always_inline)) ShapeInfo GlobalTensor<T>::GetShapeInfo() const
{
    static_assert(IsSameType<T, PrimType>::value, "Only primitive type Tensor has shape info!");

    return shapeInfo_;




}
template <typename T>
template<CacheRwMode rwMode>
[aicore] __inline__ __attribute__((always_inline)) void GlobalTensor<T>::SetL2CacheHint(CacheMode mode) {
    this->cacheMode_ = mode;
    if (mode == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = this->oriAddress_;
    } else {
        this->address_ = L2CacheAlter<PrimType, rwMode>(this->oriAddress_, mode);
    }



}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockIdxImpl()
{






    return get_subblockid();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetTaskRationImpl()
{



    return get_subblockdim();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockIdxImpl()
{
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_common_impl.h"
    if constexpr(g_coreType == AscendC::AIV) {
        return get_block_idx() * get_subblockdim() + get_subblockid();
    } else {
        return get_block_idx();
    }

}

[[deprecated(
    "NOTICE: SetSysWorkSpace has been deprecated and will be removed in the next version.")]]
[aicore] __inline__ __attribute__((always_inline)) void SetSysWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{




    if (g_sysWorkspaceReserved == nullptr) {
        g_sysWorkspaceReserved = workspace;
    }

}

[aicore] __inline__ __attribute__((always_inline)) void SetSysWorkspaceForce(__attribute__((cce_global)) uint8_t* workspace)
{




    g_sysWorkspaceReserved = workspace;

}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetUserWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{





    (void)(workspace);
    return g_sysWorkspaceReserved + RESERVED_WORKSPACE;

}

template <atomic_type_t type, atomic_op_t op>
[aicore] __inline__ __attribute__((always_inline)) void SetStoreAtomicConfigImpl()
{
    set_st_atomic_cfg(type, op);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetStoreAtomicConfigImpl()
{
    return get_st_atomic_cfg();
}

[aicore] __inline__ __attribute__((always_inline)) void GetStoreAtomicConfigImpl(uint16_t &atomicType, uint16_t &atomicOp)
{
    int64_t stAtomic = get_st_atomic_cfg();
    constexpr uint64_t typeMask = 0x7;
    constexpr uint64_t opBit = 4;
    constexpr uint64_t opMask = 0x3;
    atomicType = (static_cast<uint64_t>(stAtomic) & typeMask);
    atomicOp = ((static_cast<uint64_t>(stAtomic) >> opBit) & opMask);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCachePreloadImpl(const GlobalTensor<uint64_t> &srcTensor, const T cacheOffset)
{
    if constexpr ((IsSameType<T, int16_t>::value) || (IsSameType<T, int64_t>::value)) {
        dc_preload((__attribute__((cce_global)) uint64_t *)srcTensor.GetPhyAddr(), cacheOffset);
    } else {
        static_assert((220 == 220), "current data type is not supported on current device");
    }
}

[aicore] __inline__ __attribute__((always_inline)) void PreLoadImpl(void *pc, const int64_t preFetchLen)
{
    preload(pc, preFetchLen);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetICachePreloadStatusImpl()
{
    return get_icache_prl_st();
}

[aicore] __inline__ __attribute__((always_inline)) void CheckLocalMemoryIAImpl(const CheckLocalMemoryIAParam& checkParams)
{
    uint64_t config = 0;
    config = config | (static_cast<uint64_t>(checkParams.startAddr) << 48);
    config = config | (static_cast<uint64_t>(checkParams.endAddr) << 32);
    config = config | (static_cast<uint64_t>(checkParams.isScalarRead) << 31);
    config = config | (static_cast<uint64_t>(checkParams.isScalarWrite) << 30);
    config = config | (static_cast<uint64_t>(checkParams.isVectorRead) << 29);
    config = config | (static_cast<uint64_t>(checkParams.isVectorWrite) << 28);
    config = config | (static_cast<uint64_t>(checkParams.isMteRead) << 27);
    config = config | (static_cast<uint64_t>(checkParams.isMteWrite) << 26);
    config = config | (checkParams.reserved << 1);
    config = config | (static_cast<uint8_t>(checkParams.isEnable));
    if (checkParams.enableBit == SET_DATA_EXP_ZERO) {
        set_data_exp_0(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_ONE) {
        set_data_exp_1(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_TWO) {
        set_data_exp_2(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_THREE) {
        set_data_exp_3(config);
    } else {
        static_assert((220 == 220), "unsupport this enableBit on current device");
    }
}

[aicore] __inline__ __attribute__((always_inline)) void PreLoad(const int64_t preFetchLen)
{
    const int32_t pcOffset = 16;
    int64_t pc = (get_pc() >> pcOffset) & 0xFFFFFFFFFFFF;
    PreLoadImpl(reinterpret_cast<void *>(pc), preFetchLen);
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_atomic_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_atomic_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void SetAtomicNoneImpl()
{
    set_atomic_none();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl()
{


                                                                    ;
}

template <> [aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl<float>()
{
    set_atomic_add();
    set_atomic_f32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl<half>()
{
    set_atomic_add();
    set_atomic_f16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl<int16_t>()
{
    set_atomic_add();
    set_atomic_s16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl<int32_t>()
{
    set_atomic_add();
    set_atomic_s32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl<int8_t>()
{
    set_atomic_add();
    set_atomic_s8();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAddImpl<bfloat16_t>()
{
    set_atomic_add();
    set_atomic_bf16();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl()
{


                                                                    ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl<float>()
{
    set_atomic_max();
    set_atomic_f32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl<half>()
{
    set_atomic_max();
    set_atomic_f16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl<int16_t>()
{
    set_atomic_max();
    set_atomic_s16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl<int32_t>()
{
    set_atomic_max();
    set_atomic_s32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl<int8_t>()
{
    set_atomic_max();
    set_atomic_s8();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMaxImpl<bfloat16_t>()
{
    set_atomic_max();
    set_atomic_bf16();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl()
{


                                                                    ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl<float>()
{
    set_atomic_min();
    set_atomic_f32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl<half>()
{
    set_atomic_min();
    set_atomic_f16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl<int16_t>()
{
    set_atomic_min();
    set_atomic_s16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl<int32_t>()
{
    set_atomic_min();
    set_atomic_s32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl<int8_t>()
{
    set_atomic_min();
    set_atomic_s8();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMinImpl<bfloat16_t>()
{
    set_atomic_min();
    set_atomic_bf16();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl()
{


                                                                    ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl<float>()
{
    set_atomic_f32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl<half>()
{
    set_atomic_f16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl<int16_t>()
{
    set_atomic_s16();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl<int32_t>()
{
    set_atomic_s32();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl<int8_t>()
{
    set_atomic_s8();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicTypeImpl<bfloat16_t>()
{
    set_atomic_bf16();
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_log.h" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h" 2

namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void Barrier()
{



    __asm__ __volatile__("");

}

enum class KFC_Enum : uint16_t {
    SERVICE_ID_MASK = 0xFF00,
    SERVICE_ID_SCM = 0x0100,
    SCMFUN_GM2L1,
    SCMFUN_GM2L1ND2NZ,
    SERVICE_ID_MATMUL = 0x0300,
    MMFUN_MASK = 0x0380,
    MMFUN_ITERATE = 0x0380,
    MMFUN_ITERATE_ALL = 0x0381,
    MMFUN_INIT = 0x0301,
    MMFUN_GET_TENSOR_C,
    MMFUN_ITERATE_ALL_RESP,
    MMFUN_GET_TENSOR_C_RESP,
    MMFUN_GET_OFFSET_C,
    MMFUN_GET_OFFSET_C_RESP,
    MMFUN_SET_ORG_SHAPE,
    MMFUN_SET_HF32,
    MMFUN_SET_USER_DEF_INFO,
    MMFUN_ITERATE_BATCH_ALL,
    MMFUN_ITERATE_BATCH_ALL_RESP,
    MMFUN_ITERATE_N_BATCH_ALL,
    MMFUN_ITERATE_N_BATCH_ALL_RESP,
    MMFUN_END,
    SERVICE_QUIT = 0xfd00,
    SERVICE_BALANCE = 0xfe00,
    SERVICE_ID_NONE = 0xff00
};

enum class MSG_STATE : uint8_t {
    STATE_INVALID,
    STATE_SET,
};






constexpr int32_t MIX_NUM = 2;

constexpr int32_t MIX_COEFFICIENT = 1;
constexpr int32_t MAX_MATMUL_OBJ = 8;
constexpr int MAX_AIV_NUM = 50;
constexpr int MAX_AIC_NUM = 25;
constexpr int ALIGN_SIZE = 32;
constexpr int BIDIRECTION_NUM = 2;
constexpr bool KFC_APPLY_MSG = true;
constexpr uint64_t INC_PROCESS_CHECK = 14;
constexpr uint64_t WORKSPACE_UB_SIZE = TOTAL_UB_SIZE;
constexpr int32_t MAX_GROUP_ID = 32;
constexpr int32_t MM_CNT_MAX = 1024;
constexpr int32_t QUIT_CNT = 4;
constexpr int32_t MAX_SYNC_COUNT = 100000000;
constexpr int32_t MMCNT_L1_RESERVERD_SIZE = 64;

struct TilingInfo {
    __attribute__((cce_global)) uint8_t* tilingAddr;
};

struct MatmulOrgShape {
    uint32_t orgM;
    uint32_t orgN;
    uint32_t orgKa;
    uint32_t orgKb;
    uint32_t orgKc;
};
# 124 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
struct MatmulConfigParams {

    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t isTransA : 1;
    uint32_t isTransB : 1;
    uint32_t enPartialSum : 1;
    uint32_t setTail : 1;
    uint32_t setTensorA : 1;
    uint32_t setTensorB : 1;
    uint32_t setTensorBias : 1;
    uint32_t setClearBias : 1;
    uint32_t cIsTscm : 1;
    uint32_t isFirstIter : 1;
    uint32_t sync : 1;
    uint32_t enHF32 : 1;
    uint32_t hf32TransMode : 1;
    uint32_t setQuant : 1;
    uint32_t setBatch : 1;
    uint32_t waitIterateAll : 1;
    uint32_t waitIterateBatch : 1;
    uint32_t iterateFakeMsg : 1;

    uint32_t singleM;
    uint32_t singleN;
    uint32_t singleK;
    uint32_t sizeAmatrix;
    uint32_t sizeBmatrix;

    uint64_t aAddr;
    uint64_t bAddr;
    uint64_t cAddr;
    uint64_t biasAddr;
    uint64_t quantAddr;
    uint32_t quantSize;
    uint32_t quantMode;
    uint64_t quantScalar;
    uint32_t batchA;
    uint32_t batchB;
    uint32_t matrixStrideA;
    uint32_t matrixStrideB;
    uint32_t matrixStrideC;
    uint32_t batchLoop;
    uint32_t counterId;
    uint32_t reserved0;
    uint64_t dataPtr;
};

struct MatmulUserDefInfo {
    uint64_t tilingPtr;
};

constexpr uint16_t KFC_MSG_BYTE_OFFSET = 16;

[aicore] __inline__ __attribute__((always_inline)) uint16_t KfcMsgGetEvtCnt(uint32_t flag)
{
    return flag & 0x00007fff;
}

[aicore] __inline__ __attribute__((always_inline)) uint16_t KfcMsgGetInstID(uint32_t flag)
{
    return flag & 0x000000ff;
}
[aicore] __inline__ __attribute__((always_inline)) KFC_Enum KfcMsgGetFunID(uint32_t flag)
{
    return static_cast<KFC_Enum>((flag & 0xffff0000) >> KFC_MSG_BYTE_OFFSET);
}
[aicore] __inline__ __attribute__((always_inline)) uint32_t KfcMsgGetState(uint32_t flag)
{
    return (flag & 0x00008000);
}
[aicore] __inline__ __attribute__((always_inline)) uint32_t KfcMsgMakeFlag(KFC_Enum funID, uint16_t instID)
{
    return (((static_cast<uint16_t>(funID) << KFC_MSG_BYTE_OFFSET) + 0x8000) + (instID));
}


struct KfcMsg {
    uint32_t head = 0;
    int32_t ubAddr = -1;
    union {
        uint8_t buffer[120];
        TilingInfo tilingInfo;
        MatmulConfigParams body;
        MatmulOrgShape orgShape;
        MatmulUserDefInfo userDefInfo;
    };
};
struct MsgUBAvalied {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};
struct MsgMatmulCnt {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct QuitCnt {
    int32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct MmTaskCnt {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct MsgGroupSync {
    int32_t syncCount;
    uint8_t res[28];
    uint32_t allNumber;
    uint8_t buffer[28];
};

struct MsgGroupSyncAux {

    int32_t curNumber;
    uint8_t res[28];
    uint32_t idField;
    uint8_t buffer[28];
};

[aicore] __inline__ __attribute__((always_inline)) constexpr int AlignTo32(int size)
{
    return (size + ALIGN_SIZE - 1) / ALIGN_SIZE * ALIGN_SIZE;
}

struct SysWorkspaceDesc {
    KfcMsg kfcMsg[MAX_AIV_NUM * BIDIRECTION_NUM * 64 * MIX_COEFFICIENT];
    MsgMatmulCnt cntMsg[MAX_AIV_NUM * MIX_COEFFICIENT][MAX_MATMUL_OBJ];
    MsgUBAvalied ubMsg[MAX_AIV_NUM];
    uint8_t ubMap[MAX_AIV_NUM][WORKSPACE_UB_SIZE];
    QuitCnt quitCnt[QUIT_CNT];
    MmTaskCnt mmTaskCnt[MM_CNT_MAX];
    MsgGroupSync groupSyncMsg[MAX_GROUP_ID];
    MsgGroupSyncAux groupSyncAuxMsg[MAX_GROUP_ID];
};

[aicore] __inline__ __attribute__((always_inline)) void ClearWorkspaceImpl(__attribute__((cce_global)) uint8_t* workspace)
{
    constexpr uint32_t size = BIDIRECTION_NUM * 64 * AlignTo32(sizeof(KfcMsg)) * MIX_NUM;
    constexpr uint32_t sizeUbmsg = MIX_NUM * AlignTo32(sizeof(MsgUBAvalied));
    constexpr uint32_t offsetUbMsg = MAX_AIV_NUM * BIDIRECTION_NUM * 64 *
        MIX_COEFFICIENT * AlignTo32(sizeof(KfcMsg)) + MAX_AIV_NUM * MIX_COEFFICIENT *
        MAX_MATMUL_OBJ * AlignTo32(sizeof(MsgMatmulCnt));
    constexpr uint32_t block = size / 2048;
    __attribute__((cce_global)) uint8_t* msgStartAddr = (__attribute__((cce_global)) uint8_t*)(workspace + size * GetBlockIdxImpl());
    __attribute__((cce_global)) uint8_t* ubMsgStartAddr = (__attribute__((cce_global)) uint8_t*)(workspace + offsetUbMsg + sizeUbmsg * GetBlockIdxImpl());
    create_cbuf_matrix((__attribute__((cce_cube_buff)) uint32_t*)(0), 0x10040, 0);
    SetFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
    WaitFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
    for (size_t i = 0; i < block; i++) {
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)(msgStartAddr), (__attribute__((cce_cube_buff)) void*)(0), 0, 1, 64, 1, 1);
        msgStartAddr += 2048;
    }
    copy_cbuf_to_gm((__attribute__((cce_global)) void*)(ubMsgStartAddr), (__attribute__((cce_cube_buff)) void*)(0), 0, 1, sizeUbmsg / 32, 1, 1);
    PipeBarrier<PIPE_ALL>();
}
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetMsgHead(__attribute__((cce_global)) uint8_t* workspace, int i)
{
# 296 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
                                                                                                                ;

                                                                                 ;

    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    if constexpr(g_coreType == AscendC::AIV) {
        auto flatBlockID = GetBlockIdxImpl();
        return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr->kfcMsg[flatBlockID * BIDIRECTION_NUM * 64]);
    } else {


                                                                                                                ;
        auto flatBlockID = (GetBlockIdxImpl() * MIX_NUM + i);
        return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr->kfcMsg[flatBlockID * BIDIRECTION_NUM * 64]);
    }
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetUBMapAddr(__attribute__((cce_global)) uint8_t* workspace, int i = 0)
{







                                                                                 ;

    uint64_t flatBlockID;
    if constexpr(g_coreType == AscendC::AIC) {
        flatBlockID = (GetBlockIdxImpl() * MIX_NUM + i);
    } else {
        flatBlockID = GetBlockIdxImpl();
    }

    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(ptr->ubMap[flatBlockID]);
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetMatmulIncAddr(__attribute__((cce_global)) uint8_t* workspace, uint32_t flatBlockID, uint32_t instID)
{


                                                                                                                 ;

                                                                                 ;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&(ptr->cntMsg[flatBlockID][instID]));
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetUBAvaliedAddr(__attribute__((cce_global)) uint8_t* workspace, uint32_t i = 0)
{
                                ;
    auto flatBlockID = GetBlockIdxImpl();
    if constexpr(g_coreType == AscendC::AIC) {
        flatBlockID = flatBlockID * MIX_NUM + i;
    }
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&(ptr->ubMsg[flatBlockID]));
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *AllocMessageImpl(
    __attribute__((cce_global)) KfcMsg *&msgSendHead, uint8_t &msgSendPos, __attribute__((cce_global)) KfcMsg *&msgSendStart)
{
    auto msg = msgSendHead;

                                                                                   ;

                                                                                    ;
    if constexpr (KFC_APPLY_MSG) {
        while (static_cast<bool>(KfcMsgGetState(msg->head))) {
            Barrier();
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            Barrier();
        }
    }

                                                                                                                      ;
    msgSendPos++;
    if (msgSendPos >= 64) {
        msgSendPos = 0;
        msgSendHead = msgSendStart;
    } else {
        msgSendHead++;
    }
    return msg;
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *RcvMessageImpl(
    __attribute__((cce_global)) KfcMsg *&msgRcvHead, uint8_t &msgRcvPos, __attribute__((cce_global)) KfcMsg *&msgRcvStart)
{



                                                                                  ;

                                                                                   ;


      ;
    __attribute__((cce_global)) KfcMsg* msg = msgRcvHead;
    Barrier();
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    Barrier();

    dc_preload((__attribute__((cce_global)) uint64_t*)msg, int64_t(0));
# 413 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm.h"
    if (!(static_cast<bool>(KfcMsgGetState(msg->head)))) {
        return nullptr;
    }
    msgRcvPos++;
    if (msgRcvPos >= 64) {
        msgRcvPos = 0;
        msgRcvHead = msgRcvStart;
    } else {
        msgRcvHead++;
    }
    return msg;
}

[aicore] __inline__ __attribute__((always_inline)) void FreeMessageImpl(__attribute__((cce_global)) KfcMsg *msg)
{

                                                                           ;
    __asm__ __volatile__("" ::: "memory");
    *(reinterpret_cast<__attribute__((cce_global)) uint64_t *>(msg)) = 0;
    Barrier();
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    Barrier();
}

[aicore] __inline__ __attribute__((always_inline)) void RollBackMsgImpl(__attribute__((cce_global)) KfcMsg *&msgRcvHead, uint8_t &msgRcvPos)
{
    if (msgRcvPos == 0) {
        msgRcvPos = 64;
        msgRcvHead = msgRcvHead + 64 -1;
    } else {
        msgRcvPos--;
        msgRcvHead--;
    }
}

}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h" 2






namespace AscendC {

template <int depth>
struct TBufHandleAux {
    using T = TBufHandle[depth];
};

template <>
struct TBufHandleAux<1> {
    using T = TBufHandle;
};
constexpr TEventID INVALID_TEVENTID = (static_cast<TEventID>(-1));


struct TEventPool {
    uint64_t eventOccupy;
};

struct TPipeBufPool {
    uint32_t maxAddr;
};
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_base.h"
struct TShareBuf {
    enum class ShareHard : uint8_t {
        L1 = 0,
        L0C = 1,
        UB = 2,
        MAX,
    };
    int32_t start[static_cast<uint8_t>(ShareHard::MAX)];
    int32_t maxAddr[static_cast<uint8_t>(ShareHard::MAX)];
                                                                     ;
};

struct SpmInfo {
    uint64_t spmAddr;
    int32_t spmBuffSize;
    uint8_t spmBufType;
};

struct TPipeImpl {
    struct TEventPool eventPool_[EVENT_NUM];
    struct TPipeBufPool bufPool_[static_cast<uint8_t>(Hardware::MAX)];



    struct TBufType buf_[64];
    TShareBuf shareBufPool_;
    SpmInfo spmInfo_;

    uint32_t tscmBufferPtr_;
    uint8_t curBufSize_;
    bool isDestroy;
};

constexpr uint32_t defaultBufIDSize = 4;

template <uint32_t bufIDSize = defaultBufIDSize>
struct TBufPoolImpl {
    struct TBufType buf_[bufIDSize];
    uint32_t startAddr_;
    uint32_t maxAddr_;
    uint32_t maxLen_;
    uint8_t curBufSize_;
    uint8_t isReset_;
};

class TPipeBase {
public:
    [aicore] __inline__ __attribute__((always_inline)) void InitShareBufStart(uint32_t mode, uint32_t* shareLens, uint32_t lens, uint8_t subBlockIdx);
    [aicore] __inline__ __attribute__((always_inline)) void InitShareBufEnd();

protected:
    TPipeImpl g_tpipeImpl;
    [aicore] __inline__ __attribute__((always_inline)) void AuxShareBufStart(uint32_t mode, uint32_t* shareLens, uint8_t pos, Hardware hard,
                                            uint8_t subBlockIdx);
};

[aicore] __inline__ __attribute__((always_inline)) void TPipeBase::InitShareBufStart(uint32_t mode, uint32_t* shareLens, uint32_t lens,
                                                    uint8_t subBlockIdx)
{






    (void)(lens);



                                                                                                             ;
    AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L1), Hardware::L1, subBlockIdx);
    AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L0C), Hardware::L0C, subBlockIdx);



    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0A)].maxAddr = 0;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0B)].maxAddr = 0;

    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::BIAS)].maxAddr = 0;

    return;
}

[aicore] __inline__ __attribute__((always_inline)) void TPipeBase::InitShareBufEnd()
{

    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L1)].maxAddr =
        g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L1)];
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0C)].maxAddr =
        g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)];





    return;
}

[aicore] __inline__ __attribute__((always_inline)) void TPipeBase::AuxShareBufStart(uint32_t mode, uint32_t* shareLens, uint8_t pos, Hardware hard,
                                                   uint8_t subBlockIdx)
{
    uint8_t hardU8 = static_cast<uint8_t>(hard);
    if (__builtin_expect(!!(g_tpipeImpl.shareBufPool_.start[pos] == -1), 0)) {

        g_tpipeImpl.shareBufPool_.start[pos] = this->g_tpipeImpl.bufPool_[hardU8].maxAddr;
        g_tpipeImpl.shareBufPool_.maxAddr[pos] = g_tpipeImpl.shareBufPool_.start[pos] + shareLens[pos];
                                                                          ;
    } else {


                                                                              ;

        g_tpipeImpl.shareBufPool_.maxAddr[pos] = this->g_tpipeImpl.bufPool_[hardU8].maxAddr;
        g_tpipeImpl.bufPool_[hardU8].maxAddr = g_tpipeImpl.shareBufPool_.start[pos];
    }

    if (mode == 1 && subBlockIdx == 1) {
        this->g_tpipeImpl.bufPool_[hardU8].maxAddr += shareLens[pos] / HALF_FACTOR;
    }




      ;
}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h" 2








namespace AscendC {
class TPipe;
template <TPosition src, TPosition dst, int32_t depth, auto mask = 0> class TQueBind {
public:
    [aicore] __inline__ __attribute__((always_inline)) TQueBind();
    [aicore] __inline__ __attribute__((always_inline)) void FreeBuffer(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBuffAddr GetBufferAddr(TBufHandle buf);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> AllocTensor();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) void FreeTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) bool EnQue(const LocalTensor<T>& tensor);
    [aicore] __inline__ __attribute__((always_inline)) bool EnQue(TBufHandle buf);
    template <TPosition srcUserPos, TPosition dstUserPos, typename T>
    [aicore] __inline__ __attribute__((always_inline)) bool EnQue(const LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> DeQue();
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle DeQue();
    template <TPosition srcUserPos, TPosition dstUserPos, typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> DeQue();
    [aicore] __inline__ __attribute__((always_inline)) bool VacantInQue();
    [aicore] __inline__ __attribute__((always_inline)) bool HasTensorInQue();
    [aicore] __inline__ __attribute__((always_inline)) int32_t GetTensorCountInQue();
    [aicore] __inline__ __attribute__((always_inline)) bool HasIdleBuffer();
    [aicore] __inline__ __attribute__((always_inline)) void FreeAllEvent();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) TBufState GetState(const LocalTensor<T>& tensor) const;
protected:
    static constexpr TQueConfig config = GetTQueConfig(mask);
    static constexpr bool nd2nz = config.nd2nz;
    static constexpr bool nz2nd = config.nz2nd;
    static constexpr bool scmBlockGroup = config.scmBlockGroup;
    static constexpr TPosition srcPosition = src;
    static constexpr TPosition dstPosition = dst;
    static constexpr Hardware srcHardType = GetPhyType(src);
    static constexpr Hardware dstHardType = GetPhyType(dst);
    static constexpr HardEvent enQueEvt = GetQueEvt(srcHardType, dstHardType, true, nd2nz, nz2nd);
    static constexpr HardEvent freeBufEvt = GetQueEvt(srcHardType, dstHardType, false, nd2nz, nz2nd);
    static constexpr int32_t queDepth = depth;
    union {
        uint64_t value;
        struct {
            uint8_t bufNum = 0;
            uint8_t usedCount;
            uint16_t head;
            uint16_t tail;
            uint8_t bufUsedCount;
            uint8_t bufCursor;
        };
    };
    typename TBufHandleAux<depth>::T que_;
    struct TBufType* bufStart;
                               ;
    friend class TPipe;
    template <TPosition pos, int32_t d, auto m> friend class TQue;
    template<TPosition pos, uint32_t bufIDSize> friend class TBufPool;



private:
    [aicore] __inline__ __attribute__((always_inline)) void SetTBufPoolHandle(uint64_t bufPoolHandle);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> Buf2Tensor(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBufState GetState(const TBufHandle& handle) const;
    static constexpr bool isTQue = true;
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle AllocBuffer();
    template <TPosition srcUserPos, TPosition dstUserPos> [aicore] __inline__ __attribute__((always_inline)) bool EnQue(TBufHandle buf);
    template <TPosition srcUserPos, TPosition dstUserPos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle DeQue();
};





template <TPosition pos, int32_t depth, auto mask = 0>
class TQue : public TQueBind<GetBufferLogicPos(pos, true), GetBufferLogicPos(pos, false), depth, mask> {
public:
    [aicore] __inline__ __attribute__((always_inline)) TQue() = default;
private:
    friend class TPipe;
    template<TPosition bufPos, uint32_t bufIDSize> friend class TBufPool;
    static constexpr bool isTQue = true;
};

template <TPosition pos = TPosition::LCM> class TBuf : public TQueBind<pos, pos, 0, 0> {
public:
    [aicore] __inline__ __attribute__((always_inline)) TBuf() = default;
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> Get();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> Get(uint32_t len);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> GetWithOffset(uint32_t size, uint32_t bufOffset);

    template <typename T> [aicore] __inline__ __attribute__((always_inline)) void EnQue(const LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> DeQue();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> AllocTensor();
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) void FreeTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] __inline__ __attribute__((always_inline)) TBufState GetState(const LocalTensor<T>& tensor) const;
    [aicore] __inline__ __attribute__((always_inline)) bool EnQue(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle DeQue();
    [aicore] __inline__ __attribute__((always_inline)) void FreeBuffer(TBufHandle buf);
    [aicore] __inline__ __attribute__((always_inline)) TBuffAddr GetBufferAddr(TBufHandle buf);

private:
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle Get();
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle Get(uint32_t len);
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetBufLen() const;
    [aicore] __inline__ __attribute__((always_inline)) void SetTpipeBuf(TBufType* bufStartIn, uint32_t bufLenIn);
    template <TPosition posPopBuffer>
    friend [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(TBuf<posPopBuffer> &popBuffer, TBufType &bufStart);
    [aicore] __inline__ __attribute__((always_inline)) TBufHandle AllocBuffer();

private:
    struct TBufType* bufStart;
    uint32_t bufLen;
    uint32_t offset;
    friend class TPipe;
    template<TPosition bufPos, uint32_t bufIDSize> friend class TBufPool;
    static constexpr bool isTQue = false;
};

template<TPosition pos, uint32_t bufIDSize = defaultBufIDSize>
class TBufPool {
public:
    static constexpr TPosition poolPos = pos;
public:
    [aicore] __inline__ __attribute__((always_inline)) TBufPool();
    [aicore] __inline__ __attribute__((always_inline)) ~TBufPool();
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(T& que, uint8_t num, uint32_t len);
    template <TPosition bufPos> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(TBuf<bufPos>& buf, uint32_t len);
    template <class T, class U> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len, U& shareBuf);
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len);
    [aicore] __inline__ __attribute__((always_inline)) void Reset();
protected:
    TBufPoolImpl<bufIDSize> g_tBufPoolImpl;
private:
    [aicore] __inline__ __attribute__((always_inline)) void Init();
    [aicore] __inline__ __attribute__((always_inline)) void ResetPool();
private:
    friend class TPipe;
    template <TPosition src, TPosition dst, int32_t depth, auto mask> friend class TQueBind;
    template <TPosition bufPos, int32_t depth, auto mask> friend class TQue;
    template <TPosition bufPos> friend class TBuf;
    static constexpr bool isTbufPool = true;
};

class TPipe : public TPipeBase {
public:
    [aicore] __inline__ __attribute__((always_inline)) TPipe();
    [aicore] __inline__ __attribute__((always_inline)) ~TPipe();
    [aicore] __inline__ __attribute__((always_inline)) void Init();
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(T& que, uint8_t num, uint32_t len);
    template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool InitBuffer(TBuf<pos>& buf, uint32_t len);
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len);
    template <class T, class U> [aicore] __inline__ __attribute__((always_inline)) bool InitBufPool(T& bufPool, uint32_t len, U& shareBuf);
    template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID AllocEventID();
    template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) void ReleaseEventID(TEventID id);
    template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID FetchEventID();
    [aicore] __inline__ __attribute__((always_inline)) TEventID FetchEventID(HardEvent evt);
    template <TPosition pos, typename T>
    [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> GetAbsAddr(int32_t offset, int32_t size) const;
    template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBuffAddr GetAbsAddr(int32_t offset, int32_t len) const;
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_tpipe.h"
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void InitSpmBuffer(const GlobalTensor<T>& workspace, const int32_t bufferSize);
    [aicore] __inline__ __attribute__((always_inline)) void InitSpmBuffer(const int32_t bufferSize);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void WriteSpmBuffer(const LocalTensor<T>& writeLocal, const DataCopyParams& copyParams,
        int32_t writeOffset = 0);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void ReadSpmBuffer(const LocalTensor<T>& readLocal, const DataCopyParams& copyParams,
        int32_t readOffset = 0);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void WriteSpmBuffer(const LocalTensor<T>& writeLocal, const int32_t writeSize,
        int32_t writeOffset = 0);
    template <typename T>
    [aicore] __inline__ __attribute__((always_inline)) void ReadSpmBuffer(const LocalTensor<T>& readLocal, const int32_t readSize,
        int32_t readOffset = 0);
    [aicore] __inline__ __attribute__((always_inline)) void Destroy();
    [aicore] __inline__ __attribute__((always_inline)) void Reset();





protected:
    template <TPosition src, TPosition dst, int32_t depth, auto mask> friend class TQueBind;
    template <TPosition pos, int32_t depth, auto mask> friend class TQue;
    template <TPosition pos> friend class TBuf;
    template<TPosition pos, uint32_t bufIDSize> friend class TBufPool;
    template <TPosition pos> friend [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(TBuf<pos>& popBuffer, TBufType& bufStart);
    template <typename T, TPosition pos> friend [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(LocalTensor<T>& popLocal);




private:



    friend [aicore] __inline__ __attribute__((always_inline)) void InitShareBufStart(TPipe* tpipe, uint32_t mode, uint32_t* shareLens,
        uint32_t lens, uint8_t subBlockIdx);
    friend [aicore] __inline__ __attribute__((always_inline)) void InitShareBufEnd(TPipe* tpipe);
    [aicore] __inline__ __attribute__((always_inline)) void InitSocState() const;
    [aicore] __inline__ __attribute__((always_inline)) void ResetPool();
    template <class T> [aicore] __inline__ __attribute__((always_inline)) bool TscmInitBuffer(T& que, uint8_t num, uint32_t len);



    template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint64_t GetQueueEndAddress();
};

template<pipe_t src, pipe_t dst>
class TQueSync {
public:
    [aicore] __inline__ __attribute__((always_inline)) void SetFlag(TEventID id);
    [aicore] __inline__ __attribute__((always_inline)) void WaitFlag(TEventID id);
};

template <TPosition pos, int32_t depth = 1, auto mask = 0>
using TSCM = TQueBind<pos, TPosition::TSCM, depth, mask>;
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2
# 1 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 1 3
# 33 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3







namespace std __attribute__ ((__visibility__ ("default")))
{
# 56 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp, _Tp __v>
    struct integral_constant
    {
      static constexpr _Tp value = __v;
      typedef _Tp value_type;
      typedef integral_constant<_Tp, __v> type;
      constexpr operator value_type() const noexcept { return value; }




      constexpr value_type operator()() const noexcept { return value; }

    };

  template<typename _Tp, _Tp __v>
    constexpr _Tp integral_constant<_Tp, __v>::value;


  typedef integral_constant<bool, true> true_type;


  typedef integral_constant<bool, false> false_type;

  template<bool __v>
    using __bool_constant = integral_constant<bool, __v>;



  template<bool __v>
    using bool_constant = integral_constant<bool, __v>;




  template<bool, typename, typename>
    struct conditional;

  template <typename _Type>
    struct __type_identity
    { using type = _Type; };

  template<typename _Tp>
    using __type_identity_t = typename __type_identity<_Tp>::type;

  template<typename...>
    struct __or_;

  template<>
    struct __or_<>
    : public false_type
    { };

  template<typename _B1>
    struct __or_<_B1>
    : public _B1
    { };

  template<typename _B1, typename _B2>
    struct __or_<_B1, _B2>
    : public conditional<_B1::value, _B1, _B2>::type
    { };

  template<typename _B1, typename _B2, typename _B3, typename... _Bn>
    struct __or_<_B1, _B2, _B3, _Bn...>
    : public conditional<_B1::value, _B1, __or_<_B2, _B3, _Bn...>>::type
    { };

  template<typename...>
    struct __and_;

  template<>
    struct __and_<>
    : public true_type
    { };

  template<typename _B1>
    struct __and_<_B1>
    : public _B1
    { };

  template<typename _B1, typename _B2>
    struct __and_<_B1, _B2>
    : public conditional<_B1::value, _B2, _B1>::type
    { };

  template<typename _B1, typename _B2, typename _B3, typename... _Bn>
    struct __and_<_B1, _B2, _B3, _Bn...>
    : public conditional<_B1::value, __and_<_B2, _B3, _Bn...>, _B1>::type
    { };

  template<typename _Pp>
    struct __not_
    : public __bool_constant<!bool(_Pp::value)>
    { };



  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool __or_v = __or_<_Bn...>::value;
  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool __and_v = __and_<_Bn...>::value;



  template<typename... _Bn>
    struct conjunction
    : __and_<_Bn...>
    { };

  template<typename... _Bn>
    struct disjunction
    : __or_<_Bn...>
    { };

  template<typename _Pp>
    struct negation
    : __not_<_Pp>
    { };

  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool conjunction_v = conjunction<_Bn...>::value;

  template<typename... _Bn>
    __inline__ __attribute__((always_inline)) constexpr bool disjunction_v = disjunction<_Bn...>::value;

  template<typename _Pp>
    __inline__ __attribute__((always_inline)) constexpr bool negation_v = negation<_Pp>::value;




  template<typename>
    struct is_reference;
  template<typename>
    struct is_function;
  template<typename>
    struct is_void;
  template<typename>
    struct __is_array_unknown_bounds;




  template <typename _Tp, size_t = sizeof(_Tp)>
    constexpr true_type __is_complete_or_unbounded(__type_identity<_Tp>)
    { return {}; }

  template <typename _TypeIdentity,
      typename _NestedType = typename _TypeIdentity::type>
    constexpr typename __or_<
      is_reference<_NestedType>,
      is_function<_NestedType>,
      is_void<_NestedType>,
      __is_array_unknown_bounds<_NestedType>
    >::type __is_complete_or_unbounded(_TypeIdentity)
    { return {}; }






  template<typename _Tp>
    struct __success_type
    { typedef _Tp type; };

  struct __failure_type
  { };

  template<typename>
    struct remove_cv;


  template<typename _Tp>
    using __remove_cv_t = typename remove_cv<_Tp>::type;

  template<typename>
    struct is_const;



  template<typename>
    struct __is_void_helper
    : public false_type { };

  template<>
    struct __is_void_helper<void>
    : public true_type { };


  template<typename _Tp>
    struct is_void
    : public __is_void_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename>
    struct __is_integral_helper
    : public false_type { };

  template<>
    struct __is_integral_helper<bool>
    : public true_type { };

  template<>
    struct __is_integral_helper<char>
    : public true_type { };

  template<>
    struct __is_integral_helper<signed char>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned char>
    : public true_type { };


  template<>
    struct __is_integral_helper<wchar_t>
    : public true_type { };
# 284 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<>
    struct __is_integral_helper<char16_t>
    : public true_type { };

  template<>
    struct __is_integral_helper<char32_t>
    : public true_type { };

  template<>
    struct __is_integral_helper<short>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned short>
    : public true_type { };

  template<>
    struct __is_integral_helper<int>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned int>
    : public true_type { };

  template<>
    struct __is_integral_helper<long>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned long>
    : public true_type { };

  template<>
    struct __is_integral_helper<long long>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned long long>
    : public true_type { };
# 364 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp>
    struct is_integral
    : public __is_integral_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename>
    struct __is_floating_point_helper
    : public false_type { };

  template<>
    struct __is_floating_point_helper<float>
    : public true_type { };

  template<>
    struct __is_floating_point_helper<double>
    : public true_type { };

  template<>
    struct __is_floating_point_helper<long double>
    : public true_type { };
# 392 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp>
    struct is_floating_point
    : public __is_floating_point_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct is_array
    : public false_type { };

  template<typename _Tp, std::size_t _Size>
    struct is_array<_Tp[_Size]>
    : public true_type { };

  template<typename _Tp>
    struct is_array<_Tp[]>
    : public true_type { };

  template<typename>
    struct __is_pointer_helper
    : public false_type { };

  template<typename _Tp>
    struct __is_pointer_helper<_Tp*>
    : public true_type { };


  template<typename _Tp>
    struct is_pointer
    : public __is_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct is_lvalue_reference
    : public false_type { };

  template<typename _Tp>
    struct is_lvalue_reference<_Tp&>
    : public true_type { };


  template<typename>
    struct is_rvalue_reference
    : public false_type { };

  template<typename _Tp>
    struct is_rvalue_reference<_Tp&&>
    : public true_type { };

  template<typename>
    struct __is_member_object_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_object_pointer_helper<_Tp _Cp::*>
    : public __not_<is_function<_Tp>>::type { };


  template<typename _Tp>
    struct is_member_object_pointer
    : public __is_member_object_pointer_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename>
    struct __is_member_function_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_function_pointer_helper<_Tp _Cp::*>
    : public is_function<_Tp>::type { };


  template<typename _Tp>
    struct is_member_function_pointer
    : public __is_member_function_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_enum
    : public integral_constant<bool, __is_enum(_Tp)>
    { };


  template<typename _Tp>
    struct is_union
    : public integral_constant<bool, __is_union(_Tp)>
    { };


  template<typename _Tp>
    struct is_class
    : public integral_constant<bool, __is_class(_Tp)>
    { };


  template<typename _Tp>
    struct is_function
    : public __bool_constant<!is_const<const _Tp>::value> { };

  template<typename _Tp>
    struct is_function<_Tp&>
    : public false_type { };

  template<typename _Tp>
    struct is_function<_Tp&&>
    : public false_type { };



  template<typename>
    struct __is_null_pointer_helper
    : public false_type { };

  template<>
    struct __is_null_pointer_helper<std::nullptr_t>
    : public true_type { };


  template<typename _Tp>
    struct is_null_pointer
    : public __is_null_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename _Tp>
    struct __is_nullptr_t
    : public is_null_pointer<_Tp>
    { } __attribute__ ((__deprecated__ ("use '" "std::is_null_pointer" "' instead")));




  template<typename _Tp>
    struct is_reference
    : public __or_<is_lvalue_reference<_Tp>,
                   is_rvalue_reference<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_arithmetic
    : public __or_<is_integral<_Tp>, is_floating_point<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_fundamental
    : public __or_<is_arithmetic<_Tp>, is_void<_Tp>,
     is_null_pointer<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_object
    : public __not_<__or_<is_function<_Tp>, is_reference<_Tp>,
                          is_void<_Tp>>>::type
    { };

  template<typename>
    struct is_member_pointer;


  template<typename _Tp>
    struct is_scalar
    : public __or_<is_arithmetic<_Tp>, is_enum<_Tp>, is_pointer<_Tp>,
                   is_member_pointer<_Tp>, is_null_pointer<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_compound
    : public __not_<is_fundamental<_Tp>>::type { };

  template<typename _Tp>
    struct __is_member_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_pointer_helper<_Tp _Cp::*>
    : public true_type { };


  template<typename _Tp>
    struct is_member_pointer
    : public __is_member_pointer_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename, typename>
    struct is_same;

  template<typename _Tp, typename... _Types>
    using __is_one_of = __or_<is_same<_Tp, _Types>...>;


  template<typename _Tp>
    using __is_signed_integer = __is_one_of<__remove_cv_t<_Tp>,
   signed char, signed short, signed int, signed long,
   signed long long
# 604 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
   >;


  template<typename _Tp>
    using __is_unsigned_integer = __is_one_of<__remove_cv_t<_Tp>,
   unsigned char, unsigned short, unsigned int, unsigned long,
   unsigned long long
# 623 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
   >;


  template<typename _Tp>
    using __is_standard_integer
      = __or_<__is_signed_integer<_Tp>, __is_unsigned_integer<_Tp>>;


  template<typename...> using __void_t = void;



  template<typename _Tp, typename = void>
    struct __is_referenceable
    : public false_type
    { };

  template<typename _Tp>
    struct __is_referenceable<_Tp, __void_t<_Tp&>>
    : public true_type
    { };




  template<typename>
    struct is_const
    : public false_type { };

  template<typename _Tp>
    struct is_const<_Tp const>
    : public true_type { };


  template<typename>
    struct is_volatile
    : public false_type { };

  template<typename _Tp>
    struct is_volatile<_Tp volatile>
    : public true_type { };


  template<typename _Tp>
    struct is_trivial
    : public integral_constant<bool, __is_trivial(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_copyable
    : public integral_constant<bool, __is_trivially_copyable(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_standard_layout
    : public integral_constant<bool, __is_standard_layout(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    struct

    is_pod
    : public integral_constant<bool, __is_pod(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_literal_type
    : public integral_constant<bool, __is_literal_type(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_empty
    : public integral_constant<bool, __is_empty(_Tp)>
    { };


  template<typename _Tp>
    struct is_polymorphic
    : public integral_constant<bool, __is_polymorphic(_Tp)>
    { };




  template<typename _Tp>
    struct is_final
    : public integral_constant<bool, __is_final(_Tp)>
    { };



  template<typename _Tp>
    struct is_abstract
    : public integral_constant<bool, __is_abstract(_Tp)>
    { };

  template<typename _Tp,
    bool = is_arithmetic<_Tp>::value>
    struct __is_signed_helper
    : public false_type { };

  template<typename _Tp>
    struct __is_signed_helper<_Tp, true>
    : public integral_constant<bool, _Tp(-1) < _Tp(0)>
    { };


  template<typename _Tp>
    struct is_signed
    : public __is_signed_helper<_Tp>::type
    { };


  template<typename _Tp>
    struct is_unsigned
    : public __and_<is_arithmetic<_Tp>, __not_<is_signed<_Tp>>>
    { };
# 770 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp, typename _Up = _Tp&&>
    _Up
    __declval(int);

  template<typename _Tp>
    _Tp
    __declval(long);

  template<typename _Tp>
    auto declval() noexcept -> decltype(__declval<_Tp>(0));

  template<typename, unsigned = 0>
    struct extent;

  template<typename>
    struct remove_all_extents;

  template<typename _Tp>
    struct __is_array_known_bounds
    : public integral_constant<bool, (extent<_Tp>::value > 0)>
    { };

  template<typename _Tp>
    struct __is_array_unknown_bounds
    : public __and_<is_array<_Tp>, __not_<extent<_Tp>>>
    { };






  struct __do_is_destructible_impl
  {
    template<typename _Tp, typename = decltype(declval<_Tp&>().~_Tp())>
      static true_type __test(int);

    template<typename>
      static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_destructible_impl
    : public __do_is_destructible_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp,
           bool = __or_<is_void<_Tp>,
                        __is_array_unknown_bounds<_Tp>,
                        is_function<_Tp>>::value,
           bool = __or_<is_reference<_Tp>, is_scalar<_Tp>>::value>
    struct __is_destructible_safe;

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, false, false>
    : public __is_destructible_impl<typename
               remove_all_extents<_Tp>::type>::type
    { };

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, true, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, false, true>
    : public true_type { };


  template<typename _Tp>
    struct is_destructible
    : public __is_destructible_safe<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };





  struct __do_is_nt_destructible_impl
  {
    template<typename _Tp>
      static __bool_constant<noexcept(declval<_Tp&>().~_Tp())>
      __test(int);

    template<typename>
      static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_nt_destructible_impl
    : public __do_is_nt_destructible_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp,
           bool = __or_<is_void<_Tp>,
                        __is_array_unknown_bounds<_Tp>,
                        is_function<_Tp>>::value,
           bool = __or_<is_reference<_Tp>, is_scalar<_Tp>>::value>
    struct __is_nt_destructible_safe;

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, false, false>
    : public __is_nt_destructible_impl<typename
               remove_all_extents<_Tp>::type>::type
    { };

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, true, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, false, true>
    : public true_type { };


  template<typename _Tp>
    struct is_nothrow_destructible
    : public __is_nt_destructible_safe<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, typename... _Args>
    struct __is_constructible_impl
    : public __bool_constant<__is_constructible(_Tp, _Args...)>
    { };


  template<typename _Tp, typename... _Args>
    struct is_constructible
      : public __is_constructible_impl<_Tp, _Args...>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_default_constructible
    : public __is_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_copy_constructible_impl;

  template<typename _Tp>
    struct __is_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_copy_constructible_impl<_Tp, true>
    : public __is_constructible_impl<_Tp, const _Tp&>
    { };


  template<typename _Tp>
    struct is_copy_constructible
    : public __is_copy_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_move_constructible_impl;

  template<typename _Tp>
    struct __is_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_move_constructible_impl<_Tp, true>
    : public __is_constructible_impl<_Tp, _Tp&&>
    { };


  template<typename _Tp>
    struct is_move_constructible
    : public __is_move_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<bool, typename _Tp, typename... _Args>
    struct __is_nt_constructible_impl
    : public false_type
    { };

  template<typename _Tp, typename... _Args>
    struct __is_nt_constructible_impl<true, _Tp, _Args...>
    : public __bool_constant<noexcept(_Tp(std::declval<_Args>()...))>
    { };

  template<typename _Tp, typename _Arg>
    struct __is_nt_constructible_impl<true, _Tp, _Arg>
    : public __bool_constant<noexcept(static_cast<_Tp>(std::declval<_Arg>()))>
    { };

  template<typename _Tp>
    struct __is_nt_constructible_impl<true, _Tp>
    : public __bool_constant<noexcept(_Tp())>
    { };

  template<typename _Tp, size_t _Num>
    struct __is_nt_constructible_impl<true, _Tp[_Num]>
    : public __bool_constant<noexcept(typename remove_all_extents<_Tp>::type())>
    { };
# 1001 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp, typename... _Args>
    using __is_nothrow_constructible_impl
      = __is_nt_constructible_impl<__is_constructible(_Tp, _Args...),
       _Tp, _Args...>;


  template<typename _Tp, typename... _Args>
    struct is_nothrow_constructible
    : public __is_nothrow_constructible_impl<_Tp, _Args...>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_nothrow_default_constructible
    : public __is_nothrow_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nothrow_copy_constructible_impl;

  template<typename _Tp>
    struct __is_nothrow_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nothrow_copy_constructible_impl<_Tp, true>
    : public __is_nothrow_constructible_impl<_Tp, const _Tp&>
    { };


  template<typename _Tp>
    struct is_nothrow_copy_constructible
    : public __is_nothrow_copy_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nothrow_move_constructible_impl;

  template<typename _Tp>
    struct __is_nothrow_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nothrow_move_constructible_impl<_Tp, true>
    : public __is_nothrow_constructible_impl<_Tp, _Tp&&>
    { };


  template<typename _Tp>
    struct is_nothrow_move_constructible
    : public __is_nothrow_move_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_assignable
    : public __bool_constant<__is_assignable(_Tp, _Up)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_copy_assignable_impl;

  template<typename _Tp>
    struct __is_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_copy_assignable_impl<_Tp, true>
    : public __bool_constant<__is_assignable(_Tp&, const _Tp&)>
    { };


  template<typename _Tp>
    struct is_copy_assignable
    : public __is_copy_assignable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_move_assignable_impl;

  template<typename _Tp>
    struct __is_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_move_assignable_impl<_Tp, true>
    : public __bool_constant<__is_assignable(_Tp&, _Tp&&)>
    { };


  template<typename _Tp>
    struct is_move_assignable
    : public __is_move_assignable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, typename _Up>
    struct __is_nt_assignable_impl
    : public integral_constant<bool, noexcept(declval<_Tp>() = declval<_Up>())>
    { };

  template<typename _Tp, typename _Up>
    struct __is_nothrow_assignable_impl
    : public __and_<__bool_constant<__is_assignable(_Tp, _Up)>,
      __is_nt_assignable_impl<_Tp, _Up>>
    { };


  template<typename _Tp, typename _Up>
    struct is_nothrow_assignable
    : public __is_nothrow_assignable_impl<_Tp, _Up>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nt_copy_assignable_impl;

  template<typename _Tp>
    struct __is_nt_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_copy_assignable_impl<_Tp, true>
    : public __is_nothrow_assignable_impl<_Tp&, const _Tp&>
    { };


  template<typename _Tp>
    struct is_nothrow_copy_assignable
    : public __is_nt_copy_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nt_move_assignable_impl;

  template<typename _Tp>
    struct __is_nt_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_move_assignable_impl<_Tp, true>
    : public __is_nothrow_assignable_impl<_Tp&, _Tp&&>
    { };


  template<typename _Tp>
    struct is_nothrow_move_assignable
    : public __is_nt_move_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    struct is_trivially_constructible
    : public __bool_constant<__is_trivially_constructible(_Tp, _Args...)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_default_constructible
    : public __bool_constant<__is_trivially_constructible(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  struct __do_is_implicitly_default_constructible_impl
  {
    template <typename _Tp>
    static void __helper(const _Tp&);

    template <typename _Tp>
    static true_type __test(const _Tp&,
                            decltype(__helper<const _Tp&>({}))* = 0);

    static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_implicitly_default_constructible_impl
    : public __do_is_implicitly_default_constructible_impl
    {
      typedef decltype(__test(declval<_Tp>())) type;
    };

  template<typename _Tp>
    struct __is_implicitly_default_constructible_safe
    : public __is_implicitly_default_constructible_impl<_Tp>::type
    { };

  template <typename _Tp>
    struct __is_implicitly_default_constructible
    : public __and_<__is_constructible_impl<_Tp>,
      __is_implicitly_default_constructible_safe<_Tp>>
    { };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_copy_constructible_impl;

  template<typename _Tp>
    struct __is_trivially_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_copy_constructible_impl<_Tp, true>
    : public __and_<__is_copy_constructible_impl<_Tp>,
      integral_constant<bool,
   __is_trivially_constructible(_Tp, const _Tp&)>>
    { };


  template<typename _Tp>
    struct is_trivially_copy_constructible
    : public __is_trivially_copy_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_move_constructible_impl;

  template<typename _Tp>
    struct __is_trivially_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_move_constructible_impl<_Tp, true>
    : public __and_<__is_move_constructible_impl<_Tp>,
      integral_constant<bool,
   __is_trivially_constructible(_Tp, _Tp&&)>>
    { };


  template<typename _Tp>
    struct is_trivially_move_constructible
    : public __is_trivially_move_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_trivially_assignable
    : public __bool_constant<__is_trivially_assignable(_Tp, _Up)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_copy_assignable_impl;

  template<typename _Tp>
    struct __is_trivially_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_copy_assignable_impl<_Tp, true>
    : public __bool_constant<__is_trivially_assignable(_Tp&, const _Tp&)>
    { };


  template<typename _Tp>
    struct is_trivially_copy_assignable
    : public __is_trivially_copy_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_move_assignable_impl;

  template<typename _Tp>
    struct __is_trivially_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_move_assignable_impl<_Tp, true>
    : public __bool_constant<__is_trivially_assignable(_Tp&, _Tp&&)>
    { };


  template<typename _Tp>
    struct is_trivially_move_assignable
    : public __is_trivially_move_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_destructible
    : public __and_<__is_destructible_safe<_Tp>,
      __bool_constant<__has_trivial_destructor(_Tp)>>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    struct has_virtual_destructor
    : public integral_constant<bool, __has_virtual_destructor(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };





  template<typename _Tp>
    struct alignment_of
    : public integral_constant<std::size_t, alignof(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename>
    struct rank
    : public integral_constant<std::size_t, 0> { };

  template<typename _Tp, std::size_t _Size>
    struct rank<_Tp[_Size]>
    : public integral_constant<std::size_t, 1 + rank<_Tp>::value> { };

  template<typename _Tp>
    struct rank<_Tp[]>
    : public integral_constant<std::size_t, 1 + rank<_Tp>::value> { };


  template<typename, unsigned _Uint>
    struct extent
    : public integral_constant<std::size_t, 0> { };

  template<typename _Tp, unsigned _Uint, std::size_t _Size>
    struct extent<_Tp[_Size], _Uint>
    : public integral_constant<std::size_t,
          _Uint == 0 ? _Size : extent<_Tp,
          _Uint - 1>::value>
    { };

  template<typename _Tp, unsigned _Uint>
    struct extent<_Tp[], _Uint>
    : public integral_constant<std::size_t,
          _Uint == 0 ? 0 : extent<_Tp,
             _Uint - 1>::value>
    { };





  template<typename _Tp, typename _Up>
    struct is_same

    : public integral_constant<bool, __is_same(_Tp, _Up)>



    { };
# 1410 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Base, typename _Derived>
    struct is_base_of
    : public integral_constant<bool, __is_base_of(_Base, _Derived)>
    { };

  template<typename _From, typename _To,
           bool = __or_<is_void<_From>, is_function<_To>,
                        is_array<_To>>::value>
    struct __is_convertible_helper
    {
      typedef typename is_void<_To>::type type;
    };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  template<typename _From, typename _To>
    class __is_convertible_helper<_From, _To, false>
    {
      template<typename _To1>
 static void __test_aux(_To1) noexcept;

      template<typename _From1, typename _To1,
        typename = decltype(__test_aux<_To1>(std::declval<_From1>()))>
 static true_type
 __test(int);

      template<typename, typename>
 static false_type
 __test(...);

    public:
      typedef decltype(__test<_From, _To>(0)) type;
    };
#pragma GCC diagnostic pop


  template<typename _From, typename _To>
    struct is_convertible
    : public __is_convertible_helper<_From, _To>::type
    { };


  template<typename _ToElementType, typename _FromElementType>
    using __is_array_convertible
      = is_convertible<_FromElementType(*)[], _ToElementType(*)[]>;

  template<typename _From, typename _To,
           bool = __or_<is_void<_From>, is_function<_To>,
                        is_array<_To>>::value>
    struct __is_nt_convertible_helper
    : is_void<_To>
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  template<typename _From, typename _To>
    class __is_nt_convertible_helper<_From, _To, false>
    {
      template<typename _To1>
 static void __test_aux(_To1) noexcept;

      template<typename _From1, typename _To1>
 static
 __bool_constant<noexcept(__test_aux<_To1>(std::declval<_From1>()))>
 __test(int);

      template<typename, typename>
 static false_type
 __test(...);

    public:
      using type = decltype(__test<_From, _To>(0));
    };
#pragma GCC diagnostic pop


  template<typename _From, typename _To>
    struct __is_nothrow_convertible
    : public __is_nt_convertible_helper<_From, _To>::type
    { };
# 1508 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp>
    struct remove_const
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_const<_Tp const>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_volatile
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_volatile<_Tp volatile>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_cv
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<const _Tp>
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<volatile _Tp>
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<const volatile _Tp>
    { using type = _Tp; };


  template<typename _Tp>
    struct add_const
    { typedef _Tp const type; };


  template<typename _Tp>
    struct add_volatile
    { typedef _Tp volatile type; };


  template<typename _Tp>
    struct add_cv
    {
      typedef typename
      add_const<typename add_volatile<_Tp>::type>::type type;
    };






  template<typename _Tp>
    using remove_const_t = typename remove_const<_Tp>::type;


  template<typename _Tp>
    using remove_volatile_t = typename remove_volatile<_Tp>::type;


  template<typename _Tp>
    using remove_cv_t = typename remove_cv<_Tp>::type;


  template<typename _Tp>
    using add_const_t = typename add_const<_Tp>::type;


  template<typename _Tp>
    using add_volatile_t = typename add_volatile<_Tp>::type;


  template<typename _Tp>
    using add_cv_t = typename add_cv<_Tp>::type;





  template<typename _Tp>
    struct remove_reference
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_reference<_Tp&>
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_reference<_Tp&&>
    { typedef _Tp type; };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __add_lvalue_reference_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_lvalue_reference_helper<_Tp, true>
    { typedef _Tp& type; };


  template<typename _Tp>
    struct add_lvalue_reference
    : public __add_lvalue_reference_helper<_Tp>
    { };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __add_rvalue_reference_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_rvalue_reference_helper<_Tp, true>
    { typedef _Tp&& type; };


  template<typename _Tp>
    struct add_rvalue_reference
    : public __add_rvalue_reference_helper<_Tp>
    { };



  template<typename _Tp>
    using remove_reference_t = typename remove_reference<_Tp>::type;


  template<typename _Tp>
    using add_lvalue_reference_t = typename add_lvalue_reference<_Tp>::type;


  template<typename _Tp>
    using add_rvalue_reference_t = typename add_rvalue_reference<_Tp>::type;





  template<typename _Unqualified, bool _IsConst, bool _IsVol>
    struct __cv_selector;

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, false, false>
    { typedef _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, false, true>
    { typedef volatile _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, true, false>
    { typedef const _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, true, true>
    { typedef const volatile _Unqualified __type; };

  template<typename _Qualified, typename _Unqualified,
    bool _IsConst = is_const<_Qualified>::value,
    bool _IsVol = is_volatile<_Qualified>::value>
    class __match_cv_qualifiers
    {
      typedef __cv_selector<_Unqualified, _IsConst, _IsVol> __match;

    public:
      typedef typename __match::__type __type;
    };


  template<typename _Tp>
    struct __make_unsigned
    { typedef _Tp __type; };

  template<>
    struct __make_unsigned<char>
    { typedef unsigned char __type; };

  template<>
    struct __make_unsigned<signed char>
    { typedef unsigned char __type; };

  template<>
    struct __make_unsigned<short>
    { typedef unsigned short __type; };

  template<>
    struct __make_unsigned<int>
    { typedef unsigned int __type; };

  template<>
    struct __make_unsigned<long>
    { typedef unsigned long __type; };

  template<>
    struct __make_unsigned<long long>
    { typedef unsigned long long __type; };
# 1730 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp,
    bool _IsInt = is_integral<_Tp>::value,
    bool _IsEnum = is_enum<_Tp>::value>
    class __make_unsigned_selector;

  template<typename _Tp>
    class __make_unsigned_selector<_Tp, true, false>
    {
      using __unsigned_type
 = typename __make_unsigned<__remove_cv_t<_Tp>>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __unsigned_type>::__type;
    };

  class __make_unsigned_selector_base
  {
  protected:
    template<typename...> struct _List { };

    template<typename _Tp, typename... _Up>
      struct _List<_Tp, _Up...> : _List<_Up...>
      { static constexpr size_t __size = sizeof(_Tp); };

    template<size_t _Sz, typename _Tp, bool = (_Sz <= _Tp::__size)>
      struct __select;

    template<size_t _Sz, typename _Uint, typename... _UInts>
      struct __select<_Sz, _List<_Uint, _UInts...>, true>
      { using __type = _Uint; };

    template<size_t _Sz, typename _Uint, typename... _UInts>
      struct __select<_Sz, _List<_Uint, _UInts...>, false>
      : __select<_Sz, _List<_UInts...>>
      { };
  };


  template<typename _Tp>
    class __make_unsigned_selector<_Tp, false, true>
    : __make_unsigned_selector_base
    {

      using _UInts = _List<unsigned char, unsigned short, unsigned int,
      unsigned long, unsigned long long>;

      using __unsigned_type = typename __select<sizeof(_Tp), _UInts>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __unsigned_type>::__type;
    };






  template<>
    struct __make_unsigned<wchar_t>
    {
      using __type
 = typename __make_unsigned_selector<wchar_t, false, true>::__type;
    };
# 1806 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<>
    struct __make_unsigned<char16_t>
    {
      using __type
 = typename __make_unsigned_selector<char16_t, false, true>::__type;
    };

  template<>
    struct __make_unsigned<char32_t>
    {
      using __type
 = typename __make_unsigned_selector<char32_t, false, true>::__type;
    };





  template<typename _Tp>
    struct make_unsigned
    { typedef typename __make_unsigned_selector<_Tp>::__type type; };


  template<>
    struct make_unsigned<bool>;



  template<typename _Tp>
    struct __make_signed
    { typedef _Tp __type; };

  template<>
    struct __make_signed<char>
    { typedef signed char __type; };

  template<>
    struct __make_signed<unsigned char>
    { typedef signed char __type; };

  template<>
    struct __make_signed<unsigned short>
    { typedef signed short __type; };

  template<>
    struct __make_signed<unsigned int>
    { typedef signed int __type; };

  template<>
    struct __make_signed<unsigned long>
    { typedef signed long __type; };

  template<>
    struct __make_signed<unsigned long long>
    { typedef signed long long __type; };
# 1884 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<typename _Tp,
    bool _IsInt = is_integral<_Tp>::value,
    bool _IsEnum = is_enum<_Tp>::value>
    class __make_signed_selector;

  template<typename _Tp>
    class __make_signed_selector<_Tp, true, false>
    {
      using __signed_type
 = typename __make_signed<__remove_cv_t<_Tp>>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __signed_type>::__type;
    };


  template<typename _Tp>
    class __make_signed_selector<_Tp, false, true>
    {
      typedef typename __make_unsigned_selector<_Tp>::__type __unsigned_type;

    public:
      typedef typename __make_signed_selector<__unsigned_type>::__type __type;
    };






  template<>
    struct __make_signed<wchar_t>
    {
      using __type
 = typename __make_signed_selector<wchar_t, false, true>::__type;
    };
# 1932 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<>
    struct __make_signed<char16_t>
    {
      using __type
 = typename __make_signed_selector<char16_t, false, true>::__type;
    };

  template<>
    struct __make_signed<char32_t>
    {
      using __type
 = typename __make_signed_selector<char32_t, false, true>::__type;
    };





  template<typename _Tp>
    struct make_signed
    { typedef typename __make_signed_selector<_Tp>::__type type; };


  template<>
    struct make_signed<bool>;



  template<typename _Tp>
    using make_signed_t = typename make_signed<_Tp>::type;


  template<typename _Tp>
    using make_unsigned_t = typename make_unsigned<_Tp>::type;





  template<typename _Tp>
    struct remove_extent
    { typedef _Tp type; };

  template<typename _Tp, std::size_t _Size>
    struct remove_extent<_Tp[_Size]>
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_extent<_Tp[]>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_all_extents
    { typedef _Tp type; };

  template<typename _Tp, std::size_t _Size>
    struct remove_all_extents<_Tp[_Size]>
    { typedef typename remove_all_extents<_Tp>::type type; };

  template<typename _Tp>
    struct remove_all_extents<_Tp[]>
    { typedef typename remove_all_extents<_Tp>::type type; };



  template<typename _Tp>
    using remove_extent_t = typename remove_extent<_Tp>::type;


  template<typename _Tp>
    using remove_all_extents_t = typename remove_all_extents<_Tp>::type;




  template<typename _Tp, typename>
    struct __remove_pointer_helper
    { typedef _Tp type; };

  template<typename _Tp, typename _Up>
    struct __remove_pointer_helper<_Tp, _Up*>
    { typedef _Up type; };


  template<typename _Tp>
    struct remove_pointer
    : public __remove_pointer_helper<_Tp, __remove_cv_t<_Tp>>
    { };


  template<typename _Tp, bool = __or_<__is_referenceable<_Tp>,
          is_void<_Tp>>::value>
    struct __add_pointer_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_pointer_helper<_Tp, true>
    { typedef typename remove_reference<_Tp>::type* type; };

  template<typename _Tp>
    struct add_pointer
    : public __add_pointer_helper<_Tp>
    { };



  template<typename _Tp>
    using remove_pointer_t = typename remove_pointer<_Tp>::type;


  template<typename _Tp>
    using add_pointer_t = typename add_pointer<_Tp>::type;


  template<std::size_t _Len>
    struct __aligned_storage_msa
    {
      union __type
      {
 unsigned char __data[_Len];
 struct __attribute__((__aligned__)) { } __align;
      };
    };
# 2067 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template<std::size_t _Len, std::size_t _Align =
    __alignof__(typename __aligned_storage_msa<_Len>::__type)>
    struct aligned_storage
    {
      union type
      {
 unsigned char __data[_Len];
 struct __attribute__((__aligned__((_Align)))) { } __align;
      };
    };

  template <typename... _Types>
    struct __strictest_alignment
    {
      static const size_t _S_alignment = 0;
      static const size_t _S_size = 0;
    };

  template <typename _Tp, typename... _Types>
    struct __strictest_alignment<_Tp, _Types...>
    {
      static const size_t _S_alignment =
        alignof(_Tp) > __strictest_alignment<_Types...>::_S_alignment
 ? alignof(_Tp) : __strictest_alignment<_Types...>::_S_alignment;
      static const size_t _S_size =
        sizeof(_Tp) > __strictest_alignment<_Types...>::_S_size
 ? sizeof(_Tp) : __strictest_alignment<_Types...>::_S_size;
    };
# 2106 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template <size_t _Len, typename... _Types>
    struct aligned_union
    {
    private:
      static_assert(sizeof...(_Types) != 0, "At least one type is required");

      using __strictest = __strictest_alignment<_Types...>;
      static const size_t _S_len = _Len > __strictest::_S_size
 ? _Len : __strictest::_S_size;
    public:

      static const size_t alignment_value = __strictest::_S_alignment;

      typedef typename aligned_storage<_S_len, alignment_value>::type type;
    };

  template <size_t _Len, typename... _Types>
    const size_t aligned_union<_Len, _Types...>::alignment_value;



  template<typename _Up,
    bool _IsArray = is_array<_Up>::value,
    bool _IsFunction = is_function<_Up>::value>
    struct __decay_selector;


  template<typename _Up>
    struct __decay_selector<_Up, false, false>
    { typedef __remove_cv_t<_Up> __type; };

  template<typename _Up>
    struct __decay_selector<_Up, true, false>
    { typedef typename remove_extent<_Up>::type* __type; };

  template<typename _Up>
    struct __decay_selector<_Up, false, true>
    { typedef typename add_pointer<_Up>::type __type; };


  template<typename _Tp>
    class decay
    {
      typedef typename remove_reference<_Tp>::type __remove_type;

    public:
      typedef typename __decay_selector<__remove_type>::__type type;
    };


  template<typename _Tp>
    using __decay_t = typename decay<_Tp>::type;

  template<typename _Tp>
    class reference_wrapper;


  template<typename _Tp>
    struct __strip_reference_wrapper
    {
      typedef _Tp __type;
    };

  template<typename _Tp>
    struct __strip_reference_wrapper<reference_wrapper<_Tp> >
    {
      typedef _Tp& __type;
    };

  template<typename _Tp>
    using __decay_and_strip = __strip_reference_wrapper<__decay_t<_Tp>>;




  template<bool, typename _Tp = void>
    struct enable_if
    { };


  template<typename _Tp>
    struct enable_if<true, _Tp>
    { typedef _Tp type; };


  template<bool _Cond, typename _Tp = void>
    using __enable_if_t = typename enable_if<_Cond, _Tp>::type;

  template<typename... _Cond>
    using _Require = __enable_if_t<__and_<_Cond...>::value>;



  template<bool _Cond, typename _Iftrue, typename _Iffalse>
    struct conditional
    { typedef _Iftrue type; };


  template<typename _Iftrue, typename _Iffalse>
    struct conditional<false, _Iftrue, _Iffalse>
    { typedef _Iffalse type; };


  template<typename _Tp>
    using __remove_cvref_t
     = typename remove_cv<typename remove_reference<_Tp>::type>::type;


  template<typename... _Tp>
    struct common_type;



  struct __do_common_type_impl
  {
    template<typename _Tp, typename _Up>
      using __cond_t
 = decltype(true ? std::declval<_Tp>() : std::declval<_Up>());



    template<typename _Tp, typename _Up>
      static __success_type<__decay_t<__cond_t<_Tp, _Up>>>
      _S_test(int);
# 2239 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
    template<typename, typename>
      static __failure_type
      _S_test_2(...);

    template<typename _Tp, typename _Up>
      static decltype(_S_test_2<_Tp, _Up>(0))
      _S_test(...);
  };


  template<>
    struct common_type<>
    { };


  template<typename _Tp0>
    struct common_type<_Tp0>
    : public common_type<_Tp0, _Tp0>
    { };


  template<typename _Tp1, typename _Tp2,
    typename _Dp1 = __decay_t<_Tp1>, typename _Dp2 = __decay_t<_Tp2>>
    struct __common_type_impl
    {


      using type = common_type<_Dp1, _Dp2>;
    };

  template<typename _Tp1, typename _Tp2>
    struct __common_type_impl<_Tp1, _Tp2, _Tp1, _Tp2>
    : private __do_common_type_impl
    {


      using type = decltype(_S_test<_Tp1, _Tp2>(0));
    };


  template<typename _Tp1, typename _Tp2>
    struct common_type<_Tp1, _Tp2>
    : public __common_type_impl<_Tp1, _Tp2>::type
    { };

  template<typename...>
    struct __common_type_pack
    { };

  template<typename, typename, typename = void>
    struct __common_type_fold;


  template<typename _Tp1, typename _Tp2, typename... _Rp>
    struct common_type<_Tp1, _Tp2, _Rp...>
    : public __common_type_fold<common_type<_Tp1, _Tp2>,
    __common_type_pack<_Rp...>>
    { };




  template<typename _CTp, typename... _Rp>
    struct __common_type_fold<_CTp, __common_type_pack<_Rp...>,
         __void_t<typename _CTp::type>>
    : public common_type<typename _CTp::type, _Rp...>
    { };


  template<typename _CTp, typename _Rp>
    struct __common_type_fold<_CTp, _Rp, void>
    { };

  template<typename _Tp, bool = is_enum<_Tp>::value>
    struct __underlying_type_impl
    {
      using type = __underlying_type(_Tp);
    };

  template<typename _Tp>
    struct __underlying_type_impl<_Tp, false>
    { };


  template<typename _Tp>
    struct underlying_type
    : public __underlying_type_impl<_Tp>
    { };

  template<typename _Tp>
    struct __declval_protector
    {
      static const bool __stop = false;
    };

  template<typename _Tp>
    auto declval() noexcept -> decltype(__declval<_Tp>(0))
    {
      static_assert(__declval_protector<_Tp>::__stop,
      "declval() must not be used!");
      return __declval<_Tp>(0);
    }


  template<typename _Signature>
    class result_of;





  struct __invoke_memfun_ref { };
  struct __invoke_memfun_deref { };
  struct __invoke_memobj_ref { };
  struct __invoke_memobj_deref { };
  struct __invoke_other { };


  template<typename _Tp, typename _Tag>
    struct __result_of_success : __success_type<_Tp>
    { using __invoke_type = _Tag; };


  struct __result_of_memfun_ref_impl
  {
    template<typename _Fp, typename _Tp1, typename... _Args>
      static __result_of_success<decltype(
      (std::declval<_Tp1>().*std::declval<_Fp>())(std::declval<_Args>()...)
      ), __invoke_memfun_ref> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun_ref
    : private __result_of_memfun_ref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg, _Args...>(0)) type;
    };


  struct __result_of_memfun_deref_impl
  {
    template<typename _Fp, typename _Tp1, typename... _Args>
      static __result_of_success<decltype(
      ((*std::declval<_Tp1>()).*std::declval<_Fp>())(std::declval<_Args>()...)
      ), __invoke_memfun_deref> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun_deref
    : private __result_of_memfun_deref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg, _Args...>(0)) type;
    };


  struct __result_of_memobj_ref_impl
  {
    template<typename _Fp, typename _Tp1>
      static __result_of_success<decltype(
      std::declval<_Tp1>().*std::declval<_Fp>()
      ), __invoke_memobj_ref> _S_test(int);

    template<typename, typename>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj_ref
    : private __result_of_memobj_ref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg>(0)) type;
    };


  struct __result_of_memobj_deref_impl
  {
    template<typename _Fp, typename _Tp1>
      static __result_of_success<decltype(
      (*std::declval<_Tp1>()).*std::declval<_Fp>()
      ), __invoke_memobj_deref> _S_test(int);

    template<typename, typename>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj_deref
    : private __result_of_memobj_deref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg>(0)) type;
    };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj;

  template<typename _Res, typename _Class, typename _Arg>
    struct __result_of_memobj<_Res _Class::*, _Arg>
    {
      typedef __remove_cvref_t<_Arg> _Argval;
      typedef _Res _Class::* _MemPtr;
      typedef typename conditional<__or_<is_same<_Argval, _Class>,
        is_base_of<_Class, _Argval>>::value,
        __result_of_memobj_ref<_MemPtr, _Arg>,
        __result_of_memobj_deref<_MemPtr, _Arg>
      >::type::type type;
    };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun;

  template<typename _Res, typename _Class, typename _Arg, typename... _Args>
    struct __result_of_memfun<_Res _Class::*, _Arg, _Args...>
    {
      typedef typename remove_reference<_Arg>::type _Argval;
      typedef _Res _Class::* _MemPtr;
      typedef typename conditional<is_base_of<_Class, _Argval>::value,
        __result_of_memfun_ref<_MemPtr, _Arg, _Args...>,
        __result_of_memfun_deref<_MemPtr, _Arg, _Args...>
      >::type::type type;
    };






  template<typename _Tp, typename _Up = __remove_cvref_t<_Tp>>
    struct __inv_unwrap
    {
      using type = _Tp;
    };

  template<typename _Tp, typename _Up>
    struct __inv_unwrap<_Tp, reference_wrapper<_Up>>
    {
      using type = _Up&;
    };

  template<bool, bool, typename _Functor, typename... _ArgTypes>
    struct __result_of_impl
    {
      typedef __failure_type type;
    };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_impl<true, false, _MemPtr, _Arg>
    : public __result_of_memobj<__decay_t<_MemPtr>,
    typename __inv_unwrap<_Arg>::type>
    { };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_impl<false, true, _MemPtr, _Arg, _Args...>
    : public __result_of_memfun<__decay_t<_MemPtr>,
    typename __inv_unwrap<_Arg>::type, _Args...>
    { };


  struct __result_of_other_impl
  {
    template<typename _Fn, typename... _Args>
      static __result_of_success<decltype(
      std::declval<_Fn>()(std::declval<_Args>()...)
      ), __invoke_other> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _Functor, typename... _ArgTypes>
    struct __result_of_impl<false, false, _Functor, _ArgTypes...>
    : private __result_of_other_impl
    {
      typedef decltype(_S_test<_Functor, _ArgTypes...>(0)) type;
    };


  template<typename _Functor, typename... _ArgTypes>
    struct __invoke_result
    : public __result_of_impl<
        is_member_object_pointer<
          typename remove_reference<_Functor>::type
        >::value,
        is_member_function_pointer<
          typename remove_reference<_Functor>::type
        >::value,
 _Functor, _ArgTypes...
      >::type
    { };

  template<typename _Functor, typename... _ArgTypes>
    struct result_of<_Functor(_ArgTypes...)>
    : public __invoke_result<_Functor, _ArgTypes...>
    { };



  template<size_t _Len, size_t _Align =
     __alignof__(typename __aligned_storage_msa<_Len>::__type)>
    using aligned_storage_t = typename aligned_storage<_Len, _Align>::type;

  template <size_t _Len, typename... _Types>
    using aligned_union_t = typename aligned_union<_Len, _Types...>::type;


  template<typename _Tp>
    using decay_t = typename decay<_Tp>::type;


  template<bool _Cond, typename _Tp = void>
    using enable_if_t = typename enable_if<_Cond, _Tp>::type;


  template<bool _Cond, typename _Iftrue, typename _Iffalse>
    using conditional_t = typename conditional<_Cond, _Iftrue, _Iffalse>::type;


  template<typename... _Tp>
    using common_type_t = typename common_type<_Tp...>::type;


  template<typename _Tp>
    using underlying_type_t = typename underlying_type<_Tp>::type;


  template<typename _Tp>
    using result_of_t = typename result_of<_Tp>::type;





  template<typename...> using void_t = void;



  template<typename _Default, typename _AlwaysVoid,
    template<typename...> class _Op, typename... _Args>
    struct __detector
    {
      using value_t = false_type;
      using type = _Default;
    };


  template<typename _Default, template<typename...> class _Op,
     typename... _Args>
    struct __detector<_Default, __void_t<_Op<_Args...>>, _Op, _Args...>
    {
      using value_t = true_type;
      using type = _Op<_Args...>;
    };


  template<typename _Default, template<typename...> class _Op,
    typename... _Args>
    using __detected_or = __detector<_Default, void, _Op, _Args...>;


  template<typename _Default, template<typename...> class _Op,
    typename... _Args>
    using __detected_or_t
      = typename __detected_or<_Default, _Op, _Args...>::type;
# 2624 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
  template <typename _Tp>
    struct __is_swappable;

  template <typename _Tp>
    struct __is_nothrow_swappable;

  template<typename... _Elements>
    class tuple;

  template<typename>
    struct __is_tuple_like_impl : false_type
    { };

  template<typename... _Tps>
    struct __is_tuple_like_impl<tuple<_Tps...>> : true_type
    { };


  template<typename _Tp>
    struct __is_tuple_like
    : public __is_tuple_like_impl<__remove_cvref_t<_Tp>>::type
    { };

  template<typename _Tp>

    __inline__ __attribute__((always_inline))
    _Require<__not_<__is_tuple_like<_Tp>>,
      is_move_constructible<_Tp>,
      is_move_assignable<_Tp>>
    swap(_Tp&, _Tp&)
    noexcept(__and_<is_nothrow_move_constructible<_Tp>,
             is_nothrow_move_assignable<_Tp>>::value);

  template<typename _Tp, size_t _Nm>

    __inline__ __attribute__((always_inline))
    __enable_if_t<__is_swappable<_Tp>::value>
    swap(_Tp (&__a)[_Nm], _Tp (&__b)[_Nm])
    noexcept(__is_nothrow_swappable<_Tp>::value);

  namespace __swappable_details {
    using std::swap;

    struct __do_is_swappable_impl
    {
      template<typename _Tp, typename
               = decltype(swap(std::declval<_Tp&>(), std::declval<_Tp&>()))>
        static true_type __test(int);

      template<typename>
        static false_type __test(...);
    };

    struct __do_is_nothrow_swappable_impl
    {
      template<typename _Tp>
        static __bool_constant<
          noexcept(swap(std::declval<_Tp&>(), std::declval<_Tp&>()))
        > __test(int);

      template<typename>
        static false_type __test(...);
    };

  }

  template<typename _Tp>
    struct __is_swappable_impl
    : public __swappable_details::__do_is_swappable_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp>
    struct __is_nothrow_swappable_impl
    : public __swappable_details::__do_is_nothrow_swappable_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp>
    struct __is_swappable
    : public __is_swappable_impl<_Tp>::type
    { };

  template<typename _Tp>
    struct __is_nothrow_swappable
    : public __is_nothrow_swappable_impl<_Tp>::type
    { };






  template<typename _Tp>
    struct is_swappable
    : public __is_swappable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_nothrow_swappable
    : public __is_nothrow_swappable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool is_swappable_v =
      is_swappable<_Tp>::value;


  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_swappable_v =
      is_nothrow_swappable<_Tp>::value;


  namespace __swappable_with_details {
    using std::swap;

    struct __do_is_swappable_with_impl
    {
      template<typename _Tp, typename _Up, typename
               = decltype(swap(std::declval<_Tp>(), std::declval<_Up>())),
               typename
               = decltype(swap(std::declval<_Up>(), std::declval<_Tp>()))>
        static true_type __test(int);

      template<typename, typename>
        static false_type __test(...);
    };

    struct __do_is_nothrow_swappable_with_impl
    {
      template<typename _Tp, typename _Up>
        static __bool_constant<
          noexcept(swap(std::declval<_Tp>(), std::declval<_Up>()))
          &&
          noexcept(swap(std::declval<_Up>(), std::declval<_Tp>()))
        > __test(int);

      template<typename, typename>
        static false_type __test(...);
    };

  }

  template<typename _Tp, typename _Up>
    struct __is_swappable_with_impl
    : public __swappable_with_details::__do_is_swappable_with_impl
    {
      typedef decltype(__test<_Tp, _Up>(0)) type;
    };


  template<typename _Tp>
    struct __is_swappable_with_impl<_Tp&, _Tp&>
    : public __swappable_details::__do_is_swappable_impl
    {
      typedef decltype(__test<_Tp&>(0)) type;
    };

  template<typename _Tp, typename _Up>
    struct __is_nothrow_swappable_with_impl
    : public __swappable_with_details::__do_is_nothrow_swappable_with_impl
    {
      typedef decltype(__test<_Tp, _Up>(0)) type;
    };


  template<typename _Tp>
    struct __is_nothrow_swappable_with_impl<_Tp&, _Tp&>
    : public __swappable_details::__do_is_nothrow_swappable_impl
    {
      typedef decltype(__test<_Tp&>(0)) type;
    };


  template<typename _Tp, typename _Up>
    struct is_swappable_with
    : public __is_swappable_with_impl<_Tp, _Up>::type
    { };


  template<typename _Tp, typename _Up>
    struct is_nothrow_swappable_with
    : public __is_nothrow_swappable_with_impl<_Tp, _Up>::type
    { };



  template<typename _Tp, typename _Up>
    __inline__ __attribute__((always_inline)) constexpr bool is_swappable_with_v =
      is_swappable_with<_Tp, _Up>::value;


  template<typename _Tp, typename _Up>
    __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_swappable_with_v =
      is_nothrow_swappable_with<_Tp, _Up>::value;







  template<typename _Result, typename _Ret,
    bool = is_void<_Ret>::value, typename = void>
    struct __is_invocable_impl : false_type { };


  template<typename _Result, typename _Ret>
    struct __is_invocable_impl<_Result, _Ret,
                                true,
          __void_t<typename _Result::type>>
    : true_type
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"

  template<typename _Result, typename _Ret>
    struct __is_invocable_impl<_Result, _Ret,
                                false,
          __void_t<typename _Result::type>>
    {
    private:


      static typename _Result::type _S_get();

      template<typename _Tp>
 static void _S_conv(_Tp);


      template<typename _Tp, typename = decltype(_S_conv<_Tp>(_S_get()))>
 static true_type
 _S_test(int);

      template<typename _Tp>
 static false_type
 _S_test(...);

    public:
      using type = decltype(_S_test<_Ret>(1));
    };
#pragma GCC diagnostic pop

  template<typename _Fn, typename... _ArgTypes>
    struct __is_invocable
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>::type
    { };

  template<typename _Fn, typename _Tp, typename... _Args>
    constexpr bool __call_is_nt(__invoke_memfun_ref)
    {
      using _Up = typename __inv_unwrap<_Tp>::type;
      return noexcept((std::declval<_Up>().*std::declval<_Fn>())(
     std::declval<_Args>()...));
    }

  template<typename _Fn, typename _Tp, typename... _Args>
    constexpr bool __call_is_nt(__invoke_memfun_deref)
    {
      return noexcept(((*std::declval<_Tp>()).*std::declval<_Fn>())(
     std::declval<_Args>()...));
    }

  template<typename _Fn, typename _Tp>
    constexpr bool __call_is_nt(__invoke_memobj_ref)
    {
      using _Up = typename __inv_unwrap<_Tp>::type;
      return noexcept(std::declval<_Up>().*std::declval<_Fn>());
    }

  template<typename _Fn, typename _Tp>
    constexpr bool __call_is_nt(__invoke_memobj_deref)
    {
      return noexcept((*std::declval<_Tp>()).*std::declval<_Fn>());
    }

  template<typename _Fn, typename... _Args>
    constexpr bool __call_is_nt(__invoke_other)
    {
      return noexcept(std::declval<_Fn>()(std::declval<_Args>()...));
    }

  template<typename _Result, typename _Fn, typename... _Args>
    struct __call_is_nothrow
    : __bool_constant<
 std::__call_is_nt<_Fn, _Args...>(typename _Result::__invoke_type{})
      >
    { };

  template<typename _Fn, typename... _Args>
    using __call_is_nothrow_
      = __call_is_nothrow<__invoke_result<_Fn, _Args...>, _Fn, _Args...>;


  template<typename _Fn, typename... _Args>
    struct __is_nothrow_invocable
    : __and_<__is_invocable<_Fn, _Args...>,
             __call_is_nothrow_<_Fn, _Args...>>::type
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  struct __nonesuchbase {};
  struct __nonesuch : private __nonesuchbase {
    ~__nonesuch() = delete;
    __nonesuch(__nonesuch const&) = delete;
    void operator=(__nonesuch const&) = delete;
  };
#pragma GCC diagnostic pop





  template<typename _Functor, typename... _ArgTypes>
    struct invoke_result
    : public __invoke_result<_Functor, _ArgTypes...>
    { };


  template<typename _Fn, typename... _Args>
    using invoke_result_t = typename invoke_result<_Fn, _Args...>::type;


  template<typename _Fn, typename... _ArgTypes>
    struct is_invocable
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
    };


  template<typename _Ret, typename _Fn, typename... _ArgTypes>
    struct is_invocable_r
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, _Ret>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
    };


  template<typename _Fn, typename... _ArgTypes>
    struct is_nothrow_invocable
    : __and_<__is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>,
      __call_is_nothrow_<_Fn, _ArgTypes...>>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
    };

  template<typename _Result, typename _Ret, typename = void>
    struct __is_nt_invocable_impl : false_type { };

  template<typename _Result, typename _Ret>
    struct __is_nt_invocable_impl<_Result, _Ret,
      __void_t<typename _Result::type>>
    : __or_<is_void<_Ret>,
     __is_nothrow_convertible<typename _Result::type, _Ret>>
    { };


  template<typename _Ret, typename _Fn, typename... _ArgTypes>
    struct is_nothrow_invocable_r
    : __and_<__is_nt_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, _Ret>,
             __call_is_nothrow_<_Fn, _ArgTypes...>>::type
    { };


  template<typename _Fn, typename... _Args>
    __inline__ __attribute__((always_inline)) constexpr bool is_invocable_v = is_invocable<_Fn, _Args...>::value;


  template<typename _Fn, typename... _Args>
    __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_invocable_v
      = is_nothrow_invocable<_Fn, _Args...>::value;


  template<typename _Ret, typename _Fn, typename... _Args>
    __inline__ __attribute__((always_inline)) constexpr bool is_invocable_r_v
      = is_invocable_r<_Ret, _Fn, _Args...>::value;


  template<typename _Ret, typename _Fn, typename... _Args>
    __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_invocable_r_v
      = is_nothrow_invocable_r<_Ret, _Fn, _Args...>::value;




template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_void_v = is_void<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_null_pointer_v = is_null_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_integral_v = is_integral<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_floating_point_v = is_floating_point<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_array_v = is_array<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_pointer_v = is_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_lvalue_reference_v =
    is_lvalue_reference<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_rvalue_reference_v =
    is_rvalue_reference<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_member_object_pointer_v =
    is_member_object_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_member_function_pointer_v =
    is_member_function_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_enum_v = is_enum<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_union_v = is_union<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_class_v = is_class<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_function_v = is_function<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_reference_v = is_reference<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_arithmetic_v = is_arithmetic<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_fundamental_v = is_fundamental<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_object_v = is_object<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_scalar_v = is_scalar<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_compound_v = is_compound<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_member_pointer_v = is_member_pointer<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_const_v = is_const<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_volatile_v = is_volatile<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivial_v = is_trivial<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_copyable_v =
    is_trivially_copyable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_standard_layout_v = is_standard_layout<_Tp>::value;
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
template <typename _Tp>

  __inline__ __attribute__((always_inline)) constexpr bool is_pod_v = is_pod<_Tp>::value;
#pragma GCC diagnostic pop
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_literal_type_v = is_literal_type<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_empty_v = is_empty<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_polymorphic_v = is_polymorphic<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_abstract_v = is_abstract<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_final_v = is_final<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_signed_v = is_signed<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_unsigned_v = is_unsigned<_Tp>::value;
template <typename _Tp, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_constructible_v =
    is_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_default_constructible_v =
    is_default_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_copy_constructible_v =
    is_copy_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_move_constructible_v =
    is_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_assignable_v = is_assignable<_Tp, _Up>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_copy_assignable_v = is_copy_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_move_assignable_v = is_move_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_destructible_v = is_destructible<_Tp>::value;
template <typename _Tp, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_constructible_v =
    is_trivially_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_default_constructible_v =
    is_trivially_default_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_copy_constructible_v =
    is_trivially_copy_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_move_constructible_v =
    is_trivially_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_assignable_v =
    is_trivially_assignable<_Tp, _Up>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_copy_assignable_v =
    is_trivially_copy_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_move_assignable_v =
    is_trivially_move_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_trivially_destructible_v =
    is_trivially_destructible<_Tp>::value;
template <typename _Tp, typename... _Args>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_constructible_v =
    is_nothrow_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_default_constructible_v =
    is_nothrow_default_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_copy_constructible_v =
    is_nothrow_copy_constructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_move_constructible_v =
    is_nothrow_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_assignable_v =
    is_nothrow_assignable<_Tp, _Up>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_copy_assignable_v =
    is_nothrow_copy_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_move_assignable_v =
    is_nothrow_move_assignable<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool is_nothrow_destructible_v =
    is_nothrow_destructible<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr bool has_virtual_destructor_v =
    has_virtual_destructor<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr size_t alignment_of_v = alignment_of<_Tp>::value;
template <typename _Tp>
  __inline__ __attribute__((always_inline)) constexpr size_t rank_v = rank<_Tp>::value;
template <typename _Tp, unsigned _Idx = 0>
  __inline__ __attribute__((always_inline)) constexpr size_t extent_v = extent<_Tp, _Idx>::value;

template <typename _Tp, typename _Up>
  __inline__ __attribute__((always_inline)) constexpr bool is_same_v = __is_same(_Tp, _Up);




template <typename _Base, typename _Derived>
  __inline__ __attribute__((always_inline)) constexpr bool is_base_of_v = is_base_of<_Base, _Derived>::value;
template <typename _From, typename _To>
  __inline__ __attribute__((always_inline)) constexpr bool is_convertible_v = is_convertible<_From, _To>::value;




  template<typename _Tp>
    struct has_unique_object_representations
    : bool_constant<__has_unique_object_representations(
      remove_cv_t<remove_all_extents_t<_Tp>>
      )>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool has_unique_object_representations_v
      = has_unique_object_representations<_Tp>::value;





  template<typename _Tp>
    struct is_aggregate
    : bool_constant<__is_aggregate(remove_cv_t<_Tp>)>
    { };


  template<typename _Tp>
    __inline__ __attribute__((always_inline)) constexpr bool is_aggregate_v = is_aggregate<_Tp>::value;
# 3456 "/usr/lib/gcc/aarch64-linux-gnu/10.3.1/../../../../include/c++/10.3.1/type_traits" 3
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h" 2

namespace AscendC {
template<typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V"))) void NopInPipeV(const T &tensor)
{
    (void)(0);
}
template<typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((out_pipe("V"))) void NopOutPipeV(const T &tensor)
{
    (void)(0);
}

[aicore] __inline__ __attribute__((always_inline)) constexpr bool IsAivTscm(TPosition src, TPosition dst)
{

    if (GetPosition(src, dst) == TPosition::TSCM) {
        return true;
    }




    return false;
}


template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TQueBind<src, dst, depth, mask>::TQueBind()
{



}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::AllocTensor()
{
    auto buf = AllocBuffer();
    return Buf2Tensor<T>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::FreeTensor(LocalTensor<T>& tensor)
{
    FreeBuffer(tensor.GetBufferHandle());
    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::EnQue(const LocalTensor<T>& tensor)
{
    if constexpr (GetPhyType(src) == Hardware::UB || GetPhyType(dst) == Hardware::UB) {
        NopInPipeV<LocalTensor<T>>(tensor);
    }
    auto buf = tensor.GetBufferHandle();
    return EnQue(reinterpret_cast<TBufHandle>(buf));
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos, typename T>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::EnQue(const LocalTensor<T>& tensor)
{
    if constexpr (GetPhyType(srcUserPos) == Hardware::UB || GetPhyType(dstUserPos) == Hardware::UB) {
        NopInPipeV<LocalTensor<T>>(tensor);
    }
    auto buf = tensor.GetBufferHandle();
    return EnQue<srcUserPos, dstUserPos>(reinterpret_cast<TBufHandle>(buf));
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::EnQue(TBufHandle buf)
{
    static_assert(((srcUserPos == TPosition::GM) || (srcUserPos == TPosition::VECIN) ||
                (srcUserPos == TPosition::VECOUT) || (srcUserPos == TPosition::VECCALC)) &&
                "enque only support src position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(((dstUserPos == TPosition::GM) || (dstUserPos == TPosition::VECIN) ||
                (dstUserPos == TPosition::VECOUT) || (dstUserPos == TPosition::VECCALC)) &&
                "enque only support dst position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(!((srcUserPos == TPosition::GM) && (dstUserPos == TPosition::GM)) &&
                "enque src and dst position cannot be GM at the same time.");
    constexpr Hardware srcUserHardType = GetPhyType(srcUserPos);
    constexpr Hardware dstUserHardType = GetPhyType(dstUserPos);
    constexpr HardEvent enQueUserEvt = GetQueEvt(srcUserHardType, dstUserHardType, true, false, false);




      ;
    auto ptr = reinterpret_cast<TBufType*>(buf);
    if constexpr (depth == 1) {
        this->que_ = buf;
    } else {
        this->que_[this->tail] = buf;
    }
    this->usedCount++;




      ;


      ;
                                                ;
                                             ;


    if constexpr (enQueUserEvt == HardEvent::V_V) {
        SetFlag<enQueUserEvt>(0);
        ptr->enQueEvtID = 0;
    } else {
        auto enQueUserEvtID = GetTPipePtr()->AllocEventID<enQueUserEvt>();
        SetFlag<enQueUserEvt>(enQueUserEvtID);
        ptr->enQueEvtID = enQueUserEvtID;
    }

    if constexpr (depth != 1) {
        if (++this->tail >= depth) {
            this->tail = 0;
        }
    }







    return true;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::EnQue(TBufHandle buf)
{



      ;
    auto ptr = reinterpret_cast<TBufType*>(buf);
    if constexpr (depth == 1) {
        this->que_ = buf;
    } else {
        this->que_[this->tail] = buf;
    }
    this->usedCount++;




      ;


      ;
                                             ;





    if (g_coreType != AIV || (GetPosition(src, dst) != TPosition::TSCM)) {
        auto enQueEvtID = GetTPipePtr()->AllocEventID<enQueEvt>();
        SetFlag<enQueEvt>(enQueEvtID);
        ptr->enQueEvtID = enQueEvtID;
    }





    if constexpr (depth != 1) {
        if (++this->tail >= depth) {
            this->tail = 0;
        }
    }







    return true;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> TQueBind<src, dst, depth, mask>::DeQue()
{
    auto buf = DeQue();
    auto ret = Buf2Tensor<T>(buf);
    if constexpr (GetPhyType(src) == Hardware::UB || GetPhyType(dst) == Hardware::UB) {
        NopOutPipeV<LocalTensor<T>>(ret);
    }
    return ret;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos, typename T>
[aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> TQueBind<src, dst, depth, mask>::DeQue()
{
    auto buf = DeQue<srcUserPos, dstUserPos>();
    auto ret = Buf2Tensor<T>(buf);
    if constexpr (GetPhyType(src) == Hardware::UB || GetPhyType(dst) == Hardware::UB) {
        NopOutPipeV<LocalTensor<T>>(ret);
    }
    return ret;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBufHandle TQueBind<src, dst, depth, mask>::DeQue()
{
    TBufHandle buf;
    if constexpr (depth == 1) {
        buf = this->que_;
    } else {
        buf = this->que_[this->head];
    }
                                                                                             ;
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;



      ;
    this->usedCount--;



                                             ;

    if (g_coreType != AIV || (GetPosition(src, dst) != TPosition::TSCM)) {
        if (ptr->enQueEvtID != INVALID_TEVENTID) {
            WaitFlag<enQueEvt>(ptr->enQueEvtID);
            GetTPipePtr()->ReleaseEventID<enQueEvt>(ptr->enQueEvtID);
            ptr->enQueEvtID = INVALID_TEVENTID;
        }
    }







    if constexpr (depth != 1) {
        if (++this->head >= depth) {
            this->head = 0;
        }
    }






    return reinterpret_cast<TBufHandle>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos>
[aicore] __inline__ __attribute__((always_inline)) TBufHandle TQueBind<src, dst, depth, mask>::DeQue()
{
    static_assert(((srcUserPos == TPosition::GM) || (srcUserPos == TPosition::VECIN) ||
                (srcUserPos == TPosition::VECOUT) || (srcUserPos == TPosition::VECCALC)) &&
                "DeQue only support src position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(((dstUserPos == TPosition::GM) || (dstUserPos == TPosition::VECIN) ||
                (dstUserPos == TPosition::VECOUT) || (dstUserPos == TPosition::VECCALC)) &&
                "DeQue only support dst position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(!((srcUserPos == TPosition::GM) && (dstUserPos == TPosition::GM)) &&
                "DeQue src and dst position cannot be GM at the same time.");
    constexpr Hardware srcUserHardType = GetPhyType(srcUserPos);
    constexpr Hardware dstUserHardType = GetPhyType(dstUserPos);
    constexpr HardEvent deQueUserEvt = GetQueEvt(srcUserHardType, dstUserHardType, true, false, false);

    TBufHandle buf;
    if constexpr (depth == 1) {
        buf = this->que_;
    } else {
        buf = this->que_[this->head];
    }
                                                                                             ;
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;



      ;
    this->usedCount--;


      ;
                                             ;


    if constexpr (deQueUserEvt == HardEvent::V_V) {
        WaitFlag<deQueUserEvt>(0);
        ptr->enQueEvtID = INVALID_TEVENTID;
    } else {
        if (ptr->enQueEvtID != INVALID_TEVENTID) {
            WaitFlag<deQueUserEvt>(ptr->enQueEvtID);
            GetTPipePtr()->ReleaseEventID<deQueUserEvt>(ptr->enQueEvtID);
            ptr->enQueEvtID = INVALID_TEVENTID;
        }
    }

    if constexpr (depth != 1) {
        if (++this->head >= depth) {
            this->head = 0;
        }
    }






    return reinterpret_cast<TBufHandle>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::FreeBuffer(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;


      ;
    if constexpr (!IsAivTscm(src, dst)) {
# 378 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
        ptr->freeBufEvtID = GetTPipePtr()->AllocEventID<freeBufEvt>();
        SetFlag<freeBufEvt>(ptr->freeBufEvtID);

    }
    ptr->state = TBufState::FREE;
    this->bufUsedCount--;






    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::SetTBufPoolHandle(uint64_t bufPoolHandle)
{



    (void)(bufPoolHandle);

}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBufHandle TQueBind<src, dst, depth, mask>::AllocBuffer()
{
                                ;


      ;
    TBufType* ret;
    do {
        ret = this->bufStart + this->bufCursor;
        if constexpr (config.bufferNumber != 1) {
            this->bufCursor += 1;
            if (this->bufCursor == this->bufNum) {
                this->bufCursor = 0;
            }
        }
        if (ret->state == TBufState::FREE) {
            ret->state = TBufState::OCCUPIED;
            if constexpr (IsAivTscm(src, dst)) {
                break;
            }
            if (ret->freeBufEvtID != INVALID_TEVENTID) {
                WaitFlag<freeBufEvt>(ret->freeBufEvtID);
                GetTPipePtr()->ReleaseEventID<freeBufEvt>(ret->freeBufEvtID);
                ret->freeBufEvtID = INVALID_TEVENTID;
            }
            break;
        }


          ;
    } while (true);
    this->bufUsedCount++;
# 449 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    return reinterpret_cast<TBufHandle>(ret);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::VacantInQue()
{
    return usedCount < depth;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::HasTensorInQue()
{
    return usedCount > 0;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) int32_t TQueBind<src, dst, depth, mask>::GetTensorCountInQue()
{
    return usedCount;
}
template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) bool TQueBind<src, dst, depth, mask>::HasIdleBuffer()
{
    return bufUsedCount < bufNum;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) void TQueBind<src, dst, depth, mask>::FreeAllEvent()
{
    auto ptr = this->bufStart;
    for (int i = 0; i < this->bufNum; i++, ptr++) {


                                                                                     ;
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlag<freeBufEvt>(ptr->freeBufEvtID);
            GetTPipePtr()->ReleaseEventID<freeBufEvt>(ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
}
template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBuffAddr TQueBind<src, dst, depth, mask>::GetBufferAddr(TBufHandle buf)
{
                                                                                                                       ;
    auto ptr = reinterpret_cast<TBufType*>(buf);



      ;

    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(GetPosition(src, dst));
    addr.bufferHandle = buf;
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;





    return addr;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) TBufState TQueBind<src, dst, depth, mask>::GetState(const LocalTensor<T>& tensor) const
{
    return GetState(tensor.GetBufferHandle());
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::Buf2Tensor(TBufHandle buf)
{
    TBuffAddr addr = GetBufferAddr(buf);
    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] __inline__ __attribute__((always_inline)) TBufState TQueBind<src, dst, depth, mask>::GetState(const TBufHandle& handle) const
{
    if (handle == nullptr) {
        return TBufState::FREE;
    }
    auto ptr = reinterpret_cast<TBufType*>(handle);



      ;
    return ptr->state;
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TBuf<pos>::Get(uint32_t len)
{
    uint32_t dataLen;
    if constexpr (IsSameType<T, int4b_t>::value) {
        dataLen = len / INT4_TWO;
    } else {
        dataLen = len * sizeof(T);
    }







    auto ptr = this->bufStart;
    ptr->dataLen = dataLen;
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = reinterpret_cast<TBufHandle>(ptr);
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;
# 580 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TBuf<pos>::Get()
{
    if constexpr (IsSameType<T, int4b_t>::value) {
        return Get<T>(bufLen * INT4_TWO);
    } else {
        return Get<T>(bufLen / sizeof(T));
    }
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TBuf<pos>::GetWithOffset(uint32_t size, uint32_t bufOffset)
{
    auto ptr = this->bufStart;
    ptr->dataLen = size * sizeof(T);
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = reinterpret_cast<TBufHandle>(ptr);
    addr.bufferAddr = ptr->address + bufOffset;
    addr.dataLen = ptr->dataLen;




    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::SetTpipeBuf(TBufType* bufStartIn, uint32_t bufLenIn)
{
    this->bufStart = bufStartIn;
    this->bufLen = bufLenIn;
    this->offset = 0;
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::EnQue(const LocalTensor<T>& tensor)
{
    (void)(0);
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) LocalTensor<T> TBuf<pos>::DeQue()
{
    return Get<T>();
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TBuf<pos>::AllocTensor()
{
    return Get<T>();
}

template <TPosition pos> template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::FreeTensor(LocalTensor<T>& tensor)
{
    (void)(0);
}

template <TPosition pos>
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) TBufState TBuf<pos>::GetState(const LocalTensor<T>& tensor) const
{
    TBufHandle handle = tensor.GetBufferHandle();
    if (handle == nullptr) {
        return TBufState::FREE;
    }
    auto ptr = reinterpret_cast<TBufType*>(handle);
    return ptr->state;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool TBuf<pos>::EnQue(TBufHandle buf)
{
    return true;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::DeQue()
{
    return Get();
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::AllocBuffer()
{
    return Get();
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) void TBuf<pos>::FreeBuffer(TBufHandle buf)
{
    (void)(0);
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBuffAddr TBuf<pos>::GetBufferAddr(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = buf;
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;




    return addr;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::Get(uint32_t len)
{



    this->bufStart->dataLen = len;
    return reinterpret_cast<TBufHandle>(this->bufStart);
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) TBufHandle TBuf<pos>::Get()
{
    return Get(bufLen);
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint32_t TBuf<pos>::GetBufLen() const
{
    return bufLen;
}

[aicore] __inline__ __attribute__((always_inline)) TPipe::TPipe()
{
    InitSocState();
    Init();
}

[aicore] __inline__ __attribute__((always_inline)) TPipe::~TPipe()
{
    if (g_tpipeImpl.isDestroy) {
        return;
    }
    Destroy();
};

[aicore] __inline__ __attribute__((always_inline)) void TPipe::Init()
{
    ResetPool();



    if constexpr(g_coreType == AscendC::AIC) {
        auto enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                  ;
        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));
        enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                  ;
        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));

        enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                  ;

        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));
    }
# 787 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    g_vecTPipePtr = this;






    g_tpipeImpl.isDestroy = false;
}

template <class T> [aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBuffer(T& que, uint8_t num, uint32_t len)
{
    static_assert((T::isTQue), "TPipe::InitBuffer(T& que, uint8_t num, uint32_t len) not supports T as TBuf");


                                         ;


                                      ;
                                                                                                                      ;
    if constexpr (T::dstPosition == TPosition::TSCM) {
        return TscmInitBuffer(que, num, len);
    }
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
                                      ;

    Hardware pool = GetBufferPos(T::srcPosition, T::dstPosition);
                                                                                                               ;
                                                                                                                 ;
    auto curPoolAddr = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    auto ptr = que.bufStart;
# 828 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }

                                                                                                            ;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;
    this->g_tpipeImpl.curBufSize_ += num;

                                                    ;



                                                                                           ;
    return true;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBuffer(TBuf<pos>& buf, uint32_t len)
{
                                                                                                                      ;

    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr int32_t bufHandleSize = 1;
    buf.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
    buf.bufLen = len;
    buf.offset = 0;

    constexpr auto pool = GetPhyType(pos);
                                                                                                               ;

    auto curPoolAddr = g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    auto ptr = buf.bufStart;







    for (uint8_t i = 0; i < bufHandleSize; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }


      ;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;
    this->g_tpipeImpl.curBufSize_ += bufHandleSize;


                                                    ;
    return true;
}

template <class T>
[aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBufPool(T &bufPool, uint32_t len)
{
    static_assert(
        (T::isTbufPool), "TPipe::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T as TbufPool");
                                                                                                                      ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    bufPool.g_tBufPoolImpl.startAddr_ = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    bufPool.g_tBufPoolImpl.maxAddr_ = bufPool.g_tBufPoolImpl.startAddr_;
    bufPool.g_tBufPoolImpl.maxLen_ = len;
    auto curPoolAddr = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
# 917 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    curPoolAddr += len;

                                                                                                              ;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;






          ;
    return true;
}

template <class T, class U>
[aicore] __inline__ __attribute__((always_inline)) bool TPipe::InitBufPool(T &bufPool, uint32_t len, U &shareBuf)
{
    static_assert((T::isTbufPool && U::isTbufPool),
        "TPipe::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T and U as TBufPool");
                                                                                                                      ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);

                                                                                                     ;

    bufPool.g_tBufPoolImpl.startAddr_ = shareBuf.g_tBufPoolImpl.startAddr_;
    bufPool.g_tBufPoolImpl.maxAddr_ = bufPool.g_tBufPoolImpl.startAddr_;
    bufPool.g_tBufPoolImpl.maxLen_ = shareBuf.g_tBufPoolImpl.maxLen_;




      ;
# 961 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    return true;
}

template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID TPipe::AllocEventID()
{

                                                                                                ;
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);



      ;
    ptr->eventOccupy = sbitset1(ptr->eventOccupy, lastId);
    return lastId;
}

template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) void TPipe::ReleaseEventID(TEventID id)
{



      ;
                                                                                                          ;
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    ptr->eventOccupy = sbitset0(ptr->eventOccupy, id);
    return;
}

[aicore] __inline__ __attribute__((always_inline)) TEventID TPipe::FetchEventID(HardEvent evt)
{
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);



      ;
    return lastId;
}

template <HardEvent evt> [aicore] __inline__ __attribute__((always_inline)) TEventID TPipe::FetchEventID()
{
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);



      ;
    return lastId;
}

template <TPosition pos>
[aicore] __inline__ __attribute__((always_inline)) TBuffAddr TPipe::GetAbsAddr(int32_t offset, int32_t len) const
{
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = nullptr;
    addr.bufferAddr = offset;
    addr.dataLen = len;
# 1030 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    return addr;
}

template <TPosition pos, typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((sync_noalias)) LocalTensor<T> TPipe::GetAbsAddr(int32_t offset, int32_t size) const
{
    TBuffAddr addr = GetAbsAddr<pos>(offset, static_cast<int32_t>((size * sizeof(T))));
    LocalTensor<T> tensor;
    tensor.SetAddr(addr);
    return tensor;
}

[aicore] __inline__ __attribute__((always_inline)) void InitShareBufStart(TPipe* tpipe, uint32_t mode, uint32_t* shareLens,
    uint32_t lens, uint8_t subBlockIdx)
{






    (void)(lens);



                                                                                                             ;
    tpipe->AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L1),
        Hardware::L1, subBlockIdx);
    tpipe->AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L0C),
        Hardware::L0C, subBlockIdx);




    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0A)].maxAddr = 0;
    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0B)].maxAddr = 0;

    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::BIAS)].maxAddr = 0;

    return;
}

[aicore] __inline__ __attribute__((always_inline)) void InitShareBufEnd(TPipe* tpipe)
{

    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L1)].maxAddr =
        tpipe->g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L1)];
    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0C)].maxAddr =
        tpipe->g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)];





    return;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::InitSpmBuffer(const GlobalTensor<T>& workspace, const int32_t bufferSize)
{
    g_tpipeImpl.spmInfo_.spmBuffSize = bufferSize;
    g_tpipeImpl.spmInfo_.spmAddr = reinterpret_cast<uint64_t>(workspace.GetPhyAddr());
    g_tpipeImpl.spmInfo_.spmBufType = static_cast<uint8_t>(Hardware::GM);
}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::InitSpmBuffer(const int32_t bufferSize)
{

    (void)(bufferSize);

                                                                                    ;
# 1113 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::WriteSpmBuffer(const LocalTensor<T>& writeLocal, const DataCopyParams& copyParams,
    int32_t writeOffset)
{




    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
    SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        DataCopyUB2GMImpl(reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), copyParams);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                        ;
        DataCopyUB2L1Impl(reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), copyParams);
        event_t eventIDMTE3ToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE1));
        SetFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
        WaitFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
    }
    event_t eventIDMTE3ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_V));
    SetFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
    WaitFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::ReadSpmBuffer(const LocalTensor<T>& readLocal, const DataCopyParams& copyParams,
    int32_t readOffset)
{




    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        event_t eventIDVToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
        event_t eventIDMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
        event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        WaitFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        DataCopyGM2UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, copyParams);
        SetFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);
        WaitFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);

        SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
        WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                       ;
        event_t eventIDVToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE1));
        event_t eventIDMTE1ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_V));
        event_t eventIDMTE1ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_MTE3));
        SetFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        WaitFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        DataCopyL12UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, copyParams);

        SetFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);
        WaitFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);

        SetFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
        WaitFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::WriteSpmBuffer(const LocalTensor<T>& writeLocal, const int32_t writeSize,
    int32_t writeOffset)
{




    int computeSize = writeSize != 0 ? writeSize : GetShapeSize(writeLocal.GetShapeInfo());
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = computeSize / AscendCUtils::GetC0Count(sizeof(T));
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
    event_t eventIDMTE3ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_V));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);

    SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        DataCopyUB2GMImpl(reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), repeatParams);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                        ;

                      ;
        DataCopyUB2L1Impl(reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(writeLocal.GetPhyAddr()), repeatParams);
        event_t eventIDMTE3ToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE1));
        SetFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
        WaitFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
    }

    SetFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
    WaitFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TPipe::ReadSpmBuffer(const LocalTensor<T>& readLocal, const int32_t readSize, int32_t readOffset)
{




    int computeSize = readSize != 0 ? readSize : GetShapeSize(readLocal.GetShapeInfo());
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = computeSize / AscendCUtils::GetC0Count(sizeof(T));
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        event_t eventIDVToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
        event_t eventIDMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
        event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        WaitFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        DataCopyGM2UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, repeatParams);

        SetFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);
        WaitFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);

        SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
        WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                       ;
                                                                                                                      ;
        event_t eventIDVToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE1));
        event_t eventIDMTE1ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_V));
        event_t eventIDMTE1ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_MTE3));
        SetFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        WaitFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        DataCopyL12UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(readLocal.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, repeatParams);

        SetFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);
        WaitFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);

        SetFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
        WaitFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
    }
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint64_t TPipe::GetQueueEndAddress()
{
    Hardware hardType = GetPhyType(pos);
                                                                                                      ;
    return this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(hardType)].maxAddr;
}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::Destroy()
{
    g_tpipeImpl.isDestroy = true;
    auto ptr = this->g_tpipeImpl.buf_;
    for (uint8_t i = 0; i < this->g_tpipeImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }


    if constexpr(g_coreType == AscendC::AIC) {
        WaitFlag<HardEvent::M_MTE1>(0);
        ReleaseEventID<HardEvent::M_MTE1>(0);
        WaitFlag<HardEvent::M_MTE1>(1);
        ReleaseEventID<HardEvent::M_MTE1>(1);

        WaitFlag<HardEvent::M_MTE1>(2);
        ReleaseEventID<HardEvent::M_MTE1>(2);
    }
# 1310 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    pipe_barrier(PIPE_ALL);



}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::Reset()
{
    auto ptr = this->g_tpipeImpl.buf_;
    for (uint8_t i = 0; i < this->g_tpipeImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    InitSocState();
    ResetPool();





}
# 1436 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void TPipe::InitSocState() const
{
    set_atomic_none();

    if constexpr(g_coreType == AscendC::AIC) {
        set_mask_norm();
        set_l1_3d_size(static_cast<uint64_t>(0));
        set_padding(static_cast<uint64_t>(0));
    } else {
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
        set_mask_norm();
    }



}

[aicore] __inline__ __attribute__((always_inline)) void TPipe::ResetPool()
{
    g_tpipeImpl.tscmBufferPtr_ = TOTAL_L1_SIZE;
    g_tpipeImpl.curBufSize_ = 0;
    auto buf = g_tpipeImpl.bufPool_;
    for (int32_t i = 0; i < static_cast<int32_t>(Hardware::MAX); i++, buf++) {
        buf->maxAddr = 0;
    }
    auto evt = g_tpipeImpl.eventPool_;
    for (int32_t i = 0; i < EVENT_NUM; i++, evt++) {
        evt->eventOccupy = 0;
    }
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::L1)] = -1;
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::UB)] = -1;
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)] = -1;
}

template <class T> [aicore] __inline__ __attribute__((always_inline)) bool TPipe::TscmInitBuffer(T& que, uint8_t num, uint32_t len)
{



      ;

    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
                                      ;

    constexpr Hardware pool = Hardware::L1;






    uint32_t curPoolAddr;
    if constexpr (T::scmBlockGroup) {
        curPoolAddr = g_tpipeImpl.tscmBufferPtr_ - num * len;
        g_tpipeImpl.tscmBufferPtr_ -= num * len;
    } else {
        curPoolAddr = g_tpipeImpl.tscmBufferPtr_ - (GetTaskRationImpl() - GetSubBlockIdxImpl()) * len * num;
        g_tpipeImpl.tscmBufferPtr_ -= GetTaskRationImpl() * num * len;
    }

    auto ptr = que.bufStart;
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }





          ;
    this->g_tpipeImpl.curBufSize_ += num;


                                                    ;
    return true;
}


template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) TBufPool<pos, bufIDSize>::TBufPool()
{
    Init();
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) TBufPool<pos, bufIDSize>::~TBufPool()
{
    auto ptr = this->g_tBufPoolImpl.buf_;
    for (uint8_t i = 0; i < this->g_tBufPoolImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    ResetPool();
};

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) void TBufPool<pos, bufIDSize>::ResetPool()
{
    g_tBufPoolImpl.curBufSize_ = 0;
    g_tBufPoolImpl.startAddr_ = 0;
    g_tBufPoolImpl.maxAddr_ = 0;
    g_tBufPoolImpl.maxLen_ = 0;
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) void TBufPool<pos, bufIDSize>::Init()
{
    constexpr auto pool = GetPhyType(pos);
    static_assert((pool == Hardware::L1 || pool == Hardware::UB),
        "TbufPool Position should be one of A1/B1/C1/VECIN/VECOUT/VECCALC");
    ResetPool();
    g_tBufPoolImpl.isReset_ = true;
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] __inline__ __attribute__((always_inline)) void TBufPool<pos, bufIDSize>::Reset()
{
    auto ptr = this->g_tBufPoolImpl.buf_;
    for (uint8_t i = 0; i < this->g_tBufPoolImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    ResetPool();
    g_tBufPoolImpl.isReset_ = true;



}

template <TPosition pos, uint32_t bufIDSize>
template <class T>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBuffer(T &que, uint8_t num, uint32_t len)
{
    static_assert((T::isTQue), "TBufPool::InitBuffer(T& que, uint8_t num, uint32_t len) not supports T as TBuf");
                                                                                                                      ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tBufPoolImpl.buf_ + this->g_tBufPoolImpl.curBufSize_;
                                      ;





          ;
    auto curPoolAddr = this->g_tBufPoolImpl.maxAddr_;
    auto ptr = que.bufStart;
# 1610 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }
    this->g_tBufPoolImpl.maxAddr_ = curPoolAddr;
    this->g_tBufPoolImpl.curBufSize_ += num;


      ;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <TPosition bufPos>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBuffer(TBuf<bufPos> &buf, uint32_t len)
{
                                                                                                                      ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr int32_t bufHandleSize = 1;
    buf.bufStart = this->g_tBufPoolImpl.buf_ + this->g_tBufPoolImpl.curBufSize_;
    buf.bufLen = len;
    buf.offset = 0;





          ;
    constexpr auto pool = GetPhyType(bufPos);

                                                                                        ;
    auto curPoolAddr = this->g_tBufPoolImpl.maxAddr_;
    auto ptr = buf.bufStart;
# 1657 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    for (uint8_t i = 0; i < bufHandleSize; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }

                                                                                                                  ;
    this->g_tBufPoolImpl.maxAddr_ = curPoolAddr;
    this->g_tBufPoolImpl.curBufSize_ += bufHandleSize;





      ;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <class T>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBufPool(T &bufPool, uint32_t len)
{
    static_assert(
        (T::isTbufPool), "TBufPool::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T as TbufPool");
                                                                                                                      ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    bufPool.g_tBufPoolImpl.startAddr_ = this->g_tBufPoolImpl.maxAddr_;
    bufPool.g_tBufPoolImpl.maxAddr_ = bufPool.g_tBufPoolImpl.startAddr_;
    bufPool.g_tBufPoolImpl.maxLen_ = len;





          ;
    auto curPoolAddr = this->g_tBufPoolImpl.maxAddr_;
# 1711 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    curPoolAddr += len;

                                                                                                              ;
    this->g_tBufPoolImpl.maxAddr_ = curPoolAddr;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <class T, class U>
[aicore] __inline__ __attribute__((always_inline)) bool TBufPool<pos, bufIDSize>::InitBufPool(T &bufPool, uint32_t len, U &shareBuf)
{
    static_assert((T::isTbufPool && U::isTbufPool),
        "TBufPool::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T and U as TBufPool");
                                                                                                                      ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    constexpr auto sharedPool = GetPhyType(U::poolPos);

                                                                                                            ;
    bufPool.g_tBufPoolImpl.startAddr_ = shareBuf.g_tBufPoolImpl.startAddr_;
    bufPool.g_tBufPoolImpl.maxAddr_ = bufPool.g_tBufPoolImpl.startAddr_;
    bufPool.g_tBufPoolImpl.maxLen_ = shareBuf.g_tBufPoolImpl.maxLen_;




      ;
# 1751 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_tpipe_impl.h"
    return true;
}

template <pipe_t src, pipe_t dst>
[aicore] __inline__ __attribute__((always_inline)) void TQueSync<src, dst>::SetFlag(TEventID id)
{
    static_assert((src != dst), "src/dst pipe cannot be same.");
    static_assert(IsSupportedPipe(src), "src pipe not supported");
    static_assert(IsSupportedPipe(dst), "dst pipe not supported");


      ;
    set_flag(src, dst, id);
}

template <pipe_t src, pipe_t dst>
[aicore] __inline__ __attribute__((always_inline)) void TQueSync<src, dst>::WaitFlag(TEventID id)
{
    static_assert((src != dst), "src/dst pipe cannot be same.");
    static_assert(IsSupportedPipe(src), "src pipe not supported");
    static_assert(IsSupportedPipe(dst), "dst pipe not supported");


      ;
    wait_flag(src, dst, id);
}

template <TPosition pos>
[aicore] __inline__ __attribute__((always_inline)) uint64_t TransUBAddr(uint64_t addr)
{




    return addr;
}

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_type.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_prof_trace.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_prof_trace.h"
namespace AscendC {
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_prof_trace.h"
[aicore] __inline__ __attribute__((always_inline)) void ProfStartImpl()
{


    bisheng::cce::metrics_prof_start();




}

[aicore] __inline__ __attribute__((always_inline)) void ProfStopImpl()
{


    bisheng::cce::metrics_prof_stop();




}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h" 2

namespace AscendC {
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_prof_trace_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStart();

[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStop();
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_data_copy_base_impl.h" 1
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_data_copy_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_scm_data_copy_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_scm_data_copy_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
namespace AscendC {



[[block_local]] __inline__ AscendC::KfcCommClient* g_kfcClient;




class KfcCommClient {
public:

    __attribute__((cce_global)) KfcMsg *msgSendHead;
    __attribute__((cce_global)) KfcMsg *msgSendStart;


    __attribute__((cce_global)) KfcMsg *msgRcvHead;
    __attribute__((cce_global)) KfcMsg *msgRcvStart;

    __attribute__((cce_global)) uint8_t* ubStart;
    __attribute__((cce_global)) uint8_t* ubAvalidTail;

    __attribute__((cce_unif_buff)) KfcMsg *ubMsg;
    uint32_t head;
    uint32_t tail;
    uint8_t msgRcvPos;
    uint8_t msgSendPos;
    uint8_t eventID_;

public:
    [aicore] __inline__ __attribute__((always_inline)) KfcCommClient(__attribute__((cce_global)) uint8_t* workspace, int subBlockID);
    [aicore] __inline__ __attribute__((always_inline)) ~KfcCommClient();
    template <bool isAck>
    [aicore] __inline__ __attribute__((always_inline)) void PostMessage(__attribute__((cce_global)) KfcMsg *msg)
    {
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventID);
        WaitFlag<HardEvent::S_MTE3>(eventID);
        PipeBarrier<PIPE_MTE3>();
        copy_ubuf_to_gm((__attribute__((cce_global)) void *)msg, (__attribute__((cce_unif_buff)) void *)ubMsg, 0, 1, sizeof(KfcMsg) / ONE_BLK_SIZE, 0, 0);
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
        SetFlag<HardEvent::MTE3_S>((event_t)this->eventID_);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *AllocMessage()
    {
        auto ret = AllocMessageImpl(this->msgSendHead, this->msgSendPos, this->msgSendStart);
        WaitFlag<HardEvent::MTE3_S>((event_t)this->eventID_);

                                                                                     ;
        return ret;
    }

    [aicore] __inline__ __attribute__((always_inline)) void FreeMessage(__attribute__((cce_global)) KfcMsg *msg)
    {
        FreeMessageImpl(msg);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* AllocUB(uint32_t size, int32_t &tailInfo);

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg *RcvMessage()
    {
        auto ret = RcvMessageImpl(this->msgRcvHead, this->msgRcvPos, this->msgRcvStart);
        return ret;
    }
};
[aicore] __inline__ __attribute__((always_inline)) KfcCommClient::KfcCommClient(__attribute__((cce_global)) uint8_t* workspace, int subBlockID)
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                             ;
                                                                                                                 ;

                                                                                ;

        this->msgSendStart = (__attribute__((cce_global)) KfcMsg *)GetMsgHead(workspace, subBlockID);
        this->msgRcvStart = this->msgSendStart + 64;

        this->msgSendHead = this->msgSendStart;
        this->msgSendPos = 0;
        this->msgRcvHead = this->msgRcvStart;
        this->msgRcvPos = 0;






        ubMsg = reinterpret_cast<__attribute__((cce_unif_buff)) KfcMsg *>(TOTAL_UB_SIZE - sizeof(KfcMsg));

        eventID_ = GetTPipePtr()->AllocEventID<HardEvent::MTE3_S>();
        SetFlag<HardEvent::MTE3_S>((event_t)eventID_);

        ubStart = GetUBMapAddr(workspace);
        ubAvalidTail = GetUBAvaliedAddr(workspace);
        head = 0;
        tail = 0;
    }
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* KfcCommClient::AllocUB(uint32_t size, int32_t &tailInfo)
{
# 146 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
    __attribute__((cce_global)) uint8_t* ret;
    if (head + size >= WORKSPACE_UB_SIZE) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail),
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
        tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
        while (head < tail || tail == 0) {
            Barrier();
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
            Barrier();
            tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
        }
        if (tail == head && size == tail) {
            tail = 0;
        }
        head = 0;
    }

    while (head < tail && (head + size >= tail)) {
        Barrier();
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail),
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
        Barrier();
        tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
    }





    ret = ubStart + head;
    head += size;
    tailInfo = head;
    return ret;
}

[aicore] __inline__ __attribute__((always_inline)) KfcCommClient::~KfcCommClient()
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_global)) KfcMsg *msg = AllocMessage();

                                                                                                      ;
        uint32_t quitSignal = KfcMsgMakeFlag(KFC_Enum::SERVICE_QUIT, 0);
        *((__attribute__((cce_global)) uint32_t*)msg) = quitSignal;
# 203 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_client.h"
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    }
}

[aicore] __inline__ __attribute__((always_inline)) AscendC::KfcCommClient* GetKfcClient()
{


    return reinterpret_cast<AscendC::KfcCommClient*>(g_kfcClient);







}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_scm_data_copy_impl.h" 2

namespace AscendC {
struct Gm2L1Params {
    __attribute__((cce_cube_buff)) void* dst = nullptr;
    __attribute__((cce_global)) void* src = nullptr;
    DataCopyParams intri;
};
struct Gm2L1Nd2NzParams {
    __attribute__((cce_cube_buff)) void* dst = nullptr;
    __attribute__((cce_global)) void* src = nullptr;
    uint8_t dataTypeLen = 2;
    Nd2NzParams intri;
};




[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopyMsg(__attribute__((cce_cube_buff)) void* dst, __attribute__((cce_global)) void* src, const DataCopyParams& intriParams,
    int32_t ubAddr)
{
                             ;
                                     ;
    auto msg = GetKfcClient()->AllocMessage();
                                                             ;

    __attribute__((cce_unif_buff)) struct Gm2L1Params* p = (__attribute__((cce_unif_buff)) struct Gm2L1Params*)&(GetKfcClient()->ubMsg->buffer);
    p->dst = dst;
    p->src = src;
    p->intri.blockCount = intriParams.blockCount;
    p->intri.blockLen = intriParams.blockLen;
    p->intri.srcStride = intriParams.srcStride;
    p->intri.dstStride = intriParams.dstStride;
    GetKfcClient()->ubMsg->ubAddr = ubAddr;
    GetKfcClient()->ubMsg->head = KfcMsgMakeFlag(KFC_Enum::SCMFUN_GM2L1, 0);
    GetKfcClient()->PostMessage<false>(msg);
}

[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopyND2NZMsg(__attribute__((cce_cube_buff)) void* dst, __attribute__((cce_global)) void* src, const uint8_t dataTypeSize,
    const Nd2NzParams& intriParams, int32_t ubAddr)
{
                             ;
                          ;
                          ;
                                     ;
    auto msg = GetKfcClient()->AllocMessage();
                                                                  ;

    auto p = (__attribute__((cce_unif_buff)) struct Gm2L1Nd2NzParams*)&(GetKfcClient()->ubMsg->buffer);
    p->dst = dst;
    p->src = src;
    p->dataTypeLen = dataTypeSize;
    p->intri.ndNum = intriParams.ndNum;
    p->intri.nValue = intriParams.nValue;
    p->intri.dValue = intriParams.dValue;
    p->intri.srcNdMatrixStride = intriParams.srcNdMatrixStride;
    p->intri.dstNzC0Stride = intriParams.dstNzC0Stride;
    p->intri.dstNzNStride = intriParams.dstNzNStride;
    p->intri.dstNzMatrixStride = intriParams.dstNzMatrixStride;
    p->intri.srcDValue = intriParams.srcDValue;
    GetKfcClient()->ubMsg->ubAddr = ubAddr;
    GetKfcClient()->ubMsg->head = KfcMsgMakeFlag(KFC_Enum::SCMFUN_GM2L1ND2NZ, 0);
    GetKfcClient()->PostMessage<false>(msg);
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h" 2


namespace AscendC {


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckDataCopyPadParams(uint16_t blockCount, uint32_t blockLen, bool isGMtoUB)
{
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
}


[aicore] __inline__ __attribute__((always_inline)) void CheckDataCopyParams(uint16_t blockCount, uint16_t blockLen)
{
                                                                                  ;
                                                                              ;
}

[aicore] __inline__ __attribute__((always_inline)) void ValidateUbL1Address(uint64_t absUbAddr, uint64_t absL1Addr, uint32_t tensorSize)
{


      ;



      ;


      ;



      ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, false, intriParams);
        }
        copy_gm_to_ubuf((__attribute__((cce_unif_buff)) void*)dst, (__attribute__((cce_global)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
                                                                                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, false, intriParams);
        }
        copy_gm_to_cbuf((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)src, (int8_t)0, (uint16_t)intriParams.blockCount,
            (uint16_t)intriParams.blockLen, (uint16_t)intriParams.srcStride, (uint16_t)intriParams.dstStride, (pad_t)0);
    } else if constexpr(g_coreType == AscendC::AIV) {
# 110 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        ScmDataCopyMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)src, intriParams, -1);


    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                     ;
                                                                                                     ;
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                   ;

        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, false, intriParams);
        }
        copy_ubuf_to_gm((__attribute__((cce_global)) void*)dst, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                   ;

                                                   ;
        copy_ubuf_to_ubuf((__attribute__((cce_unif_buff)) void*)dst, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;

                                                   ;

        uint32_t tensorSize = intriParams.blockCount * intriParams.blockLen * 32;
        int32_t ubAddr = -1;







        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);

        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_ubuf_to_gm((__attribute__((cce_global)) void*)gmAddr, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.srcStride);
# 190 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        ScmDataCopyMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, intriParams, ubAddr);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L1ND2NZImpl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const Nd2NzParams& intriParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
                                         ;

                                                   ;
        uint32_t tensorSize = intriParams.nValue * intriParams.dValue;
        int32_t ubAddr = -1;
# 219 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);

        copy_ubuf_to_gm((__attribute__((cce_global)) void*)gmAddr, (__attribute__((cce_unif_buff)) void*)src, 0, 1, tensorSize * sizeof(T) / 32, 0, 0);





        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), intriParams, ubAddr);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
                                                                                ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12BTImpl(const uint64_t dst, __attribute__((cce_cube_buff)) T* src, const uint16_t isenableConv,
    const DataCopyParams &intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_cbuf_to_bt(dst, (__attribute__((cce_cube_buff)) void*)src, isenableConv, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12FBImpl(__attribute__((cce_fixpipe_buff)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams &intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_cbuf_to_fbuf((__attribute__((cce_fixpipe_buff)) void*)dst, (__attribute__((cce_cube_buff)) void*)src, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2L1ND2NZImplBase(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNd2Nz(src, workSpace, true, intriParams);
        }
        if constexpr (sizeof(T) == B8_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b8((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b16((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b32s((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        }
    } else if constexpr(g_coreType == AscendC::AIV) {





        ScmDataCopyND2NZMsg(dst, src, sizeof(T), intriParams, -1);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2L1ND2NZImpl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{

                                                       ;
    if constexpr (SupportType<T, int4b_t>()) {
        DataCopyGM2L1ND2NZImplBase((__attribute__((cce_cube_buff)) int8_t *)dst, (__attribute__((cce_global)) int8_t *)src, intriParams);
    } else if (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE || sizeof(T) == B32_BYTE_SIZE){
        DataCopyGM2L1ND2NZImplBase(dst, src, intriParams);
    } else {


                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
                                                                                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, false, intriParams);
        }
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)dst, (__attribute__((cce_cube_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CopyIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const CopyRepeatParams& repeatParams)
{
                                                                                                 ;
                                                                                                 ;
    if constexpr(sizeof(T) == B16_BYTE_SIZE) {
        vcopy((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src, repeatTimes, repeatParams.dstStride,
            repeatParams.srcStride, repeatParams.dstRepeatSize, repeatParams.srcRepeatSize);
    } else if constexpr(sizeof(T) == B32_BYTE_SIZE) {
        vcopy((__attribute__((cce_unif_buff)) uint32_t*)dst, (__attribute__((cce_unif_buff)) uint32_t*)src, repeatTimes, repeatParams.dstStride,
            repeatParams.srcStride, repeatParams.dstRepeatSize, repeatParams.srcRepeatSize);
    } else {

                                                                                                                      ;
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CopyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask[], uint8_t repeatTimes,
    const CopyRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        CopyIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CopyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const CopyRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        CopyIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}



template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12L0CImpl(__attribute__((cce_cube_c)) DstT* dst, __attribute__((cce_cube_buff)) SrcT* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        static_assert((SupportType<Tuple<SrcT, DstT>, Tuple<half, half>, Tuple<float, half>, Tuple<float, bfloat16_t>,
            Tuple<float, float>, Tuple<bfloat16_t, bfloat16_t>, Tuple<int32_t, int32_t>, Tuple<uint32_t, uint32_t>,
            Tuple<uint32_t, uint32_t>>()), "Failed to check dtype in DataCopy from A1 / B1 to CO1, current api support "
            "dtype combination is src: half, dst: half; src: float, dst: half / bfloat16_t / float; src: bfloat16_t, "
            "dst: bfloat16_t; src: int32_t, dst: int32_t; src: uint32_t, dst: uint32_t.");
                                                                                                                    ;
                                                                                                             ;
        copy_matrix_cbuf_to_cc((__attribute__((cce_cube_c)) DstT*)dst, (__attribute__((cce_cube_buff)) SrcT*)src, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}




template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
                                                                 ;
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L0CImpl(__attribute__((cce_cube_c)) T* dst, __attribute__((cce_unif_buff)) U* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
                                                                 ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopySliceGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParamsIn)
{

                                               ;
    DataCopyPadExtParams<T> padParams{ false, 0, 0, 0 };
    uint16_t burstLen = intriParamsIn.blockLen * ONE_BLK_SIZE;
    DataCopyExtParams intriParams{ intriParamsIn.blockCount, burstLen, intriParamsIn.srcStride, intriParamsIn.dstStride,
        0 };
    DataCopyPadGm2UBImpl(dst, src, intriParams, padParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams,
    const DataCopyPadParams& padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), true);
    if (padParams.isPad) {
        set_mov_pad_val(padParams.paddingValue);
    }
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B64_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen,
            (padParams.leftPadding << 1), (padParams.rightPadding << 1), intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyExtParams& intriParams,
    const DataCopyPadExtParams<T>& padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, true);
    if (padParams.isPad) {
        set_mov_pad_val(GetScalarBitcodeValue(static_cast<T>(padParams.paddingValue)));
    }
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B64_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen,
            (padParams.leftPadding << 1), (padParams.rightPadding << 1), intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopySliceUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParamsIn)
{

                                               ;
    uint32_t burstLen = intriParamsIn.blockLen * ONE_BLK_SIZE;
    DataCopyExtParams intriParams{ intriParamsIn.blockCount, burstLen, intriParamsIn.srcStride, intriParamsIn.dstStride,
        0 };
    DataCopyPadUB2GMImpl(dst, src, intriParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), false);
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyExtParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                      ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, false);
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            (uint32_t)intriParams.srcStride, (uint32_t)intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, (uint8_t)0, (uint8_t)0,
            intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams,
    const Nd2NzParams& nd2nzParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), false);
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;


                         ;
        uint32_t tensorSize = nd2nzParams.nValue * nd2nzParams.dValue;
        int32_t ubAddr = -1;
# 591 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);


                                                                                            ;
                                                                                                                       ;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)src, intriParams);




        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), nd2nzParams, ubAddr);

    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyExtParams& intriParams,
    const Nd2NzParams& nd2nzParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, false);
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;


                         ;
        uint32_t tensorSize = nd2nzParams.nValue * nd2nzParams.dValue;
        int32_t ubAddr = -1;
# 638 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);


                                                                                            ;
                                                                                                                       ;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)src, intriParams);




        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), nd2nzParams, ubAddr);

    }
}



[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopy(__attribute__((cce_global)) void* kfcMsgPtr)
{
                                                                                               ;
    auto scmCopyParams = reinterpret_cast<__attribute__((cce_global)) struct Gm2L1Params*>(kfcMsgPtr);



      ;
    auto dst = reinterpret_cast<__attribute__((cce_cube_buff)) void*>(scmCopyParams->dst);
    auto& intriParams = scmCopyParams->intri;







    copy_gm_to_cbuf((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)scmCopyParams->src, (int8_t)0, (uint16_t)intriParams.blockCount,
        (uint16_t)intriParams.blockLen, (uint16_t)intriParams.srcStride, (uint16_t)intriParams.dstStride, (pad_t)0);

    event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE1));
    SetFlag<HardEvent::MTE2_MTE1>(eventID);
    WaitFlag<HardEvent::MTE2_MTE1>(eventID);
}

[aicore] __inline__ __attribute__((always_inline)) void ScmDataCopyND2NZ(__attribute__((cce_global)) void* kfcMsgPtr)
{
                                                                                               ;
    auto scmCopyParams = reinterpret_cast<__attribute__((cce_global)) struct Gm2L1Nd2NzParams*>(kfcMsgPtr);
    auto& intriParams = scmCopyParams->intri;
    auto l1AddrDst = reinterpret_cast<__attribute__((cce_cube_buff)) void*>(scmCopyParams->dst);
# 700 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_data_copy_impl.h"
    if (scmCopyParams->dataTypeLen == 2) {
        copy_gm_to_cbuf_multi_nd2nz_b16((__attribute__((cce_cube_buff)) half*)l1AddrDst, (__attribute__((cce_global)) half*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    } else if (scmCopyParams->dataTypeLen == 4) {
        copy_gm_to_cbuf_multi_nd2nz_b32s((__attribute__((cce_cube_buff)) float*)l1AddrDst, (__attribute__((cce_global)) float*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    } else {



          ;
        copy_gm_to_cbuf_multi_nd2nz_b8((__attribute__((cce_cube_buff)) int8_t*)l1AddrDst, (__attribute__((cce_global)) int8_t*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    }
    event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE1));
    SetFlag<HardEvent::MTE2_MTE1>(eventID);
    WaitFlag<HardEvent::MTE2_MTE1>(eventID);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBSingleImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams,
    const int copyTime, const int computeNum)
{

                                               ;
    const uint16_t &nValue = intriParams.nValue;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& computeLen = computeNum * sizeof(T);
    const uint16_t& c0Count = DEFAULT_C0_SIZE / sizeof(T);
    const uint16_t& maxC0Count = MAX_REPEAT_TIMES * c0Count;
    const uint16_t& maxdValue = MAX_REPEAT_TIMES * dValue;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& repeatCount = nValue / MAX_REPEAT_TIMES;
    const uint16_t& repeatTail = nValue % MAX_REPEAT_TIMES;
    const uint16_t& srcCopyStartOffset = copyTime * c0Count;
    const uint16_t& dstCopyStartOffset = copyTime * dstNzC0Stride * (DEFAULT_C0_SIZE / sizeof(T));
    DataCopyExtParams copyParams = { MAX_REPEAT_TIMES, (uint32_t)computeLen,
        (uint32_t)(intriParams.srcDValue * sizeof(T) - computeLen),
        (uint32_t)((dstNzNStride - (uint16_t)DEFAULT_C0_SIZE) / (uint16_t)DEFAULT_C0_SIZE), 0 };
    DataCopyPadExtParams<T> padParams;
    for (int repeatTime = 0; repeatTime < repeatCount; ++repeatTime) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)(dst + dstCopyStartOffset + repeatTime * maxC0Count),
            (__attribute__((cce_global)) T*)(src + srcCopyStartOffset + repeatTime * maxdValue), copyParams, padParams);
    }
    copyParams.blockCount = repeatTail;
    if (repeatTail != 0) {
        int dstOffset = (dstCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * c0Count);
        int srcOffset = (srcCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * dValue);
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)(dst + dstOffset), (__attribute__((cce_global)) T*)(src + srcOffset), copyParams, padParams);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBND2NZImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType != AscendC::AIV) {
        return;
    }

                                               ;
    const uint16_t &ndNum = intriParams.ndNum;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t& srcDValue = intriParams.srcDValue;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzMatrixStride = intriParams.dstNzMatrixStride;
    const uint16_t& c0Count = DEFAULT_C0_SIZE / sizeof(T);
    for (int index = 0; index < ndNum; ++index) {
        int16_t copyNum = (dValue + c0Count - 1) / c0Count;
        for (int copyTime = 0; copyTime < copyNum; ++copyTime) {
            int computeCount = (dValue >= (copyTime + 1) * c0Count) ? c0Count : (dValue % c0Count);
            DataCopyGM2UBSingleImpl(dst + dstNzMatrixStride, src + srcNdMatrixStride, intriParams, copyTime,
                computeCount);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2GMNZ2NDImplBase(__attribute__((cce_global)) T* dstAddr, __attribute__((cce_unif_buff)) T* srcAddr, uint16_t height,
    uint16_t width, uint16_t srcNStride, uint16_t dstDStride)
{

                                               ;
    const uint16_t BLK_CNT_LIMIT = UINT12_MAX;
    const uint16_t repeatTimes = height / BLK_CNT_LIMIT;
    const uint16_t tailBlock = height % BLK_CNT_LIMIT;
    const uint16_t widthBlkNum = (width + BLOCK_CUBE - 1) / BLOCK_CUBE;

    for (uint16_t i = 0; i < widthBlkNum; ++i) {
        uint16_t num = (i != widthBlkNum -1) ? BLOCK_CUBE : (width - i * BLOCK_CUBE);
        uint32_t blockLen = static_cast<uint32_t>(num * sizeof(T));
        uint32_t dstStride = static_cast<uint32_t>((dstDStride - num) * sizeof(T));
        for (uint16_t j = 0; j < repeatTimes; ++j) {
            DataCopyPadUB2GMImpl(dstAddr + i * BLOCK_CUBE + j * BLK_CNT_LIMIT * dstDStride,
                srcAddr + i * srcNStride * BLOCK_CUBE + j * BLK_CNT_LIMIT * BLOCK_CUBE,
                {BLK_CNT_LIMIT, blockLen, 0, dstStride, 0});
        }
        if (tailBlock) {
            DataCopyPadUB2GMImpl(dstAddr + i * BLOCK_CUBE + repeatTimes * BLK_CNT_LIMIT * dstDStride,
                srcAddr + i * srcNStride * BLOCK_CUBE + repeatTimes * BLK_CNT_LIMIT * BLOCK_CUBE,
                {tailBlock, blockLen, 0, dstStride, 0});
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2GMNZ2NDImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const Nz2NdParamsFull& intriParams)
{

                                               ;

                                                                                       ;
    const uint16_t ndNum = intriParams.ndNum;
    const uint16_t nValue = intriParams.nValue;
    const uint16_t dValue = intriParams.dValue;
    const uint16_t srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t srcNStride = intriParams.srcNStride;
    const uint16_t dstDStride = intriParams.dstDStride;
    const uint16_t dstNdMatrixStride = intriParams.dstNdMatrixStride;

    if (ndNum != 1 && nValue != 0) {

                                                                                                                      ;
    }
                                                                                                                ;
    for (uint16_t i = 0; i < ndNum; ++i) {
        DataCopyUB2GMNZ2NDImplBase(dst + i * dstNdMatrixStride, src + i * srcNdMatrixStride * BLOCK_CUBE * BLOCK_CUBE,
            nValue, dValue, srcNStride, dstDStride);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBSingleImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_global)) float* src, const Nd2NzParams& intriParams,
    const int copyTime, const int computeNum)
{

                                               ;
    const uint16_t &nValue = intriParams.nValue;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& computeLen = computeNum * sizeof(float);
    const uint16_t& c0Count = BLOCK_CUBE;
    const uint16_t& maxC0Count = MAX_REPEAT_TIMES * c0Count;
    const uint16_t& maxdValue = MAX_REPEAT_TIMES * dValue;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& repeatCount = nValue / MAX_REPEAT_TIMES;
    const uint16_t& repeatTail = nValue % MAX_REPEAT_TIMES;
    const uint16_t& srcCopyStartOffset = copyTime * c0Count;
    const uint16_t& dstCopyStartOffset = copyTime * dstNzC0Stride * (DEFAULT_C0_SIZE / sizeof(float));
    DataCopyExtParams copyParams = { MAX_REPEAT_TIMES, (uint32_t)computeLen,
        (uint32_t)(intriParams.srcDValue * sizeof(float) - computeLen),
        (uint32_t)((dstNzNStride * DEFAULT_C0_SIZE - (uint16_t)c0Count * sizeof(float)) / (uint16_t)DEFAULT_C0_SIZE),
        0 };
    DataCopyPadExtParams<float> padParams;
    if (computeNum < c0Count) {
        copyParams.dstStride = (c0Count - computeNum) * sizeof(float) / DEFAULT_C0_SIZE;
        padParams.paddingValue = 0;
    }

    for (int repeatTime = 0; repeatTime < repeatCount; ++repeatTime) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) float*)(dst + dstCopyStartOffset + repeatTime * maxC0Count),
            (__attribute__((cce_global)) float*)(src + srcCopyStartOffset + repeatTime * maxdValue), copyParams, padParams);
    }
    copyParams.blockCount = repeatTail;
    if (repeatTail != 0) {
        int dstOffset = (dstCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * c0Count);
        int srcOffset = (srcCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * dValue);
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) float*)(dst + dstOffset), (__attribute__((cce_global)) float*)(src + srcOffset), copyParams,
            padParams);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyGM2UBND2NZImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_global)) float* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType != AscendC::AIV) {
        return;
    }

                                               ;
    const uint16_t &ndNum = intriParams.ndNum;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t& srcDValue = intriParams.srcDValue;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzMatrixStride = intriParams.dstNzMatrixStride;
    const uint16_t& c0Count = BLOCK_CUBE;
    for (int index = 0; index < ndNum; ++index) {
        int16_t copyNum = (dValue + c0Count - 1) / c0Count;
        for (int copyTime = 0; copyTime < copyNum; ++copyTime) {
            int computeCount = (dValue >= (copyTime + 1) * c0Count) ? c0Count : (dValue % c0Count);
            DataCopyGM2UBSingleImpl(dst + dstNzMatrixStride, src + srcNdMatrixStride, intriParams, copyTime,
                computeCount);
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyCO12DstParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322F16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322BF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::DEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VDEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::QF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VQF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::REQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VREQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            default:


                                                      ;
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyCO12DstParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {





                          ;
        switch (intriParams.quantPre) {
            case QuantMode_t::NoQuant:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::NoQuant,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322F16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322BF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::DEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VDEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::QF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VQF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::REQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VREQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            default:


                                                      ;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L1Intf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams)
{
    DataCopyUB2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2L0CIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyUB2L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams, enhancedParams);
}

#pragma begin_pipe(V)
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyUB2UBIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams)
{
    DataCopyUB2UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams);
}
#pragma end_pipe

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL12UBIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams)
{
    DataCopyL12UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __attribute__((inout_pipe("MTE1"))) DataCopyL12L0CIntf(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyL12L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams, enhancedParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyL0C2UBIntf(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<T>*)srcLocal.GetPhyAddr(),
        intriParams, enhancedParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE1"))) void DataCopyL12BTIntf(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &repeatParams)
{
    DataCopyL12BTImpl((uint64_t)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(), (uint16_t)0,
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("FIX"))) void DataCopyL12FBIntf(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &repeatParams)
{
    DataCopyL12FBImpl((__attribute__((cce_fixpipe_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)srcLocal.GetPhyAddr(),
        repeatParams);
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_data_copy_base_impl.h" 2







namespace AscendC {

enum class ReduceType : uint8_t {
    NO_REDUCE,
    REDUCE_ADD,
    REDUCE_MIN,
    REDUCE_MAX,
};

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyWithReduce(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const uint32_t calCount)
{
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(T));
    DataCopyWithReduce<T, reduceType>(dstGlobal, srcLocal, repeatParams);
}

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyWithReduce(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyParams& repeatParams)
{
    AscendC::SetAtomicNoneImpl();
    if constexpr (reduceType == ReduceType::REDUCE_ADD) {
        AscendC::SetAtomicAddImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MIN) {
        AscendC::SetAtomicMinImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MAX) {
        AscendC::SetAtomicMaxImpl<T>();
    }
    DataCopy(dstGlobal, srcLocal, repeatParams);
    AscendC::SetAtomicNoneImpl();
}

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPadWithReduce(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyExtParams& dataCopyExtParams)
{
    AscendC::SetAtomicNoneImpl();
    if constexpr (reduceType == ReduceType::REDUCE_ADD) {
        AscendC::SetAtomicAddImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MIN) {
        AscendC::SetAtomicMinImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MAX) {
        AscendC::SetAtomicMaxImpl<T>();
    }
    DataCopyPad(dstGlobal, srcLocal, dataCopyExtParams);
    AscendC::SetAtomicNoneImpl();
}


[aicore] __inline__ __attribute__((always_inline)) void DataCopyGetOffsetList(
    const SliceInfo sliceInfo[], uint32_t shapeInfo[], const uint32_t dimValue, uint32_t *count, uint32_t *offsetList)
{
    uint32_t sliceSize = 1;
    uint32_t copyCount = 1;
    uint32_t currentCount = 1;
    uint32_t preCopyCount = 0;
    uint32_t iter = 0;
    uint32_t totalSliceCount = 0;

    for (uint32_t i = 0; i < dimValue; i++) {
        if (i == 0) {
            *(offsetList + totalSliceCount) = 0;
            totalSliceCount++;
            continue;
        }
        iter = 0;
        sliceSize = sliceSize * shapeInfo[i - 1];
        currentCount =
            (sliceInfo[i].endIndex - sliceInfo[i].startIndex + 1 + sliceInfo[i].stride) / (1 + sliceInfo[i].stride);
        preCopyCount = copyCount;
        copyCount = copyCount * currentCount;
        for (uint32_t j = preCopyCount; j < copyCount; j += preCopyCount) {
            iter++;
            for (uint32_t k = 0; k < preCopyCount; k++) {
                *(offsetList + totalSliceCount) =
                    (*(offsetList + k)) + (iter * (1 + sliceInfo[i].stride)) * sliceSize;
                totalSliceCount++;
            }
        }
    }
    *count = totalSliceCount;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t DataCopyGetPhyStartIndex(
    const SliceInfo sliceInfo[], uint32_t shapeInfo[], const uint32_t dimValue)
{
    uint32_t phyStartIndex = 0;
    uint32_t sliceSize = 1;
    for (uint32_t i = 0; i < dimValue; i++) {
        if (i == 0) {
            phyStartIndex = phyStartIndex + sliceInfo[i].startIndex;
        } else {
            sliceSize = sliceSize * shapeInfo[i - 1];
            phyStartIndex = phyStartIndex + sliceSize * sliceInfo[i].startIndex;
        }
    }
    return phyStartIndex;
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h" 2

namespace AscendC {
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __attribute__((inout_pipe("MTE2")))
    DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal, const DataCopyParams& repeatParams);
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const Nd2NzParams& intriParams);
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcGlobal,
                                     const Nd2NzParams& intriParams);
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const DataCopyParams& repeatParams);
# 110 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                const DataCopyParams& repeatParams);
# 124 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src_T>& srcLocal,
                                const DataCopyParams& repeatParams);
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                              const uint64_t mask[], const uint8_t repeatTimes,
                                              const CopyRepeatParams& repeatParams);


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                              const uint64_t mask, const uint8_t repeatTimes,
                                              const CopyRepeatParams& repeatParams);
# 161 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[],
                                                     const uint32_t dimValue = 1);
# 175 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[],
                                                     const uint32_t dimValue = 1);
# 187 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const uint32_t calCount);
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const uint32_t calCount);
# 209 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                const uint32_t calCount);







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const Nz2NdParamsFull& intriParams);
# 241 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
                                                     const DataCopyParams& intriParams,
                                                     const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
                                                     const DataCopyParams& intriParams,
                                                     const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                const DataCopyCO12DstParams& intriParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<U>& srcLocal,
                                const DataCopyCO12DstParams& intriParams);



template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, bfloat16_t>::value && IsSameType<PrimT<U>, float>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);



template <
    typename T, typename U,
    typename std::enable_if<IsSameType<PrimT<T>, half>::value && IsSameType<PrimT<U>, float>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, half>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, int16_t>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, uint8_t>::value && IsSameType<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <
    typename T, typename U,
    typename std::enable_if<IsSameType<PrimT<T>, float>::value && IsSameType<PrimT<U>, half>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dstLocal,
                                                        const GlobalTensor<T>& srcGlobal,
                                                        const DataCopyParams& dataCopyParams,
                                                        const DataCopyPadParams& padParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T>& dstGlobal,
                                                        const LocalTensor<T>& srcLocal,
                                                        const DataCopyParams& dataCopyParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                        const DataCopyParams& dataCopyParams, const Nd2NzParams& nd2nzParams);


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dstLocal,
                                                        const GlobalTensor<T>& srcGlobal,
                                                        const DataCopyExtParams& dataCopyParams,
                                                        const DataCopyPadExtParams<T>& padParams);



template <typename T, typename U,
          typename std::enable_if<IsSameType<PrimT<T>, U>::value && (!IsSameType<T, U>::value), bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dstLocal,
                                                        const GlobalTensor<T>& srcGlobal,
                                                        const DataCopyExtParams& dataCopyParams,
                                                        const DataCopyPadExtParams<U>& padParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T>& dstGlobal,
                                                        const LocalTensor<T>& srcLocal,
                                                        const DataCopyExtParams& dataCopyParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
                                        const DataCopyExtParams& dataCopyParams, const Nd2NzParams& nd2nzParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetPadValue(T paddingValue);
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_pop_stack_buffer.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_pop_stack_buffer.h"
namespace AscendC {
template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) uint64_t GetEndAddress()
{
    Hardware hardType = GetPhyType(pos);
                                                                                                      ;


    return TOTAL_UB_SIZE - sizeof(KfcMsg);



}

template <typename T, TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(LocalTensor<T>& popLocal)
{
    TBuffAddr addr;
    addr.logicPos = (int8_t)pos;
                                                                                                       ;
    uint64_t endAddress = GetEndAddress<pos>();
    uint64_t queEndAddress = GetTPipePtr()->GetQueueEndAddress<pos>();

                                                                                                                   ;
    addr.dataLen = (uint32_t)(endAddress - queEndAddress);
    addr.bufferAddr = queEndAddress;
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_pop_stack_buffer.h"
    popLocal.SetAddr(addr);
    return true;
}

template <TPosition pos> [aicore] __inline__ __attribute__((always_inline)) bool PopStackBuffer(TBuf<pos>& popBuffer, TBufType& bufStart)
{
    uint64_t endAddress = GetEndAddress<pos>();
    uint64_t queEndAddress = GetTPipePtr()->GetQueueEndAddress<pos>();

                                                                                                                   ;
    uint32_t dataLen = (uint32_t)(endAddress - queEndAddress);
    bufStart.address = queEndAddress;
    bufStart.dataLen = dataLen;
    popBuffer.SetTpipeBuf(&bufStart, dataLen);







    return true;
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h" 2





# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_spr_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_set_spr_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SetQuantPreImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_quant_pre(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetNdParaImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_nd_para(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetFpcImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_fpc(config);
    }
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_check.h" 1
# 1913 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_check.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorAlign(const LocalTensor<T>& tensor, uint32_t alignByte, __attribute__((cce_global)) const char* tensorName,
    __attribute__((cce_global)) const char* apiMsg)
{






}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorPos(const LocalTensor<T>& tensor, const Hardware expectPos,
    __attribute__((cce_global)) const char* tensorName, __attribute__((cce_global)) const char* tposName, __attribute__((cce_global)) const char* apiMsg)
{




}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_impl.h" 2
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfigImpl(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag = false)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckTensorPos<T>(reluPre, Hardware::FIXBUF, "reluPre", "C2PIPE2GM", "SetFixPipeConfig");
        CheckTensorPos<T>(quantPre, Hardware::FIXBUF, "quantPre", "C2PIPE2GM", "SetFixPipeConfig");
        uint64_t config = 0;
        config = config | ((uint64_t)reluPre.GetPhyAddr() >> 6);
        config = config | (((uint64_t)quantPre.GetPhyAddr() >> 7) << 8);
        config = config | ((uint64_t)isUnitFlag << 63);
        set_fpc(config);
    }
}

template <typename T, bool setRelu = false>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfigImpl(const LocalTensor<T> &preTensor, bool isUnitFlag = false)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckTensorPos<T>(preTensor, Hardware::FIXBUF, "preTensor", "C2PIPE2GM", "SetFixPipeConfig");
        uint64_t config = 0;
        if constexpr (setRelu) {
            config = config | ((uint64_t)preTensor.GetPhyAddr() >> 6);
        } else {
            config = config | (((uint64_t)preTensor.GetPhyAddr() >> 7) << 8);
        }
        config = config | ((uint64_t)isUnitFlag << 63);
        set_fpc(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipeNz2ndFlagImpl(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride)
{
    if constexpr(g_coreType == AscendC::AIC) {
                                                                                                  ;
        uint64_t config = 0;
        config = config | ((uint64_t)ndNum);
        config = config | ((uint64_t)srcNdStride << 16);
        config = config | ((uint64_t)dstNdStride << 32);
        set_nd_para(config);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipePreQuantFlagImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_quant_pre(config);
    }
}






struct FixpipeTiling {
    uint16_t nIterNum = 0;
    uint16_t nSize = 0;
    bool isDb = false;
    uint16_t tailNSize = 0;
};


[aicore] __inline__ __attribute__((always_inline)) FixpipeTiling GenFixpipeTiling(uint16_t n)
{
    FixpipeTiling tiling;

    uint16_t maxDeqNums = 256;
    if (n <= maxDeqNums) {
        tiling.nIterNum = 1;
        tiling.nSize = n;
        tiling.isDb = false;
        tiling.tailNSize = 0;
    } else {
        tiling.isDb = true;
        uint16_t dbMaxDeqNums = maxDeqNums / 2;
        tiling.nIterNum = n / dbMaxDeqNums;
        tiling.nSize = dbMaxDeqNums;
        tiling.tailNSize = n % dbMaxDeqNums;
    }
    return tiling;
}

template <typename SrcT> struct FixpipeInfoParams {
    [aicore] __inline__ __attribute__((always_inline)) FixpipeInfoParams() {}

    [aicore] __inline__ __attribute__((always_inline)) FixpipeInfoParams(const FixpipeParams<SrcT>& intriParams, const uint8_t dstByteSize)
    {
        dstTypeSize = dstByteSize;
        srcTypeSize = B32_BYTE_SIZE;
        howo = (intriParams.burstLen * ONE_BLK_SIZE / srcTypeSize) / BLOCK_CUBE;
        roundHowo = DivCeil(howo, BLOCK_CUBE) * BLOCK_CUBE;
        fracLen = BLOCK_CUBE;
        c0 = fracLen;




        n = intriParams.cburstNum * BLOCK_CUBE;
        m = howo;



        srcStride = intriParams.srcStride * BLOCK_CUBE + roundHowo;




        if (intriParams.nz2ndParams.nz2ndEn) {

            dstStride = intriParams.dstStride;



              ;
            n = intriParams.nz2ndParams.originalNSize;
        } else {


            dstStride = intriParams.dstStride + intriParams.burstLen * dstTypeSize / srcTypeSize;
        }

        sid = 0;
        quantPre = intriParams.quantParams.quantPre;
        reluEn = intriParams.reluEn;
        nz2ndEn = intriParams.nz2ndParams.nz2ndEn;
        ndNum = intriParams.nz2ndParams.ndNum;
        srcNdStride = intriParams.nz2ndParams.srcNdStride;
        dstNdStride = intriParams.nz2ndParams.dstNdStride;


        if (intriParams.quantParams.quantPre == QuantMode_t::DEQF16 ||
            intriParams.quantParams.quantPre == QuantMode_t::QF322B8_PRE ||
            intriParams.quantParams.quantPre == QuantMode_t::REQ8) {
            deqScalar = intriParams.quantParams.deqScalar;
        }

        unitFlag = intriParams.unitFlag;
    }


    uint8_t dstTypeSize = 0;
    uint8_t srcTypeSize = 0;
    uint16_t howo = 0;
    uint16_t roundHowo = 0;
    uint8_t fracLen = 0;
    uint8_t c0 = 0;
    uint16_t n = 0;
    uint16_t m = 0;
    uint16_t srcStride = 0;
    uint32_t dstStride = 0;
    uint16_t burstLen = 0;
    uint8_t sid = 0;
    bool channelSplit = false;
    uint8_t unitFlag = 0;


    QuantMode_t quantPre = QuantMode_t::NoQuant;
    __attribute__((cce_cube_buff)) uint64_t* cbufWorkspace;
    uint64_t deqScalar = 0;

    bool reluEn = false;

    bool nz2ndEn = false;
    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;

    FixpipeTiling tiling;
};


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, FixpipeInfoParams<T> &fixpipeInfo)
{
                                                                                                          ;
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src, FixpipeInfoParams<SrcT>& fixpipeInfo)
{
                                                                                                         ;






    if (fixpipeInfo.quantPre == QuantMode_t::VDEQF16 || fixpipeInfo.quantPre == QuantMode_t::VQF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::VREQ8) {
        fixpipeInfo.tiling = GenFixpipeTiling(fixpipeInfo.n);
        for (uint16_t i = 0; i < fixpipeInfo.tiling.nIterNum; ++i) {
            FixpipeL0C2L1ImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.nSize, i);
        }

        if (fixpipeInfo.tiling.tailNSize > 0) {
            FixpipeL0C2L1ImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.tailNSize, fixpipeInfo.tiling.nIterNum);
        }
        return;
    }





    if (fixpipeInfo.quantPre == QuantMode_t::DEQF16 || fixpipeInfo.quantPre == QuantMode_t::QF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::REQ8) {

        SetQuantPreImpl(fixpipeInfo.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1(dst, src, fixpipeInfo, fixpipeInfo.n);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImpl(__attribute__((cce_global)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src, FixpipeInfoParams<SrcT>& fixpipeInfo)
{
    if (fixpipeInfo.nz2ndEn) {
        uint64_t ndPara = static_cast<uint64_t>(fixpipeInfo.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(fixpipeInfo.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(fixpipeInfo.ndNum);
        SetNdParaImpl(ndPara);
    }






    if (fixpipeInfo.quantPre == QuantMode_t::VDEQF16 || fixpipeInfo.quantPre == QuantMode_t::VQF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::VREQ8) {
        fixpipeInfo.tiling = GenFixpipeTiling(fixpipeInfo.n);
        for (uint16_t i = 0; i < fixpipeInfo.tiling.nIterNum; ++i) {
            FixpipeL0C2GMImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.nSize, i);
        }

        if (fixpipeInfo.tiling.tailNSize > 0) {
            FixpipeL0C2GMImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.tailNSize, fixpipeInfo.tiling.nIterNum);
        }
        return;
    }






    if (fixpipeInfo.quantPre == QuantMode_t::DEQF16 || fixpipeInfo.quantPre == QuantMode_t::QF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::REQ8) {
        SetQuantPreImpl(fixpipeInfo.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut(dst, src, fixpipeInfo, fixpipeInfo.n);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1ImplN(__attribute__((cce_cube_buff)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(fixpipeInfo, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1(dst, src, fixpipeInfo, calNSize, nIterIndex);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImplN(__attribute__((cce_global)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(fixpipeInfo, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut(dst, src, fixpipeInfo, calNSize, nIterIndex);
}



template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToL1(__attribute__((cce_cube_buff)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * fixpipeInfo.srcStride * fixpipeInfo.c0;
    uint32_t dstOffset = 0;
    if (fixpipeInfo.nz2ndEn) {
        dstOffset = nIterIndex * fixpipeInfo.tiling.nSize;
    } else {
        dstOffset = cburstNum * nIterIndex * fixpipeInfo.dstStride * 32 / sizeof(DstT);
    }



    return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), fixpipeInfo.sid,
        calNSize, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride, fixpipeInfo.unitFlag,
        fixpipeInfo.quantPre, static_cast<uint8_t>(fixpipeInfo.reluEn), fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
}

template <typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) uint64_t GetGMLen(const FixpipeInfoParams<SrcT>& fixpipeInfo,
                                    const uint16_t& calNSize, const uint16_t& dstEleSize)
{
    constexpr uint16_t dstStrideUnit = 32;
    constexpr uint16_t fractalNsize = 16;
    uint64_t cburstNum = calNSize / fractalNsize;
    uint64_t gmLen = (cburstNum - 1) * fixpipeInfo.dstStride * dstStrideUnit +
        fixpipeInfo.m * fractalNsize * dstEleSize;
    if (fixpipeInfo.nz2ndEn) {

        gmLen = (static_cast<uint64_t>(fixpipeInfo.ndNum) - 1) * dstEleSize * fixpipeInfo.dstNdStride +
            (fixpipeInfo.m - 1) * fixpipeInfo.dstStride * dstEleSize + cburstNum * fractalNsize * dstEleSize;
    }
    return gmLen;
}



template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToOut(__attribute__((cce_global)) DstT* dst, __attribute__((cce_cube_c)) SrcT* src,
    const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * fixpipeInfo.srcStride * fixpipeInfo.c0;
    uint32_t dstOffset = 0;
    if (fixpipeInfo.nz2ndEn) {
        dstOffset = nIterIndex * fixpipeInfo.tiling.nSize;
    } else {
        dstOffset = cburstNum * nIterIndex * fixpipeInfo.dstStride * 32 / sizeof(DstT);
    }
    if constexpr (g_gm_overflow_check) {
        bool isSrc = false;
        uint16_t dstEleSize = sizeof(DstT);
        uint64_t gmLen = GetGMLen(fixpipeInfo, calNSize, dstEleSize);
        AscendCUtils::CheckGmMemOverflow((__attribute__((cce_global)) DstT*)(dst + dstOffset), isSrc, gmLen);
    }


    return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), fixpipeInfo.sid,
        calNSize, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride, fixpipeInfo.unitFlag,
        fixpipeInfo.quantPre, static_cast<uint8_t>(fixpipeInfo.reluEn), fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
}

template <typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void CopyDeqTensorToFbuf(const FixpipeInfoParams<SrcT>& fixpipeInfo, uint16_t calNSize,
    uint16_t nIterIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t deqDataSize = DivCeil(calNSize * sizeof(uint64_t), 128) * 128;
    __attribute__((cce_fixpipe_buff)) uint64_t* deqTensorTempBuf =
        AscendCUtils::GetTemporaryFbBufferAddr<uint64_t>(0, deqDataSize / sizeof(uint64_t));
    uint32_t deqValueOffset = nIterIndex * fixpipeInfo.tiling.nSize;

    uint16_t fbufBurstLen = deqDataSize / 128;
    copy_cbuf_to_fbuf(deqTensorTempBuf, fixpipeInfo.cbufWorkspace + deqValueOffset, 1, fbufBurstLen, 0, 0);

    uint64_t deqTensorAddr = (((uint64_t)deqTensorTempBuf) >> (uint64_t)7) << 8;
    set_fpc(deqTensorAddr);
    AscendCUtils::FreeTemporaryFbBuffer<uint64_t>(deqTensorTempBuf);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParams<SrcT>& intriParams)
{
    FixpipeInfoParams<SrcT> fixpipeInfo(intriParams, sizeof(DstT));
    FixpipeL0C2L1Impl((__attribute__((cce_cube_buff)) DstT*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) SrcT*)srcLocal.GetPhyAddr(), fixpipeInfo);
}

template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<uint64_t>& cbufWorkspace, const FixpipeParams<SrcT>& intriParams)
{
    FixpipeInfoParams<SrcT> fixpipeInfo(intriParams, sizeof(DstT));
    fixpipeInfo.cbufWorkspace = (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr();
    FixpipeL0C2L1Impl((__attribute__((cce_cube_buff)) DstT*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) SrcT*)srcLocal.GetPhyAddr(), fixpipeInfo);
}


template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT>& dstGlobal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParams<SrcT>& intriParams)
{







    FixpipeInfoParams<SrcT> fixpipeInfo(intriParams, sizeof(DstT));

    FixpipeL0C2GMImpl((__attribute__((cce_global)) DstT*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_c)) SrcT*)srcLocal.GetPhyAddr(), fixpipeInfo);






}


template <typename DstT, typename SrcT>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT> &dstGlobal, const LocalTensor<SrcT> &srcLocal,
    const LocalTensor<uint64_t> &cbufWorkspace, const FixpipeParams<SrcT> &intriParams)
{
    FixpipeInfoParams<SrcT> fixpipeInfo(intriParams, sizeof(DstT));
    fixpipeInfo.cbufWorkspace = (__attribute__((cce_cube_buff)) uint64_t *)cbufWorkspace.GetPhyAddr();
    FixpipeL0C2GMImpl((__attribute__((cce_global)) DstT *)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(), fixpipeInfo);
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_dump_tensor_impl.h" 2


namespace AscendC {
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_dumpWorkspaceReserved;

template <typename T> [aicore] __inline__ __attribute__((always_inline)) uint32_t GetDataType(T data)
{
    uint32_t type;

    if (IsSameType<T, uint8_t>::value) {
        return 4;
    } else if (IsSameType<T, int8_t>::value) {
        return 2;
    } else if (IsSameType<T, int16_t>::value) {
        return 6;
    } else if (IsSameType<T, uint16_t>::value) {
        return 7;
    } else if (IsSameType<T, int32_t>::value) {
        return 3;
    } else if (IsSameType<T, uint32_t>::value) {
        return 8;
    } else if (IsSameType<T, uint64_t>::value) {
        return 10;
    } else if (IsSameType<T, int64_t>::value) {
        return 9;
    } else if (IsSameType<T, float>::value) {
        return 0;
    } else if (IsSameType<T, half>::value) {
        return 1;
    } else if (IsSameType<T, bfloat16_t>::value) {
        return 27;
    } else {
        return 33;
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint8_t GetDumpBlockIdx()
{
    if constexpr(g_coreType == AscendC::AIV) {
        return GetBlockIdxImpl();
    } else {
        return GetBlockIdxImpl() + AIV_CORE_NUM;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void InitDumpImpl(bool mixFlag, uint32_t gmLen)
{
    uint32_t totalBlockNum;

    if (g_dumpWorkspaceReserved == nullptr) {

                                                                                        ;
        return;
    }
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;

    if (mixFlag == true) {
        totalBlockNum = get_block_num() * (1 + MIX_NUM);
    } else {
        totalBlockNum = get_block_num();
    }
    uint32_t blockDumpSize = DUMP_UINTSIZE;

    uint32_t blockDim = GetDumpBlockIdx();
    if (blockDim >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t blkInfoLen = sizeof(BlockInfo) + sizeof(DumpMeta);
    uint64_t blockInfoStart = dumpWorkspaceStart + blockDim * DUMP_UINTSIZE;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_LEN_POS) = blockDumpSize;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_CORE_POS) = blockDim;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_BLOCKNUM_POS) = totalBlockNum;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_DUMPOFFSET_POS) = blockDumpSize - blkInfoLen;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_MAGIC_POS) = 0x5aa5bccd;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_RSV_POS) = 0;
    *((__attribute__((cce_global)) uint64_t*)((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_DUMP_ADDR)) = blockInfoStart + blkInfoLen;
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);

    blockInfoStart = blockInfoStart + sizeof(BlockInfo);
    *(__attribute__((cce_global)) uint32_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_TYPE_POS) =
        static_cast<uint32_t>(DumpType::DUMP_META);
    *(__attribute__((cce_global)) uint32_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_LEN_POS) = 8;
    *(__attribute__((cce_global)) uint16_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_BLOCK_DIM_POS) =
        static_cast<uint16_t>(get_block_num());
    *(__attribute__((cce_global)) uint8_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_CORE_TYPE_POS) =
        static_cast<uint8_t>(g_coreType);
    *(__attribute__((cce_global)) uint8_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_TASK_RATION) =
        static_cast<uint8_t>(mixFlag);
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + DUMP_META_RSV_POS) = 0;
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
[aicore] __inline__ __attribute__((always_inline)) DataCopyParams GetDataCopyParamImpl(uint32_t offset)
{
    DataCopyParams repeatParams;
    repeatParams.blockCount = 1;
    repeatParams.blockLen = offset / ONE_BLK_SIZE;
    repeatParams.srcStride = 0;
    repeatParams.dstStride = 0;
    return repeatParams;
}
[aicore] __inline__ __attribute__((always_inline)) FixpipeInfoParams<float> GetFixpipeParamImpl(uint32_t dumpSize)
{
    FixpipeParams<float> fixpipeParams;
    uint16_t align = (dumpSize % DEFAULT_BLOCK_SIZE == 0) ? 0 : 1;
    uint16_t cout_blocks = align + dumpSize / DEFAULT_BLOCK_SIZE;
    fixpipeParams = { cout_blocks, static_cast<uint16_t>(16 * 16 * sizeof(float) / 32), 0, 0};
    FixpipeInfoParams<float> fixpipeInfo(fixpipeParams, sizeof(float));
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    return fixpipeInfo;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t CheckValidPosition(const LocalTensor<T>& tensor)
{

    uint32_t position = 0;
    if ((Hardware)GetPhyType((TPosition)tensor.GetPosition()) == Hardware::UB) {
        position = static_cast<uint32_t>(AscendC::Hardware::UB);
        return position;
    } else if ((Hardware)GetPhyType((TPosition)tensor.GetPosition()) == Hardware::L1) {
        position = static_cast<uint32_t>(AscendC::Hardware::L1);
        return position;
    } else if ((Hardware)GetPhyType((TPosition)tensor.GetPosition()) == Hardware::L0C) {
        position = static_cast<uint32_t>(AscendC::Hardware::L0C);
        return position;
    } else {
        return false;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void DumpShapeImpl(const ShapeInfo &shapeInfo)
{
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t valueSize = sizeof(DumpShapeMessageHead);
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    uint32_t tlvSize = valueSize + DUMP_SHAPE_MESSAGE_TL_LEN;
    if (ptr->dumpOffset < tlvSize) {


                          ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_SHAPE);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_LEN_POS) = valueSize;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_DIM_POS) = shapeInfo.shapeDim;
    for (uint32_t idx = 0; idx < shapeInfo.shapeDim && idx < 8; idx++) {
        *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_SHAPE_START_POS + idx) = shapeInfo.shape[idx];
    }
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += tlvSize;
    ptr->dumpOffset -= tlvSize;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensorLocal2GMImpl(const LocalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint32_t position = CheckValidPosition(tensor);

    if (position == 0) {

                                                                                                          ;
        return;
    }

    T data;
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t offset = dumpSize * sizeof(T);

    if (offset % ONE_BLK_SIZE != 0) {

                                                                                                       ;
        return;
    }

    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;

    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    if (ptr->dumpOffset < (offset + sizeof(DumpMessageHead))) {


                                                   ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }

    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_LEN_POS) = offset + DUMP_MSG_HEAD_SIZE;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_ADDR_POS) =
        static_cast<uint32_t>(reinterpret_cast<uintptr_t>(tensor.GetPhyAddr()));
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DATA_TYPE_POS) = GetDataType(data);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DESC_POS) = desc;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_BUFFERID_POS) = 0;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_POSITION_POS) = position;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += sizeof(DumpMessageHead);
    ptr->dumpOffset -= sizeof(DumpMessageHead);
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    DataCopyParams repeatParams = GetDataCopyParamImpl(offset);
    const Hardware srcHWPos = GetPhyType((QuePosition)tensor.GetPosition());

    PipeBarrier<PIPE_ALL>();
    if (srcHWPos == Hardware::UB) {
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)(ptr->dumpAddr), (__attribute__((cce_unif_buff)) T*)tensor.GetPhyAddr(), repeatParams);
    } else if (srcHWPos == Hardware::L1) {
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)(ptr->dumpAddr), (__attribute__((cce_cube_buff)) T*)tensor.GetPhyAddr(), repeatParams);
    } else if (srcHWPos == Hardware::L0C) {
        if constexpr(g_coreType != AscendC::AIC) {
            return;
        }

        FixpipeInfoParams<float> fixpipeInfo = GetFixpipeParamImpl(dumpSize);
        copy_matrix_cc_to_gm((__attribute__((cce_global)) float*)(ptr->dumpAddr), (__attribute__((cce_cube_c)) float*)(tensor.GetPhyAddr()),
                    fixpipeInfo.sid, fixpipeInfo.n, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride,
                    fixpipeInfo.unitFlag, QuantMode_t::NoQuant, static_cast<uint8_t>(fixpipeInfo.reluEn),
                    fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
    }
    PipeBarrier<PIPE_ALL>();
    ptr->dumpOffset -= offset;
    ptr->dumpAddr += offset;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetLoopCount(uint32_t offset)
{
    uint32_t loopCount = 0;
    if (offset % ONE_DUMP_BACKUP_SIZE != 0) {
        loopCount = 1 + offset / ONE_DUMP_BACKUP_SIZE;
    } else {
        loopCount = offset / ONE_DUMP_BACKUP_SIZE;
    }
    return loopCount;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitTmpTensor(LocalTensor<T>& tmpLocal, uint8_t quePos)
{
    TBuffAddr tbuf_tmpLocal;
    tbuf_tmpLocal.logicPos = quePos;
    tmpLocal.SetAddr(tbuf_tmpLocal);



    tmpLocal.address_.bufferAddr = (uint64_t)(0);

    tmpLocal.address_.dataLen = ONE_DUMP_BACKUP_SIZE;
}
[aicore] __inline__ __attribute__((always_inline)) bool CheckDumpValid(uint32_t offset)
{
    if (offset % ONE_BLK_SIZE != 0) {
                                                                                      ;
        return false;
    }
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return false;
    }
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    if (reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) < DUMP_WORKSPACE_SIZE) {
                                                                                                   ;
        return false;
    }
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    if (ptr->dumpOffset < (offset + sizeof(DumpMessageHead) + ONE_DUMP_BACKUP_SIZE)) {

                                                                      ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return false;
    }

    return true;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpBlockInfoImpl(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    uint32_t position = static_cast<uint32_t>(AscendC::Hardware::GM);
    T data;
    uint32_t offset = dumpSize * sizeof(T);

    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * GetDumpBlockIdx());
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_LEN_POS) = offset + DUMP_MSG_HEAD_SIZE;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_ADDR_POS) =
        static_cast<uint32_t>(reinterpret_cast<uintptr_t>(tensor.GetPhyAddr()));
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DATA_TYPE_POS) = GetDataType(data);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DESC_POS) = desc;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_BUFFERID_POS) = 0;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_POSITION_POS) = position;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += sizeof(DumpMessageHead);
    ptr->dumpOffset -= sizeof(DumpMessageHead);
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpGMTailImpl(LocalTensor<T>& tmpLocal, uint32_t alignSize, uint64_t tmpAddr,
                                      uint64_t gmAddr, uint32_t offset)
{
    DataCopyParams tailParams = GetDataCopyParamImpl((alignSize + ONE_BLK_SIZE - 1) & (~(ONE_BLK_SIZE - 1)));
    if (g_coreType == AIV) {
        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(),
                          (__attribute__((cce_global)) T*)(tmpAddr + offset - alignSize), tailParams);
        PipeBarrier<PIPE_ALL>();
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), tailParams);
    } else if (g_coreType == AIC) {
        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(),
                          (__attribute__((cce_global)) T*)(tmpAddr + offset - alignSize), tailParams);
        PipeBarrier<PIPE_ALL>();
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), tailParams);
    }
    PipeBarrier<PIPE_ALL>();
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensorGM2GMImpl(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{
    uint32_t position = static_cast<uint32_t>(AscendC::Hardware::GM);
    T data;
    uint32_t offset = dumpSize * sizeof(T);
    if (!CheckDumpValid(offset)) {
        return;
    }
    DumpBlockInfoImpl(tensor, desc, dumpSize);
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * GetDumpBlockIdx());
    DataCopyParams backupParams = GetDataCopyParamImpl(ONE_DUMP_BACKUP_SIZE);
    LocalTensor<T> tmpLocal;
    uint64_t gmBackAddr = dumpWorkspaceStart + DUMP_UINTSIZE * (GetDumpBlockIdx() + 1) - ONE_DUMP_BACKUP_SIZE;


    PipeBarrier<PIPE_ALL>();
    if (g_coreType == AIV) {
        InitTmpTensor(tmpLocal, (uint8_t)QuePosition::VECIN);
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)(gmBackAddr), (__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
    } else if (g_coreType == AIC) {
        InitTmpTensor(tmpLocal, (uint8_t)QuePosition::A1);
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)(gmBackAddr), (__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
    }
    PipeBarrier<PIPE_ALL>();
    dcci((__attribute__((cce_global)) uint64_t*)gmBackAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);

    uint32_t alignSize = offset % ONE_DUMP_BACKUP_SIZE;
    uint64_t tmpAddr = static_cast<uint64_t>(reinterpret_cast<uintptr_t>(tensor.GetPhyAddr()));
    uint64_t gmAddr = ptr->dumpAddr;
    for (int i = 0; i < offset / ONE_DUMP_BACKUP_SIZE; i++) {
        if (g_coreType == AIV) {
            DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(),
                              (__attribute__((cce_global)) T*)(tmpAddr + ONE_DUMP_BACKUP_SIZE * i), backupParams);
            PipeBarrier<PIPE_ALL>();
            DataCopyUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
            gmAddr += ONE_DUMP_BACKUP_SIZE;
        } else if (g_coreType == AIC) {
            DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(),
                              (__attribute__((cce_global)) T*)(tmpAddr + ONE_DUMP_BACKUP_SIZE * i), backupParams);
            PipeBarrier<PIPE_ALL>();
            DataCopyL12GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), backupParams);
            gmAddr += ONE_DUMP_BACKUP_SIZE;
        }
        PipeBarrier<PIPE_ALL>();
    }
    if (alignSize != 0) {
        DumpGMTailImpl(tmpLocal, alignSize, tmpAddr, gmAddr, offset);
    }
    if (g_coreType == AIV) {
        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmpLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)gmBackAddr, backupParams);
    } else if (g_coreType == AIC) {
        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmpLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)gmBackAddr, backupParams);
    }
    PipeBarrier<PIPE_ALL>();
    ptr->dumpOffset -= offset;
    ptr->dumpAddr += offset;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsNum()
{
    return 0;
}

template <typename T, typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsNum(T scalar, Args... args)
{
    return 1 + GetArgsNum(args...);
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetStringLength(__attribute__((cce_global)) const char* s)
{
    uint32_t i = 0;
    while (*(s + i) != '\0') {
        i++;
    }
    return i + 1;
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSize()
{
    return 0;
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSize(Args&&... args);

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSizeImpl(__attribute__((cce_global)) const char* s, Args&&... args)
{
    uint32_t strLen = GetStringLength(s);
    uint32_t strParamSize = ONE_PARAM_SIZE + strLen;
    return strParamSize + GetArgsSize(args...);
}

template <typename T, typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSizeImpl(T scalar, Args&&... args)
{
    return ONE_PARAM_SIZE + GetArgsSize(args...);
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetArgsSize(Args&&... args)
{
    return GetArgsSizeImpl(args...);
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetParamSize(__attribute__((cce_global)) const char* fmt, Args&&... args)
{
    uint32_t fmtSize = GetStringLength(fmt);
    uint32_t argsSize = GetArgsSize(args...);
    return fmtSize + argsSize + ONE_PARAM_SIZE;
}

[aicore] __attribute__((cce_global)) __inline__ __attribute__((always_inline)) BlockInfo *GetBlockInfo()
{
    uint8_t core = GetDumpBlockIdx();
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo *blockInfo = (__attribute__((cce_global)) BlockInfo *)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    return blockInfo;
}

[aicore] __inline__ __attribute__((always_inline)) void WriteString(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, __attribute__((cce_global)) const char* s, uint32_t& offset)
{
    __attribute__((cce_global)) uint64_t *stringAddr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(paramAddr) + paramIdx;
    __attribute__((cce_global)) uint64_t *dstStrAddr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(paramAddr + offset);


    *((__attribute__((cce_global)) uint64_t *)stringAddr) = static_cast<uint64_t>(offset - ONE_PARAM_SIZE * paramIdx);
    dcci((__attribute__((cce_global)) uint64_t*)stringAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);


    __attribute__((cce_global)) char *d = (__attribute__((cce_global)) char *)(dstStrAddr);
    uint32_t strLen = GetStringLength(s);

    for (uint32_t i = 0; i < strLen; i++) {
        *(d + i) = *(s + i);
        dcci((__attribute__((cce_global)) uint64_t*)d, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    }
    offset += strLen;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void WriteScalar(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, T scalar)
{
    __attribute__((cce_global)) uint64_t *scalarAddr = (__attribute__((cce_global)) uint64_t *)paramAddr + paramIdx;
    *scalarAddr = 0;

    static_assert(!SupportType<T, double>(), "printf unsupport double type");

    if constexpr (SupportType<T, half, float>()) {
        *((__attribute__((cce_global)) float *)scalarAddr) = static_cast<float>(scalar);
    } else if constexpr (SupportType<T, bool, int8_t, int16_t, int32_t, int64_t>()) {
        *((__attribute__((cce_global)) int64_t *)scalarAddr) = static_cast<int64_t>(scalar);
    } else if constexpr(SupportType<T, bfloat16_t>()) {
        *((__attribute__((cce_global)) float *)scalarAddr) = ToFloat(scalar);
    } else if constexpr(SupportType<T, bool, uint8_t, uint16_t, uint32_t, uint64_t>()) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = static_cast<uint64_t>(scalar);
    } else if constexpr(std::is_pointer<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = (uintptr_t)scalar;
    }

    dcci((__attribute__((cce_global)) uint64_t*)scalarAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] __inline__ __attribute__((always_inline)) void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset)
{
    return;
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, Args&&... args);

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParamImpl(__attribute__((cce_global)) uint8_t *paramAddr, uint32_t paramIdx, uint32_t &offset,
                                    __attribute__((cce_global)) const char *s, Args&&... args)
{
    WriteString(paramAddr, paramIdx, s, offset);
    SetParam(paramAddr, paramIdx + 1, offset, args...);
}

template <typename T, typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParamImpl(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, T scalar,
                                    Args&&... args)
{
    WriteScalar(paramAddr, paramIdx, scalar);
    SetParam(paramAddr, paramIdx + 1, offset, args...);
}

template <typename... Args>
[aicore] __inline__ __attribute__((always_inline)) void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, Args&&... args)
{
    SetParamImpl(paramAddr, paramIdx, offset, args...);
}

[aicore] __inline__ __attribute__((always_inline)) void WriteTLHead(DumpType printType, __attribute__((cce_global)) uint8_t *tlv, uint32_t valueSize)
{
    *((__attribute__((cce_global)) uint32_t *)tlv) = static_cast<uint32_t>(printType);
    *((__attribute__((cce_global)) uint32_t *)tlv + 1) = valueSize;
    dcci((__attribute__((cce_global)) uint64_t*)tlv, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
[aicore] __inline__ __attribute__((always_inline)) void UpdateBlockInfo(uint32_t tlvSize)
{
    __attribute__((cce_global)) BlockInfo *blockInfo = GetBlockInfo();
    uint32_t remainSize = blockInfo->dumpOffset;
    uint64_t lastDumpAddr = blockInfo->dumpAddr;

    __attribute__((cce_global)) uint8_t *blockInfoStart = (__attribute__((cce_global)) uint8_t *)blockInfo;
    *((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_DUMPOFFSET_POS) = remainSize - tlvSize;
    *((__attribute__((cce_global)) uint64_t *)((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_DUMP_ADDR)) = lastDumpAddr + tlvSize;
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PrintfImpl(DumpType printType, __attribute__((cce_global)) const char* fmt, Args&&... args)
{

    uint8_t blockIdx = GetDumpBlockIdx();
    if (blockIdx >= DUMP_CORE_COUNT) {
        return;
    }
    __attribute__((cce_global)) BlockInfo *blockInfo = GetBlockInfo();
    uint32_t remainSize = blockInfo->dumpOffset;
    uint64_t dumpAddr = blockInfo->dumpAddr;

    uint32_t paramSize = GetParamSize(fmt, args...);
    uint32_t paramNum = GetArgsNum(args...) + 1;
    paramSize = (paramSize + ONE_PARAM_SIZE - 1) & (~(ONE_PARAM_SIZE - 1));

    uint32_t tlvSize = paramSize + ONE_PARAM_SIZE;
    if (tlvSize > remainSize) {
        __attribute__((cce_global)) uint8_t *blockInfoStart = (__attribute__((cce_global)) uint8_t *)blockInfo;
        *((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }

    __attribute__((cce_global)) uint8_t *tlvAddr = (__attribute__((cce_global)) uint8_t *)dumpAddr;
    WriteTLHead(printType, tlvAddr, paramSize);
    __attribute__((cce_global)) uint8_t *paramAddr = tlvAddr + ONE_PARAM_SIZE;
    uint32_t offset = paramNum * ONE_PARAM_SIZE;
    WriteString(paramAddr, 0, fmt, offset);
    uint32_t paramIdx = 1;
    SetParam(paramAddr, paramIdx, offset, args...);


    UpdateBlockInfo(tlvSize);

}
[aicore] __inline__ __attribute__((always_inline)) void InitDump(bool mixFlag, uint32_t gmLen)
{

    g_dumpWorkspaceReserved = GetSysWorkSpacePtr();
    InitDumpImpl(mixFlag, gmLen);



}
[aicore] __inline__ __attribute__((always_inline)) void InitDump(bool mixFlag, __attribute__((cce_global)) uint8_t* dumpStartAddr, uint32_t gmLen)
{

    g_dumpWorkspaceReserved = dumpStartAddr + DUMP_WORKSPACE_SIZE;
    InitDumpImpl(mixFlag, gmLen);



}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T> &tensor, uint32_t desc, uint32_t dumpSize);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T>& tensor, uint32_t desc,
    uint32_t dumpSize, const ShapeInfo& shapeInfo);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc,
    uint32_t dumpSize, const ShapeInfo& shapeInfo);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const LocalTensor<T> &tensor,
    uint32_t index, uint32_t countOff, uint32_t dumpSize);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const GlobalTensor<T> &tensor,
    uint32_t index, uint32_t countOff, uint32_t dumpSize);

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PRINTF(__attribute__((cce_global)) const char* fmt, Args&&... args);
template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void printf(__attribute__((cce_global)) const char* fmt, Args&&... args);
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_dump_tensor_intf.h"
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_mm_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_mm_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_cube_buff)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, loadDataParam.sid, 0, inc);
        } else {
            if (loadDataParam.ifTranspose) {
                load_cbuf_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 1, inc);
            } else {
                load_cbuf_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 0, inc);
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_cube_buff)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_cb_s4((__attribute__((cce_cube_b)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, loadDataParam.sid, 0, inc);
        } else {
            if (loadDataParam.ifTranspose) {
                load_cbuf_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 1, inc);
            } else {
                load_cbuf_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 0, inc);
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        load_gm_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
            loadDataParam.dstGap, loadDataParam.sid, (addr_cal_mode_t)0);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        load_gm_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
            loadDataParam.dstGap, loadDataParam.sid, (addr_cal_mode_t)0);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L1Cal(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (loadDataParam.addrMode == 0) {
            load_gm_to_cbuf(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                loadDataParam.dstGap, loadDataParam.sid, inc);
        } else {
            load_gm_to_cbuf(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                loadDataParam.dstGap, loadDataParam.sid, dec);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DGM2L1Cal(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                            ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0ATransposeCal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            load_cbuf_to_ca_transpose(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes,
                loadDataParam.srcStride, loadDataParam.dstGap, inc, loadDataParam.dstFracGap);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BTransposeCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_cb_transpose_s4((__attribute__((cce_cube_b)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, inc,
                loadDataParam.dstFracGap);
        } else {
            load_cbuf_to_cb_transpose(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes,
                loadDataParam.srcStride, loadDataParam.dstGap, inc, loadDataParam.dstFracGap);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData2DL12L0BTransposeCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParamsV2 &loadDataParam)
{
                                                                                                             ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2<T> &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParams.kExtension,
                loadDataParams.mExtension, loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW,
                loadDataParams.strideH, loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        } else {
            img2colv2_cbuf_to_ca(dst, src, loadDataParams.kExtension, loadDataParams.mExtension,
                loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH,
                loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2<T> &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_cb(dst, src, loadDataParams.kExtension, loadDataParams.mExtension,
                loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH,
                loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_ca(reinterpret_cast<__attribute__((cce_cube_a)) half*>(dst),
            reinterpret_cast<__attribute__((cce_cube_buff)) half*>(src),
            loadDataParams.kExtension, loadDataParams.mExtension, loadDataParams.kStartPt,
            loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH, loadDataParams.filterW,
            loadDataParams.filterH, loadDataParams.dilationFilterW, loadDataParams.dilationFilterH,
            loadDataParams.filterSizeW, loadDataParams.filterSizeH, loadDataParams.enTranspose,
            loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_cb(reinterpret_cast<__attribute__((cce_cube_b)) half*>(dst),
            reinterpret_cast<__attribute__((cce_cube_buff)) half*>(src),
            loadDataParams.kExtension, loadDataParams.mExtension, loadDataParams.kStartPt,
            loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH, loadDataParams.filterW,
            loadDataParams.filterH, loadDataParams.dilationFilterW, loadDataParams.dilationFilterH,
            loadDataParams.filterSizeW, loadDataParams.filterSizeH, loadDataParams.enTranspose,
            loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParams.extConfig,
                loadDataParams.extConfig >> LOAD_M_EXTENSION, loadDataParams.extConfig >> LOAD_K_START_POSITION,
                loadDataParams.extConfig >> LOAD_M_START_POSITION, loadDataParams.filterConfig,
                loadDataParams.filterConfig >> LOAD_STRIDE_H, loadDataParams.filterConfig >> LOAD_FILTER_W,
                loadDataParams.filterConfig >> LOAD_FILTER_H, loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        } else {
            img2colv2_cbuf_to_ca(dst, src, loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
                loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
                loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
                loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_cb(dst, src, loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
                loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
                loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
                loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        }
    }
}



template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void MmadCal(__attribute__((cce_cube_c)) DstT* c, __attribute__((cce_cube_a)) Src0T* a, __attribute__((cce_cube_b)) Src1T* b, const MmadParams& mmadParams)
{
    if constexpr(g_coreType == AscendC::AIC) {





                                                            ;
        bool cmatrixInitVal = mmadParams.cmatrixInitVal && (!mmadParams.isBias);
        if constexpr ((IsSameType<Src0T, int4b_t>::value) && (IsSameType<Src1T, int4b_t>::value)) {
            mad_s4(c, (__attribute__((cce_cube_a)) void *)a, (__attribute__((cce_cube_b)) void *)b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag,
                mmadParams.kDirectionAlign, mmadParams.cmatrixSource, cmatrixInitVal);
        } else {
            mad(c, a, b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag, mmadParams.kDirectionAlign,
                mmadParams.cmatrixSource, cmatrixInitVal);
        }
    }
}

template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void MmadCal(__attribute__((cce_cube_c)) DstT* c, __attribute__((cce_cube_a)) Src0T* a, __attribute__((cce_cube_b)) Src1T* b, uint64_t bias,
    const MmadParams& mmadParams, bool cmatrixSource)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr ((IsSameType<Src0T, int4b_t>::value) && (IsSameType<Src1T, int4b_t>::value)) {
            mad_s4(c, (__attribute__((cce_cube_a)) void *)a, (__attribute__((cce_cube_b)) void *)b, mmadParams.m, mmadParams.k, mmadParams.n,
                mmadParams.unitFlag, mmadParams.kDirectionAlign, cmatrixSource,
                mmadParams.cmatrixInitVal);
        } else {
            mad(c, a, b, bias, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag,
                mmadParams.kDirectionAlign, cmatrixSource, mmadParams.cmatrixInitVal);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void MmadSpCal(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, const MmadParams &mmadParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        mad_sp(c, a, b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag, 0, 1);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparseCal(const LocalTensor<int8_t> &dstLocal, const LocalTensor<int8_t> &srcLocal,
    const LocalTensor<uint8_t> &idxLocal, const LoadData2dParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {




        uint64_t src0tmp = reinterpret_cast<uint64_t>(srcLocal.GetPhyAddr());
        uint64_t src1tmp = reinterpret_cast<uint64_t>(idxLocal.GetPhyAddr());


        uint64_t srctmp = (src0tmp & 0xffffffff) | ((src1tmp & 0xffffffff) << 32);
        __attribute__((cce_cube_buff)) int8_t *src = reinterpret_cast<__attribute__((cce_cube_buff)) int8_t *>(srctmp);

        load_cbuf_to_cb_sp((__attribute__((cce_cube_b)) int8_t *)dstLocal.GetPhyAddr(), src, loadDataParam.startIndex,
            loadDataParam.repeatTimes);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void LoadUnzipIndexCal(const GlobalTensor<int8_t>& srcTensor, uint32_t numOfIndexTabEntry)
{
                                                       ;
}




[aicore] __inline__ __attribute__((always_inline)) void Load3DSetFMatrixCal(uint16_t l1H, uint16_t l1W, const uint8_t padList[4])
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t regFMatrix = 0;
        regFMatrix |= uint64_t(l1W & 0xFFFF);

        uint32_t l1HShiftBit = 16;
        regFMatrix |= uint64_t(l1H & 0xFFFF) << l1HShiftBit;

        uint32_t padNumber = 4;
        uint32_t padListShiftBit = 8;
        uint32_t padListShiftBase = 32;
        for (uint32_t i = 0; i < padNumber; i++) {
            regFMatrix |= uint64_t(padList[i] & 0xFF) << (padListShiftBase + i * padListShiftBit);
        }
        set_fmatrix(regFMatrix);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void Load3DSetFMatrixBCal(uint16_t l1H, uint16_t l1W, const uint8_t padList[4])
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t regFMatrix = 0;
        regFMatrix |= (uint64_t)l1W;

        uint32_t l1HShiftBit = 16;
        regFMatrix |= (uint64_t)l1H << l1HShiftBit;

        uint32_t padNumber = 4;
        uint32_t padListShiftBit = 8;
        uint32_t padListShiftBase = 32;
        for (uint32_t i = 0; i < padNumber; i++) {
            regFMatrix |= uint64_t(padList[i] & 0xFF) << (padListShiftBase + i * padListShiftBit);
        }
        set_fmatrix_b(regFMatrix);
    }
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void Load3DSetPaddingCal(const T padValue)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint16_t paddingValue = 0;
        uint16_t padValueShiftBit = 8;
        if constexpr (sizeof(T) == B16_BYTE_SIZE) {
            paddingValue = (uint16_t)GetScalarBitcodeValue((T)padValue);
        } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
            paddingValue = (uint32_t)GetScalarBitcodeValue((T)padValue);
        } else {
            paddingValue = (((uint16_t)padValue) << padValueShiftBit) | (uint16_t)padValue;
        }
        set_padding(paddingValue);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV1L12L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV1L12L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV1L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV2<T>& loadDataParams)
{
                                                                                       ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) int8_t* dst, __attribute__((cce_cube_buff)) int8_t* src,
    const LoadData3DParamsV2<int8_t>& loadDataParams)
{
                                                                                                        ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) uint8_t* dst, __attribute__((cce_cube_buff)) uint8_t* src,
    const LoadData3DParamsV2<uint8_t>& loadDataParams)
{
                                                                                                         ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                          ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) int8_t* dst, __attribute__((cce_cube_buff)) int8_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                                           ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) uint8_t* dst, __attribute__((cce_cube_buff)) uint8_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                                            ;
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastVecToMMCal(__attribute__((cce_cube_c)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t blockCount,
    const uint8_t blockLen, const uint8_t srcGap, const uint8_t dstGap)
{
                                                         ;
}




[aicore] __inline__ __attribute__((always_inline)) void CheckInitConstValueParams(uint16_t repeatTimes, uint16_t blockNum, uint16_t dstGap)
{
                                                                                          ;
                                                                                    ;
                                                                                ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitL1BufferCal(__attribute__((cce_cube_buff)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = ((uint64_t)initConstValueParams.blockNum << 16) |
            ((uint64_t)initConstValueParams.dstGap << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_cbuf_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, half>::value) {
            create_cbuf_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_cbuf_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_cbuf_matrix(dst, repeatBit, (uint32_t)GetScalarBitcodeValue(initConstValueParams.initValue));
        } else {

                                                                                                                     ;
        }
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitL0ANzMatrixCal(__attribute__((cce_cube_a)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = ((uint64_t)initConstValueParams.blockNum << 16) |
            ((uint64_t)initConstValueParams.dstGap << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_ca_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, half>::value) {
            create_ca_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_ca_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_ca_matrix(dst, repeatBit, (uint32_t)GetScalarBitcodeValue(initConstValueParams.initValue));
        } else {

                                                                                                                     ;
        }
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitL0BNzMatrixCal(__attribute__((cce_cube_b)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = ((uint64_t)initConstValueParams.blockNum << 16) |
            ((uint64_t)initConstValueParams.dstGap << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_cb_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, half>::value) {
            create_cb_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_cb_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_cb_matrix(dst, repeatBit, (uint32_t)GetScalarBitcodeValue(initConstValueParams.initValue));
        } else {

                                                                                                                     ;
        }
    }
}



[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeatCal(const LoadDataRepeatParam& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t rptConfig = (uint64_t)repeatParams.repeatStride | ((uint64_t)repeatParams.repeatTime << 16) |
            ((uint64_t)repeatParams.repeatMode << 24);
        set_l3d_rpt(rptConfig);
    }
}




[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundaryCal(uint32_t boundaryValue)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_l1_3d_size(static_cast<uint64_t>(boundaryValue));
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadImageToLocalCal(__attribute__((cce_cube_buff)) T *dst, const LoadImageToLocalParams &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {

                                                                                                ;
        load_image_to_cbuf(dst, static_cast<uint16_t>(loadDataParams.horizSize - 1),
            static_cast<uint16_t>(loadDataParams.vertSize - 1), loadDataParams.horizStartPos,
            loadDataParams.vertStartPos, static_cast<uint16_t>(loadDataParams.srcHorizSize - 1),
            loadDataParams.topPadSize, loadDataParams.botPadSize, loadDataParams.leftPadSize,
            loadDataParams.rightPadSize, loadDataParams.sid);
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipToL1Cal(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipToL0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipToL0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h" 2








namespace AscendC {
struct IsResetLoad3dConfig {
    [aicore] constexpr IsResetLoad3dConfig(const bool isSetFMatrixIn, const bool isSetPaddingIn)
    {
        isSetFMatrix = isSetFMatrixIn;
        isSetPadding = isSetPaddingIn;
    }
    bool isSetFMatrix = true;
    bool isSetPadding = true;
};

constexpr IsResetLoad3dConfig IS_RESER_LOAD3D_DEFAULT_CONFIG = {true, true};
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams)
{





    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadData with LoadData2DParams");
    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadData with LoadData2DParams");
    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData2DParams");
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                 ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadDataImpl(const LocalTensor<T>& dstLocal,
    const GlobalTensor<T>& srcLocal, const LoadData2DParams& loadDataParams)
{





    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData2DParams when dst position is A2");
        LoadData2DGM2L0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData2DParams when dst position is B2");
        LoadData2DGM2L0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L1) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal",
            "LoadData with LoadData2DParams when dst position is A1 / B1");
        LoadData2DGM2L1Cal((__attribute__((cce_cube_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                  ;
    }
}
# 130 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTransposeImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParams& loadDataParams)
{





    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadDataWithTranspose");

    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadDataWithTranspose");

    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadDataWithTranspose");
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {


                                                                                                                   ;
        LoadData2DL12L0ATransposeCal((__attribute__((cce_cube_a)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T *)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else if (dstScope == Hardware::L0B) {



                         ;
        LoadData2DL12L0BTransposeCal((__attribute__((cce_cube_b)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T *)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else {
                                                                                                                  ;
    }
}
# 176 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTransposeImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParamsV2& loadDataParams)
{





    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadDataWithTranspose");

    CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadDataWithTranspose");

    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadDataWithTranspose");
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0B) {



                         ;
        LoadData2DL12L0BTransposeCal((__attribute__((cce_cube_b)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T *)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else {
                                                                                                                  ;
    }
}
# 220 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams)
{





    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1",
        "LoadData with LoadData2DParamsV2 when srcLocal tensor is LocalTensor");
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                   ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadDataImpl(const LocalTensor<T>& dstLocal,
    const GlobalTensor<T>& srcLocal, const LoadData2DParamsV2& loadDataParams)
{





    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DGM2L0ACal((__attribute__((cce_cube_a)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T *)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DGM2L0BCal((__attribute__((cce_cube_b)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T *)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L1) {
        LoadData2DGM2L1Cal((__attribute__((cce_cube_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T *)srcLocal.GetPhyAddr(), loadDataParams);
    } else {

                                                                                    ;
    }
}
# 293 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV1<T>& loadDataParams)
{







                     ;

    if constexpr (defaultConfig.isSetFMatrix) {
        Load3DSetFMatrixCal(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.padList);
    }
    if constexpr (defaultConfig.isSetPadding) {
        Load3DSetPaddingCal(loadDataParams.padValue);
    }

    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadData with LoadData3DParamsV1");
    CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadData with LoadData3DParamsV1");
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12L0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12L0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12UBCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {
                                                                                                        ;
    }
}
# 357 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2<T>& loadDataParams)
{


      ;
    if constexpr (defaultConfig.isSetFMatrix) {
        Load3DSetFMatrixCal(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.padList);
    }
    if constexpr (defaultConfig.isSetPadding) {
        Load3DSetPaddingCal(loadDataParams.padValue);
    }

    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());





    if (dstScope == Hardware::L0A) {



                                                       ;
    } else if (dstScope == Hardware::L0B) {


                                                                                                        ;
    }
# 399 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    CheckTensorPos<T>(srcLocal, Hardware::L1, "srcLocal", "A1 / B1", "LoadData with LoadData3DParamsV2");
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12UBCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {
                                                                                                        ;
    }
}



template <const IsResetLoad3dConfig& defaultConfig>
[[deprecated("NOTICE: LoadData<IsResetLoad3dConfig> has been deprecated and will be removed in the next version."
             " Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<bfloat16_t>& dstLocal, const LocalTensor<bfloat16_t>& srcLocal,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    LoadDataImpl<bfloat16_t, defaultConfig>(dstLocal, srcLocal, loadDataParams);
}
# 449 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams)
{





    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        LoadData3DV2L12UBCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) T*)srcLocal.GetPhyAddr(), loadDataParams);
    } else {
                                                                                                           ;
    }
}




template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataImpl(const LocalTensor<bfloat16_t>& dstLocal, const LocalTensor<bfloat16_t>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams)
{






    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());

    if (dstScope == Hardware::L0A) {
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) half*)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) half*)srcLocal.GetPhyAddr(),
            loadDataParams);
    } else {

                                                                         ;
    }
}
# 518 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename DstT, typename Src0T, typename Src1T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckMmadAlign(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal) {
    constexpr uint64_t ALIGN_1024B = 1024;
    if constexpr ((IsSameType<Src0T, half>::value) && (IsSameType<Src1T, half>::value) &&
    (IsSameType<DstT, half>::value)) {
        CheckTensorAlign<DstT>(dstLocal, VALUE_512, "dstLocal", "Mmad");
    } else {
        CheckTensorAlign<DstT>(dstLocal, ALIGN_1024B, "dstLocal", "Mmad");
        }
    CheckTensorAlign<Src0T>(fmLocal, VALUE_512, "fmLocal", "Mmad");
    CheckTensorAlign<Src1T>(filterLocal, VALUE_512, "filterLocal", "Mmad");
}

template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void MmadImpl(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const MmadParams& mmadParams)
{






    MmadCal((__attribute__((cce_cube_c)) DstT*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_a)) Src0T*)fmLocal.GetPhyAddr(),
        (__attribute__((cce_cube_b)) Src1T*)filterLocal.GetPhyAddr(), mmadParams);
}

template <typename DstT, typename Src0T, typename Src1T, typename BiasT>
[aicore] __inline__ __attribute__((always_inline)) void MmadImpl(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const LocalTensor<BiasT>& biasLocal, const MmadParams& mmadParams)
{
# 566 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    const Hardware biasScope = GetPhyType((QuePosition)biasLocal.GetPosition());
    bool cmatrixSource = false;
    if (biasScope == Hardware::BIAS) {
        cmatrixSource = true;
    } else if (biasScope == Hardware::L0C) {
        cmatrixSource = false;
    } else {

                                                                                                       ;
    }
    MmadCal((__attribute__((cce_cube_c)) DstT *)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_a)) Src0T *)fmLocal.GetPhyAddr(),
        (__attribute__((cce_cube_b)) Src1T *)filterLocal.GetPhyAddr(), (uint64_t)biasLocal.GetPhyAddr(), mmadParams, cmatrixSource);
}


[aicore] __inline__ __attribute__((always_inline)) void MmadSpImpl(const LocalTensor<int32_t>& dstLocal, const LocalTensor<int8_t>& fmLocal,
    const LocalTensor<int8_t>& filterLocal, const MmadParams& mmadParams)
{
    CheckTensorPos<int32_t>(dstLocal, Hardware::L0C, "dstLocal", "CO1", "MmadWithSparse");
    CheckTensorPos<int8_t>(fmLocal, Hardware::L0A, "fmLocal", "A2", "MmadWithSparse");
    CheckTensorPos<int8_t>(filterLocal, Hardware::L0B, "filterLocal", "B2", "MmadWithSparse");
    CheckTensorAlign<int32_t>(dstLocal, 1024, "dstLocal", "MmadWithSparse");
    CheckTensorAlign<int8_t>(fmLocal, VALUE_512, "fmLocal", "MmadWithSparse");
    CheckTensorAlign<int8_t>(filterLocal, VALUE_512, "filterLocal", "MmadWithSparse");
                                                                                 ;
                                                                                 ;
                                                                                 ;
    MmadSpCal((__attribute__((cce_cube_c)) int32_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_a)) int8_t*)fmLocal.GetPhyAddr(),
        (__attribute__((cce_cube_b)) int8_t*)filterLocal.GetPhyAddr(), mmadParams);
}

[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparseImpl(const LocalTensor<int8_t> &dstLocal, const LocalTensor<int8_t> &srcLocal,
    const LocalTensor<uint8_t> &idxLocal, const LoadData2dParams &loadDataParam)
{
    CheckTensorPos<int8_t>(dstLocal, Hardware::L0B, "dstLocal", "B2", "LoadDataWithSparse");
    CheckTensorPos<int8_t>(srcLocal, Hardware::L1, "srcLocal", "B1", "LoadDataWithSparse");
    CheckTensorPos<uint8_t>(idxLocal, Hardware::L1, "idxLocal", "B1", "LoadDataWithSparse");
    CheckTensorAlign<int8_t>(dstLocal, VALUE_512, "dstLocal", "LoadDataWithSparse");
    CheckTensorAlign<int8_t>(srcLocal, ONE_BLK_SIZE, "srcLocal", "LoadDataWithSparse");
    CheckTensorAlign<uint8_t>(idxLocal, ONE_BLK_SIZE, "idxLocal", "LoadDataWithSparse");
    LoadDataWithSparseCal(dstLocal, srcLocal, idxLocal, loadDataParam);
}
# 620 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void BroadCastVecToMMImpl(const LocalTensor<T> &dstLocal,
    const LocalTensor<U> &srcLocal, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap)
{





    BroadCastVecToMMCal((__attribute__((cce_cube_c)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), blockCount, blockLen,
        srcGap, dstGap);
}
# 642 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Load3DSetPaddingImpl(const T padValue)
{
    Load3DSetPaddingCal(padValue);
}
# 660 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConstValueImpl(const LocalTensor<T> &dstLocal,
    const InitConstValueParams<T> &initConstValueParams)
{
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "InitConstValue when TPosition is A2");
        InitL0ANzMatrixCal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), initConstValueParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dstLocal, VALUE_512, "dstLocal", "InitConstValue when TPosition is B2");
        InitL0BNzMatrixCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), initConstValueParams);
    } else if (dstScope == Hardware::L1) {
        CheckTensorAlign<T>(dstLocal, ONE_BLK_SIZE, "dstLocal", "InitConstValue when TPosition is A1 / B1");
        InitL1BufferCal((__attribute__((cce_cube_buff)) T*)dstLocal.GetPhyAddr(), initConstValueParams);
    } else {

                                                                            ;
    }
}
# 691 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void SetFmatrixImpl(uint16_t l1H, uint16_t l1W, const uint8_t padList[4],
    const FmatrixMode &fmatrixMode)
{
    if (fmatrixMode == FmatrixMode::FMATRIX_LEFT) {
        Load3DSetFMatrixCal(l1H, l1W, padList);
    } else if (fmatrixMode == FmatrixMode::FMATRIX_RIGHT) {
        Load3DSetFMatrixBCal(l1H, l1W, padList);
    }
}
# 709 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundaryImpl(uint32_t boundaryValue)
{
    SetLoadDataBoundaryCal(boundaryValue);
}




[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeatImpl(const LoadDataRepeatParam& repeatParams)
{
    SetLoadDataRepeatCal(repeatParams);
}
# 731 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzipImpl(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal)
{
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
# 746 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    if (dstScope == Hardware::L1) {
        LoadDataUnzipToL1Cal((__attribute__((cce_cube_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcGlobal.GetPhyAddr());
    } else if (dstScope == Hardware::L0A) {
        LoadDataUnzipToL0ACal((__attribute__((cce_cube_a)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcGlobal.GetPhyAddr());
    } else if (dstScope == Hardware::L0B) {
        LoadDataUnzipToL0BCal((__attribute__((cce_cube_b)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcGlobal.GetPhyAddr());
    } else {

                                                       ;
    }
}


template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData2dDatatype()
{
# 770 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
                                                   ;





}


[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData3dParams(const uint16_t srcHeight, const uint16_t srcWeight,
    const uint8_t srcWStride, const uint8_t srcHStride)
{
                                                                                                               ;
                                                                                                               ;

                                         ;

                                         ;
}

template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) bool ChannelSizeRemainder(const uint16_t channelSize, uint16_t remainder[], uint16_t size)
{
    uint16_t oneBlkNum = ONE_BLK_SIZE / sizeof(T);
    if constexpr (IsSameType<T, int4b_t>::value) {
        oneBlkNum = 64;
    }
    for (uint16_t i = 0; i < size; i++) {
        if (channelSize % oneBlkNum == remainder[i]) {
            return true;
        }
    }
    return false;
}


template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData3dv2ChannelSize(const uint16_t channelSize)
{
# 837 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_mm_base_impl.h"
    if constexpr (SupportType<T, half, bfloat16_t>()) {
        uint16_t remainderList[] = {0, 4, 8};


                                                                                    ;
    }

    if constexpr (SupportType<T, float, int32_t, uint32_t>()) {
        uint16_t remainderList[] = {0, 4};


                                                                                         ;
    } else if constexpr (SupportType<T, int8_t, uint8_t>()) {
        uint16_t remainderList[] = {0, 4, 8, 16};


                                                                                         ;
    } else if constexpr (IsSameType<T, int4b_t>::value) {
        uint16_t remainderList[] = {0, 8, 16, 32};


                                                                                      ;
    }

}


template <typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckLoadData3dv2MatrixParams(const uint16_t kExtension, const uint16_t mExtension,
    const uint16_t kStartPt, const uint16_t mStartPt) {
    constexpr uint16_t base16 = 16;
    if constexpr (SupportType<T, half, int8_t, int4b_t>()) {


                                                 ;
    }
    uint16_t kExtBase = (SupportType<T, int4b_t>()) ? 64 : ONE_BLK_SIZE / sizeof(T);
    if constexpr (SupportType<T, half, int8_t, int4b_t, int32_t, uint32_t, float>()) {


                                                                                      ;


                                                                                    ;
    }







                                                                                                      ;

}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h" 2

namespace AscendC {
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams);
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams);
# 104 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV1<T>& loadDataParams);
# 135 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2<T>& loadDataParams);
# 166 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams);



template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<bfloat16_t>& dstLocal, const LocalTensor<bfloat16_t>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams);
# 192 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParams& loadDataParams);
# 209 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParamsV2& loadDataParams);
# 233 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const MmadParams& mmadParams);

template <typename DstT, typename Src0T, typename Src1T, typename BiasT>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const LocalTensor<BiasT>& biasLocal, const MmadParams& mmadParams);


[aicore] __inline__ __attribute__((always_inline)) void MmadWithSparse(const LocalTensor<int32_t>& dstLocal, const LocalTensor<int8_t>& fmLocal,
    const LocalTensor<int8_t>& filterLocal, const MmadParams& mmadParams);

[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparse(const LocalTensor<int8_t> &dstLocal, const LocalTensor<int8_t> &srcLocal,
    const LocalTensor<uint8_t> &idxLocal, const LoadData2dParams &loadDataParam);
# 257 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void BroadCastVecToMM(const LocalTensor<T> &dstLocal,
    const LocalTensor<U> &srcLocal, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap);
# 274 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConstValue(const LocalTensor<T> &dstLocal,
    const InitConstValueParams<T> &initConstValueParams);
# 285 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataPaddingValue(const T padValue);
# 298 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void SetFmatrix(uint16_t l1H, uint16_t l1W,
    const uint8_t padList[4], const FmatrixMode &fmatrixMode);
# 309 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundary(uint32_t boundaryValue);

[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeat(const LoadDataRepeatParam& repeatParams);
# 330 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadImageToLocal(const LocalTensor<T>& dstLocal, const LoadImageToLocalParams& loadDataParams);
# 342 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_mm_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzip(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal);
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h"
namespace AscendC {
# 133 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_gemm_base_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void CalculateGemmTiling(GemmTiling& tilling)
{
    tilling.mIterNum = 1;
    tilling.nIterNum = 1;
    tilling.kIterNum = DivCeil(tilling.kBlockNum, tilling.kTileBlock);

    tilling.mTileBlock = DivCeil(tilling.mBlockNum, tilling.mIterNum);
    tilling.nTileBlock = DivCeil(tilling.nBlockNum, tilling.nIterNum);

    tilling.kTailBlock = tilling.kBlockNum - (tilling.kIterNum - 1) * tilling.kTileBlock;
    tilling.mTailBlock = tilling.mBlockNum - (tilling.mIterNum - 1) * tilling.mTileBlock;
    tilling.nTailBlock = tilling.nBlockNum - (tilling.nIterNum - 1) * tilling.nTileBlock;

    tilling.kHasTail = tilling.kTailBlock != tilling.kTileBlock;
    tilling.kHasTailEle = tilling.roundK != tilling.kNum;
    tilling.kTailEle = tilling.kNum % (tilling.kTileBlock * tilling.c0Size);

    if (tilling.mNum != tilling.mTileBlock * tilling.blockSize) {
        tilling.mHasTail = true;
    } else {
        tilling.mHasTail = false;
    }
    tilling.nHasTail = tilling.nTileBlock != tilling.nTailBlock;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0B(uint32_t kBlocks, uint32_t nBlocks, GemmTiling tilling, uint32_t i, uint32_t j,
    const LocalTensor<T>& src1Local, const LocalTensor<T>& L0b)
{
    if (tilling.nIterNum == 1) {
        uint32_t wSize = tilling.blockSize * tilling.c0Size;
        uint32_t wIdx = (i * tilling.kTileBlock * tilling.nBlockNum + j * tilling.nTileBlock) * wSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = kBlocks * nBlocks;
        params.srcStride = 1;
        LoadDataImpl(L0b, src1Local[wIdx], params);
    } else {

        for (size_t index = 0; index < kBlocks; ++index) {
            uint32_t wSize = j * tilling.nTileBlock * tilling.blockSize * tilling.c0Size;
            uint32_t wIdx =
                (i * tilling.kTileBlock + index) * tilling.nBlockNum * tilling.blockSize * tilling.c0Size + wSize;
            uint32_t l0bIdx = index * nBlocks * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = nBlocks;
            params.srcStride = 1;
            LoadDataImpl(L0b[l0bIdx], src1Local[wIdx], params);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0A(uint32_t kBlocks, uint32_t mBlocks, GemmTiling tilling, uint32_t i, uint32_t t,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{
    if (kBlocks == 1) {
        uint32_t l1aSize = i * tilling.kTileBlock * tilling.mBlockNum * tilling.blockSize * tilling.c0Size;
        uint32_t l1aOffset = t * tilling.mTileBlock * tilling.blockSize * tilling.c0Size + l1aSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = mBlocks;
        params.srcStride = 1;
        LoadDataImpl(L0a, src0Local[l1aOffset], params);
    } else {

        for (size_t index = 0; index < mBlocks; index++) {
            uint32_t l0aOffset = index * kBlocks * tilling.blockSize * tilling.c0Size;
            uint32_t l1aOffset = (t * tilling.mTileBlock + index) * tilling.blockSize * tilling.c0Size +
                i * tilling.kTileBlock * tilling.mBlockNum * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = kBlocks;
            params.srcStride = tilling.mBlockNum;
            LoadDataImpl(L0a[l0aOffset], src0Local[l1aOffset], params);
        }
    }
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void MmadFunc(const LocalTensor<src0_T>& L0a, const LocalTensor<src1_T>& L0b,
    const LocalTensor<dst_T>& L0c, int32_t initValue, GemmTiling tilling, size_t i)
{
    MmadParams mmadParams;
    mmadParams.m = tilling.mTileBlock * tilling.blockSize;
    mmadParams.n = tilling.nTileBlock * tilling.blockSize;
    mmadParams.isBias = 1;

    if (tilling.kIterNum == 1) {
        mmadParams.k = tilling.kNum;
        mmadParams.isBias = initValue;
    } else if (initValue == 1 && tilling.kHasTailEle) {
        if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailEle;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else if (initValue != 1 && tilling.kHasTailEle) {
        if (i == 0) {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
            mmadParams.isBias = 0;
        } else if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailEle;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else if (initValue == 1 && !tilling.kHasTailEle) {
        if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailBlock * tilling.c0Size;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else {
        if (i == 0) {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
            mmadParams.isBias = 0;
        } else if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailBlock * tilling.c0Size;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    }
    MmadImpl(L0c, L0a, L0b, mmadParams);
}

template <typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GetPingPongBuffer(LocalTensor<src0_T>& L0aPing, LocalTensor<src0_T>& L0aPong,
    LocalTensor<src1_T>& L0bPing, LocalTensor<src1_T>& L0bPong)
{

    TBuffAddr tbufaPing;
    tbufaPing.logicPos = (uint8_t)QuePosition::A2;
    L0aPing.SetAddr(tbufaPing);
    L0aPing.InitBuffer(0, TOTAL_L0A_SIZE / 2 / sizeof(src0_T));

    TBuffAddr tbufaPong;
    tbufaPong.logicPos = (uint8_t)QuePosition::A2;
    L0aPong.SetAddr(tbufaPong);
    L0aPong.InitBuffer(TOTAL_L0A_SIZE / 2, TOTAL_L0A_SIZE / 2 / sizeof(src0_T));


    TBuffAddr tbufbPing;
    tbufbPing.logicPos = (uint8_t)QuePosition::B2;
    L0bPing.SetAddr(tbufbPing);
    L0bPing.InitBuffer(0, TOTAL_L0B_SIZE / 2 / sizeof(src1_T));

    TBuffAddr tbufbPong;
    tbufbPong.logicPos = (uint8_t)QuePosition::B2;
    L0bPong.SetAddr(tbufbPong);
    L0bPong.InitBuffer(TOTAL_L0B_SIZE / 2, TOTAL_L0B_SIZE / 2 / sizeof(src1_T));
    return;
}

template <typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GetSingleThreadBuffer(LocalTensor<src0_T>& L0a, LocalTensor<src1_T>& L0b)
{

    TBuffAddr tbufa;
    tbufa.logicPos = (uint8_t)QuePosition::A2;
    L0a.SetAddr(tbufa);
    L0a.InitBuffer(0, TOTAL_L0A_SIZE / sizeof(src0_T));


    TBuffAddr tbufb;
    tbufb.logicPos = (uint8_t)QuePosition::B2;
    L0b.SetAddr(tbufb);
    L0b.InitBuffer(0, TOTAL_L0B_SIZE / sizeof(src1_T));
    return;
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecNmNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    LocalTensor<src0_T> L0a;
    LocalTensor<src1_T> L0b;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

            LoadL0B(kBlocks, tilling.nTileBlock, tilling, indexK, indexN, src1Local, L0b);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, indexK, indexM, src0Local, L0a);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFunc(L0a, L0b, L0c, initValue, tilling, indexK);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecNmPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t ping = 1;
    LocalTensor<src0_T> L0aPing;
    LocalTensor<src0_T> L0aPong;
    LocalTensor<src1_T> L0bPing;
    LocalTensor<src1_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t i = 0; i < tilling.kIterNum; i++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (i == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPing);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPing);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPing, L0bPing, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPong);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPong, L0bPong, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }


    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);




}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecNm(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t needL0Asize = tilling.roundM * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Bsize = tilling.roundN * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        GemmExecNmNopingpong(L0c, src0Local, src1Local, tilling, initValue);
        return;
    }
    GemmExecNmPingPong(L0c, src0Local, src1Local, tilling, initValue);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecMnNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    LocalTensor<src1_T> L0b;
    LocalTensor<src0_T> L0a;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

            LoadL0A(kBlocks, tilling.mTileBlock, tilling, indexK, indexM, src0Local, L0a);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, indexK, indexN, src1Local, L0b);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFunc(L0a, L0b, L0c, initValue, tilling, indexK);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecMnPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t ping = 1;
    LocalTensor<src0_T> L0aPing;
    LocalTensor<src0_T> L0aPong;
    LocalTensor<src1_T> L0bPing;
    LocalTensor<src1_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t i = 0; i < tilling.kIterNum; i++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (i == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPing);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPing);

                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPing, L0bPing, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0Local, L0aPong);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1Local, L0bPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(L0aPong, L0bPong, L0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) void GemmExecMn(const LocalTensor<dst_T>& L0c, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, GemmTiling tilling, const int32_t initValue)
{
    uint32_t needL0Bsize = tilling.roundN * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Asize = tilling.roundM * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        GemmExecMnNopingpong(L0c, src0Local, src1Local, tilling, initValue);
        return;
    }
    GemmExecMnPingPong(L0c, src0Local, src1Local, tilling, initValue);
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h" 2

namespace AscendC {

template <typename T> [aicore] __inline__ __attribute__((always_inline)) GemmTiling GetGemmTiling(uint32_t m, uint32_t k, uint32_t n);
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_gemm_intf.h"
template <typename dst_T, typename src0_T, typename src1_T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Gemm(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, const uint32_t m, const uint32_t k, const uint32_t n, GemmTiling tilling,
    bool partialsum = true, int32_t initValue = 0);
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_v2_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_v2_impl.h"
namespace AscendC {



const uint32_t L0C_SRC_ALIGN = 16 * sizeof(float);

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CheckCommonFixpipeParam(__attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &params)
{
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_fixpipe_v2_impl.h"
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CheckFixpipeL0C2L1Param(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &params)
{
    CheckCommonFixpipeParam<DstT, SrcT, config>(src, params);
                                                                                                                   ;

                                                                              ;

                                                                             ;





                                     ;
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void CheckFixpipeL0C2GMParam(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &params)
{
    CheckCommonFixpipeParam<DstT, SrcT, config>(src, params);





                                                                                                                ;
    if constexpr(IsSameType<SrcT, float>::value && IsSameType<DstT, float>::value) {

                                                                                               ;
    } else if constexpr(IsSameType<SrcT, int32_t>::value && IsSameType<DstT, int32_t>::value) {

                                                                                                   ;
    }
    if (params.isChannelSplit) {

                                                                                                                      ;

                                                                           ;
    }
}


struct FixpipeTilingV220 {
    uint16_t nIterNum = 0;
    uint16_t nSize = 0;
    bool isDb = false;
    uint16_t tailNSize = 0;
};


[aicore] __inline__ __attribute__((always_inline)) FixpipeTilingV220 GenFixpipeTilingV220(uint16_t n)
{
    FixpipeTilingV220 tiling;

    uint16_t maxDeqNums = 256;
    if (n <= maxDeqNums) {
        tiling.nIterNum = 1;
        tiling.nSize = n;
        tiling.isDb = false;
        tiling.tailNSize = 0;
    } else {
        tiling.isDb = true;
        uint16_t dbMaxDeqNums = maxDeqNums / 2;
        tiling.nIterNum = n / dbMaxDeqNums;
        tiling.nSize = dbMaxDeqNums;
        tiling.tailNSize = n % dbMaxDeqNums;
    }
    return tiling;
}

[aicore] __inline__ __attribute__((always_inline)) void CopyDeqTensorToFbuf(
    __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t deqDataSize = DivCeil(calNSize * sizeof(uint64_t), 128) * 128;
    __attribute__((cce_fixpipe_buff)) uint64_t *deqTensorTempBuf =
        AscendCUtils::GetTemporaryFbBufferAddr<uint64_t>(0, deqDataSize / sizeof(uint64_t));
    uint32_t deqValueOffset = nIterIndex * fixpipeTiling.nSize;

    uint16_t fbufBurstLen = deqDataSize / 128;
    copy_cbuf_to_fbuf(deqTensorTempBuf, cbufWorkspace + deqValueOffset, 1, fbufBurstLen, 0, 0);

    uint64_t deqTensorAddr = (((uint64_t)deqTensorTempBuf) >> (uint64_t)7) << 8;
    set_fpc(deqTensorAddr);
    AscendCUtils::FreeTemporaryFbBuffer<uint64_t>(deqTensorTempBuf);
}

template <typename T, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, const FixpipeParamsV220 &intriParams)
{


                                    ;
}

template <typename T, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(
    __attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{


                                    ;
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2UBImpl(__attribute__((cce_unif_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams)
{
                                                                                        ;
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2UBImpl(
    __attribute__((cce_unif_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
                                                                                        ;
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2L1Param<DstT, SrcT, config>(dst, src, intriParams);





    if (intriParams.quantPre == QuantMode_t::DEQF16 || intriParams.quantPre == QuantMode_t::QF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::REQ8) {

        SetQuantPreImpl(intriParams.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();
    FixpipeTilingV220 fixpipeTiling;

    FixpipeL0cToL1<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, intriParams.nSize);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1Impl(
    __attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2L1Param<DstT, SrcT, config>(dst, src, intriParams);






    FixpipeTilingV220 fixpipeTiling = GenFixpipeTilingV220(intriParams.nSize);
    if (intriParams.quantPre == QuantMode_t::VDEQF16 || intriParams.quantPre == QuantMode_t::VQF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::VREQ8) {
        for (uint16_t i = 0; i < fixpipeTiling.nIterNum; ++i) {
            FixpipeL0C2L1ImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.nSize, i);
        }

        if (fixpipeTiling.tailNSize > 0) {
            FixpipeL0C2L1ImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.tailNSize, fixpipeTiling.nIterNum);
        }
        return;
    }
}

template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImpl(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2GMParam<DstT, SrcT, config>(dst, src, intriParams);
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        uint64_t ndPara = static_cast<uint64_t>(intriParams.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(intriParams.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(intriParams.ndNum);
        SetNdParaImpl(ndPara);
    }
    FixpipeTilingV220 fixpipeTiling;





    if (intriParams.quantPre == QuantMode_t::DEQF16 || intriParams.quantPre == QuantMode_t::QF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::REQ8) {
        SetQuantPreImpl(intriParams.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, intriParams.nSize);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImpl(
    __attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2GMParam<DstT, SrcT, config>(dst, src, intriParams);
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        uint64_t ndPara = static_cast<uint64_t>(intriParams.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(intriParams.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(intriParams.ndNum);
        SetNdParaImpl(ndPara);
    }






    FixpipeTilingV220 fixpipeTiling = GenFixpipeTilingV220(intriParams.nSize);
    if (intriParams.quantPre == QuantMode_t::VDEQF16 || intriParams.quantPre == QuantMode_t::VQF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::VREQ8) {
        for (uint16_t i = 0; i < fixpipeTiling.nIterNum; ++i) {
            FixpipeL0C2GMImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.nSize, i);
        }

        if (fixpipeTiling.tailNSize > 0) {
            FixpipeL0C2GMImplN<DstT, SrcT, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.tailNSize, fixpipeTiling.nIterNum);
        }
        return;
    }
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2L1ImplN(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace,
    const FixpipeParamsV220 &intriParams, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize,
    uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(cbufWorkspace, fixpipeTiling, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, calNSize, nIterIndex);
}

template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0C2GMImplN(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace,
    const FixpipeParamsV220 &intriParams, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize,
    uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(cbufWorkspace, fixpipeTiling, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut<DstT, SrcT, config>(dst, src, intriParams, fixpipeTiling, calNSize, nIterIndex);
}



template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToL1(__attribute__((cce_cube_buff)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams,
    const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    uint16_t cburstNum = fixpipeTiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * intriParams.srcStride * BLOCK_CUBE;
    uint32_t dstOffset = cburstNum * nIterIndex * intriParams.dstStride * 32 / sizeof(DstT);


    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322F16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322BF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::DEQF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VDEQF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::QF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VQF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::REQ8, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VREQ8, static_cast<uint8_t>(intriParams.reluEn), false, false);
            default:
                                                                                            ;
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint64_t GetGMLength(
    const FixpipeParamsV220 &intriParams, const uint16_t &calNSize, const uint16_t &dstEleSize, const bool &nz2ndEn)
{
    constexpr uint16_t dstStrideUnit = 32;
    constexpr uint16_t fractalNsize = 16;
    uint64_t cburstNum = calNSize / fractalNsize;
    uint64_t gmLen =
        (cburstNum - 1) * intriParams.dstStride * dstStrideUnit + intriParams.mSize * fractalNsize * dstEleSize;
    if (nz2ndEn) {
        gmLen = (static_cast<uint64_t>(intriParams.ndNum) - 1) * dstEleSize * intriParams.dstNdStride +
                (intriParams.mSize - 1) * intriParams.dstStride * dstEleSize + cburstNum * fractalNsize * dstEleSize;
    }
    return gmLen;
}



template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void FixpipeL0cToOut(__attribute__((cce_global)) DstT *dst, __attribute__((cce_cube_c)) SrcT *src, const FixpipeParamsV220 &intriParams,
    const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    uint16_t cburstNum = fixpipeTiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * intriParams.srcStride * BLOCK_CUBE;
    uint32_t dstOffset = 0;
    bool nz2ndEn = false;
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        dstOffset = nIterIndex * fixpipeTiling.nSize;
        nz2ndEn = true;
    } else {
        dstOffset = cburstNum * nIterIndex * intriParams.dstStride * 32 / sizeof(DstT);
    }

    if constexpr (g_gm_overflow_check) {
        uint64_t gmLen = GetGMLength(intriParams, calNSize, sizeof(DstT), nz2ndEn);
        AscendCUtils::CheckGmMemOverflow((__attribute__((cce_global)) DstT *)(dst + dstOffset), false, gmLen);
    }
    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::NoQuant:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::NoQuant, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322F16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322BF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::DEQF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VDEQF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::QF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VQF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::REQ8, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) DstT*)(dst + dstOffset), (__attribute__((cce_cube_c)) SrcT*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VREQ8, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            default:
                                                                                            ;
        }
    }
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h" 2







namespace AscendC {
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_fixpipe_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag = false);

template <typename T, bool setRelu = false>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &preTensor, bool isUnitFlag = false);

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipeNz2ndFlag(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride);

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipePreQuantFlag(uint64_t config);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParamsV220& intriParams);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<uint64_t>& cbufWorkspace, const FixpipeParamsV220& intriParams);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT>& dstGlobal, const LocalTensor<SrcT>& srcLocal,
    const FixpipeParamsV220& intriParams);


template <typename DstT, typename SrcT, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT>& dstGlobal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<uint64_t>& cbufWorkspace, const FixpipeParamsV220& intriParams);
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_conv2d_base_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_conv2d_base_impl.h"
namespace AscendC {
# 157 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_operator_conv2d_base_impl.h"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GetTypeforC0(Conv2dParams& conv2dParams, Conv2dTilling& tilling)
{
    if (IsSameType<T, int8_t>::value) {
        tilling.c0Size = 32;
        tilling.dTypeSize = 1;
    } else if (IsSameType<T, half>::value) {
        tilling.c0Size = 16;
        tilling.dTypeSize = 2;
    } else {
        tilling.c0Size = 0;
        tilling.dTypeSize = 0;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CalculateConv2dTiling(Conv2dTilling& tilling)
{
    tilling.mBlockNum = DivCeil(tilling.mNum, tilling.blockSize);
    tilling.nBlockNum = DivCeil(tilling.nNum, tilling.blockSize);
    tilling.kBlockNum = DivCeil(tilling.kNum, tilling.c0Size);

    tilling.roundM = DivCeil(tilling.mNum, tilling.blockSize) * tilling.blockSize;
    tilling.roundN = DivCeil(tilling.nNum, tilling.blockSize) * tilling.blockSize;
    tilling.roundK = DivCeil(tilling.kNum, tilling.c0Size) * tilling.c0Size;

    uint32_t k0a = TOTAL_L0A_SIZE / 2 / (tilling.roundM * tilling.dTypeSize);
    uint32_t k0b = TOTAL_L0B_SIZE / 2 / (tilling.roundN * tilling.dTypeSize);
    uint32_t k0 = k0a > k0b ? k0b : k0a;
    k0 = k0 > tilling.kNum ? tilling.kNum : k0;

    tilling.kTileBlock = k0 / tilling.c0Size;
    if (tilling.kTileBlock == 0) {
        tilling.kTileBlock = 1;
    }

    tilling.mIterNum = 1;
    tilling.nIterNum = 1;
    tilling.kIterNum = DivCeil(tilling.kBlockNum, tilling.kTileBlock);

    tilling.mTileBlock = DivCeil(tilling.mBlockNum, tilling.mIterNum);
    tilling.nTileBlock = DivCeil(tilling.nBlockNum, tilling.nIterNum);

    tilling.mTileNums = tilling.mTileBlock * tilling.blockSize;

    tilling.mHasTail = (tilling.howo != tilling.mIterNum * tilling.mTileBlock * tilling.blockSize) ? true : false;
    tilling.kHasTail = (tilling.kBlockNum < tilling.kIterNum * tilling.kTileBlock) ? true : false;
    tilling.nHasTail = (tilling.nBlockNum < tilling.nIterNum * tilling.nTileBlock) ? true : false;

    tilling.mTailBlock = tilling.mBlockNum - (tilling.mIterNum - 1) * tilling.mTileBlock;
    tilling.mTailNums = tilling.howo - (tilling.mIterNum - 1) * tilling.mTileBlock * tilling.blockSize;

    tilling.kTailBlock = tilling.kBlockNum - (tilling.kIterNum - 1) * tilling.kTileBlock;
    tilling.nTailBlock = tilling.nBlockNum - (tilling.nIterNum - 1) * tilling.nTileBlock;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0AForConv2DV1(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{
    uint32_t cinPos = indexK * tilling.kTileBlock;

    for (size_t index = 0; index < tilling.mTileBlock; index++) {
        uint32_t hoWoPos = (indexM * tilling.mTileBlock + index) * tilling.blockSize;
        uint32_t hoIdx = hoWoPos / tilling.wo;
        uint32_t woIdx = hoWoPos % tilling.wo;
        uint32_t hiIdx = hoIdx * tilling.strideH;
        uint32_t wiIdx = woIdx * tilling.strideW;

        uint32_t c1Idx = cinPos / (tilling.height * tilling.width);
        uint32_t kHwIdx = cinPos % (tilling.height * tilling.width);
        uint32_t l0aIdx = index * kBlocks * tilling.blockSize * tilling.c0Size;
        uint32_t disableC1 = 0;
        uint32_t c1Offset = c1Idx * tilling.c0Size * tilling.hi * tilling.wi;

        LoadData3DParamsV1<T> params;

        for (size_t i = 0; i < PAD_SIZE; i++) {
            params.padList[i] = conv2dParams.padList[i];
        }

        params.l1H = tilling.hi;
        params.l1W = tilling.wi;
        params.c1Index = disableC1;
        params.fetchFilterW = kHwIdx % tilling.width;
        params.fetchFilterH = kHwIdx / tilling.width;
        params.leftTopW = wiIdx - params.padList[0];
        params.leftTopH = hiIdx - params.padList[2];
        params.strideW = tilling.strideW;
        params.strideH = tilling.strideH;
        params.filterW = tilling.width;
        params.filterH = tilling.height;
        params.dilationFilterW = tilling.dilationW;
        params.dilationFilterH = tilling.dilationH;
        params.jumpStride = 1;
        params.repeatMode = 0;
        params.repeatTime = kBlocks;
        params.cSize = 0;
        params.padValue = 0;

        LoadDataImpl(L0a[l0aIdx], src0Local[c1Offset], params);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0AForConv2DV2(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{


    uint32_t kStartPt = indexK * kBlocks * tilling.c0Size;
    uint32_t mStartPt = indexM * mBlocks;
    uint32_t channelSize = conv2dParams.cin;

    LoadData3DParamsV2<T> params;

    for (size_t i = 0; i < PAD_SIZE; i++) {
        params.padList[i] = conv2dParams.padList[i];
    }

    params.l1H = tilling.hi;
    params.l1W = tilling.wi;
    params.channelSize = channelSize;
    params.kExtension = kBlocks * tilling.c0Size;
    params.mExtension = mBlocks;
    params.kStartPt = kStartPt;
    params.mStartPt = mStartPt;
    params.strideW = tilling.strideW;
    params.strideH = tilling.strideH;
    params.filterW = tilling.width;
    params.filterH = tilling.height;
    params.dilationFilterW = tilling.dilationW;
    params.dilationFilterH = tilling.dilationH;
    params.enTranspose = false;
    params.enSmallK = false;
    params.padValue = 0;
    params.filterSizeW = false;
    params.filterSizeH = false;
    params.fMatrixCtrl = false;

    LoadDataImpl(L0a, src0Local, params);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0AForConv2D(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0Local, const LocalTensor<T>& L0a)
{



    LoadL0AForConv2DV2(kBlocks, indexK, mBlocks, indexM, conv2dParams, tilling, src0Local, L0a);

}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadL0BForConv2D(uint32_t kBlocks, uint32_t nBlocks, uint32_t indexK, uint32_t indexN,
    Conv2dTilling& tilling, const LocalTensor<T>& src1Local, const LocalTensor<T>& L0b)
{
    if (tilling.nIterNum == 1) {

        uint32_t wSize = tilling.blockSize * tilling.c0Size;
        uint32_t wIdx = (indexK * tilling.kTileBlock * tilling.nBlockNum + indexN * tilling.nTileBlock) * wSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = kBlocks * nBlocks;
        params.srcStride = 1;
        LoadDataImpl(L0b, src1Local[wIdx], params);
    } else {

        for (size_t index = 0; index < kBlocks; index++) {
            uint32_t wSize = indexN * tilling.nTileBlock * tilling.blockSize * tilling.c0Size;
            uint32_t wIdx =
                (indexK * tilling.kTileBlock + index) * tilling.nBlockNum * tilling.blockSize * tilling.c0Size + wSize;
            uint32_t l0bIdx = index * nBlocks * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = nBlocks;
            params.srcStride = 1;
            LoadDataImpl(L0b[l0bIdx], src1Local[wIdx], params);
        }
    }
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void MmadFuncForConv2D(const LocalTensor<src_T>& L0a, const LocalTensor<src_T>& L0b,
    const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias, Conv2dParams& conv2dParams, Conv2dTilling tilling,
    uint32_t kBlocks, uint32_t mBlocks, uint32_t nBlocks, uint32_t indexK, uint32_t indexM, uint32_t indexN)
{

    uint32_t bSize = tilling.blockSize * tilling.blockSize;
    uint32_t dstFlattenIdx = (indexN * tilling.mBlockNum * tilling.nTileBlock + indexM * tilling.mTileBlock) * bSize;
    uint32_t hwActualSize = mBlocks;




    if (hwActualSize == 1) {
        hwActualSize = 2;
    }

    MmadParams mmadParams;

    mmadParams.m = hwActualSize;
    mmadParams.k = kBlocks * tilling.c0Size;
    mmadParams.n = nBlocks * tilling.blockSize;
    mmadParams.isBias = 1;

    if ((indexK == 0) && (conv2dParams.initY == 0)) {
        mmadParams.isBias = 0;
    }

    if ((indexK == 0) && (conv2dParams.initY == 2)) {
        mmadParams.isBias = 0;
        uint32_t biasOffset = nBlocks * indexN * 16;

        uint32_t burstLenUnit = 64;
        uint32_t extent = sizeof(dst_T) * nBlocks * 16;
        uint32_t burstLen = extent / burstLenUnit;
        BroadCastVecToMM(L0c[dstFlattenIdx], bias[biasOffset], 1, burstLen, 0, 0);
        event_t eventIdVToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_M));
        SetFlag<HardEvent::V_M>(eventIdVToM);
        WaitFlag<HardEvent::V_M>(eventIdVToM);
    }

    MmadImpl(L0c[dstFlattenIdx], L0a, L0b, mmadParams);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecNmNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    LocalTensor<src_T> L0b;
    LocalTensor<src_T> L0a;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

            LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0b);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0a);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFuncForConv2D(L0a, L0b, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                    tilling.nTileBlock, indexK, indexM, indexN);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecNmPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t ping = 1;
    LocalTensor<src_T> L0aPing;
    LocalTensor<src_T> L0bPing;
    LocalTensor<src_T> L0aPong;
    LocalTensor<src_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());

    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPing);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local,
                        L0aPing);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFuncForConv2D(L0aPing, L0bPing, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPong);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local,
                        L0aPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFuncForConv2D(L0aPong, L0bPong, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecNm(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t needL0Asize = tilling.roundM * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Bsize = tilling.roundN * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        Conv2DExecNmNopingpong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
        return;
    }
    Conv2DExecNmPingPong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecMnNopingpong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    LocalTensor<src_T> L0a;
    LocalTensor<src_T> L0b;
    GetSingleThreadBuffer(L0a, L0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

            LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0a);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0b);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFuncForConv2D(L0a, L0b, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                    tilling.nTileBlock, indexK, indexM, indexN);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecMnPingPong(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t ping = 1;
    LocalTensor<src_T> L0aPing;
    LocalTensor<src_T> L0aPong;
    LocalTensor<src_T> L0bPing;
    LocalTensor<src_T> L0bPong;
    GetPingPongBuffer(L0aPing, L0aPong, L0bPing, L0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0aPing);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPing);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFuncForConv2D(L0aPing, L0bPing, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0Local, L0aPong);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1Local, L0bPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFuncForConv2D(L0aPong, L0bPong, L0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void Conv2DExecMn(const LocalTensor<dst_T>& L0c, const LocalTensor<dst_T>& bias,
    const LocalTensor<src_T>& src0Local, const LocalTensor<src_T>& src1Local, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t needL0Bsize = tilling.roundN * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Asize = tilling.roundM * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        Conv2DExecMnNopingpong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
        return;
    }
    Conv2DExecMnPingPong(L0c, bias, src0Local, src1Local, conv2dParams, tilling);
}

}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h" 2


namespace AscendC {

template <typename T> [aicore] __inline__ __attribute__((always_inline)) Conv2dTilling GetConv2dTiling(Conv2dParams& conv2dParams);
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_conv2d_intf.h"
template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))
    __attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal, const LocalTensor<src_T> &featureMap,
    const LocalTensor<src_T> &weight, Conv2dParams &conv2dParams, Conv2dTilling &tilling);

template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))__attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal,
    const LocalTensor<dst_T> &bias, const LocalTensor<src_T> &featureMap, const LocalTensor<src_T> &weight,
    Conv2dParams &conv2dParams, Conv2dTilling &tilling);
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 1
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sync_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sync_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void ClcSyncCount(__attribute__((cce_global)) int32_t* localSyncGM, __attribute__((cce_unif_buff)) int32_t* ubWorkspaceAddr,
    const int32_t blockIdx, const int32_t totalBlocks, bool isFirst, int32_t& count)
{
    if (isFirst) {
        __attribute__((cce_unif_buff)) int32_t* localUbAddr = ubWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);
        *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) = 1;
        event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
        copy_ubuf_to_gm(static_cast<__attribute__((cce_global)) void*>(localSyncGM), static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), 0, 1, 1, 0,
            0);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        count = 1;
        for (int32_t i = 0; i < totalBlocks; i++) {
            if (i != blockIdx) {
                count += *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(ubWorkspaceAddr) + i * ONE_BLK_FLOAT_NUM);
            }
        }
    } else {
        for (int32_t i = 0; i < totalBlocks; i++) {
            count += *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(ubWorkspaceAddr) + i * ONE_BLK_FLOAT_NUM);
        }
    }
}

template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void SoftSyncAllImpl(__attribute__((cce_global)) int32_t* gmWorkspaceAddr, __attribute__((cce_unif_buff)) int32_t* ubWorkspaceAddr,
    const int usedCores)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    __sync_all_stub(usedCores, isAIVOnly);
    PipeBarrier<PIPE_ALL>();

    int32_t totalBlocks = isAIVOnly ? get_block_num() : (GetTaskRationImpl() * get_block_num());
    totalBlocks = usedCores != 0 ? usedCores : totalBlocks;
    int32_t blockIdx = isAIVOnly ? get_block_idx() : GetBlockIdxImpl();

    __attribute__((cce_global)) int32_t* localSyncGM = gmWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);
    __attribute__((cce_unif_buff)) int32_t* localUbAddr = ubWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);

    copy_gm_to_ubuf(static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), static_cast<__attribute__((cce_global)) void*>(localSyncGM), 0, 1, 1, 0, 0);
    event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
    SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
    WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
    int32_t curValue = *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) + 1;
    bool isFirst = curValue == 1 ? true : false;
    *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) = curValue;
    event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
    copy_ubuf_to_gm(static_cast<__attribute__((cce_global)) void*>(localSyncGM), static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), 0, 1, 1, 0, 0);
    event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
    SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    int32_t totalBlockCount = ONE_BLK_FLOAT_NUM * totalBlocks;
    uint16_t blockLen = totalBlockCount / AscendCUtils::GetC0Count(sizeof(int32_t));
    while (true) {
        copy_gm_to_ubuf(static_cast<__attribute__((cce_unif_buff)) void*>(ubWorkspaceAddr), static_cast<__attribute__((cce_global)) void*>(gmWorkspaceAddr), 0, 1,
            blockLen, 0, 0);
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        int32_t count = 0;
        ClcSyncCount(localSyncGM, ubWorkspaceAddr, blockIdx, totalBlocks, isFirst, count);
        event_t eventIdSToMte2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE2));
        SetFlag<HardEvent::S_MTE2>(eventIdSToMte2);
        WaitFlag<HardEvent::S_MTE2>(eventIdSToMte2);
        if (count >= (totalBlocks * curValue)) {
            break;
        }
    }
    __sync_all_stub(usedCores, isAIVOnly);
}

constexpr uint16_t SYNC_AIV_FLAG = 12;
constexpr uint16_t SYNC_AIC_FLAG = 11;
constexpr uint16_t SYNC_AIC_AIV_FLAG = 13;
constexpr uint16_t SYNC_AIV_ONLY_ALL = 14;
constexpr uint16_t SYNC_MODE_SHIFT_VALUE = 4;
constexpr uint16_t SYNC_FLAG_SHIFT_VALUE = 8;

[aicore] __inline__ __attribute__((always_inline)) uint16_t GetffstMsg(uint16_t mode, uint16_t flagId)
{
    return (0x1 + ((mode & 0x3) << SYNC_MODE_SHIFT_VALUE) + ((flagId & 0xf) << SYNC_FLAG_SHIFT_VALUE));
}

template <bool isAIVOnly = true> [aicore] __inline__ __attribute__((always_inline)) void SyncAllImpl()
{
    PipeBarrier<PIPE_ALL>();
    if (isAIVOnly) {
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x0, SYNC_AIV_ONLY_ALL));
        wait_flag_dev(SYNC_AIV_ONLY_ALL);
        return;
    }

    if constexpr(g_coreType == AscendC::AIC) {
        wait_flag_dev(SYNC_AIV_FLAG);
        ffts_cross_core_sync(PIPE_FIX, GetffstMsg(0x0, SYNC_AIC_FLAG));
        wait_flag_dev(SYNC_AIC_FLAG);
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x02, SYNC_AIC_AIV_FLAG));
        return;
    }
    if constexpr(g_coreType == AscendC::AIV) {
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x02, SYNC_AIV_FLAG));
        wait_flag_dev(SYNC_AIC_AIV_FLAG);
        return;
    }
}

template <uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void NotifyEventImpl(uint16_t flagId)
{
    ffts_cross_core_sync(pipe, GetffstMsg(modeId, flagId));
}

[aicore] __inline__ __attribute__((always_inline)) void WaitEventImpl(uint16_t flagId)
{
    wait_flag_dev(flagId);
}

[aicore] __inline__ __attribute__((always_inline)) void SetSyncBaseAddrImpl(uint64_t config)
{
    set_ffts_base_addr(config);
}

[aicore] __inline__ __attribute__((always_inline)) void SetSyncBaseAddr(uint64_t config)
{
    SetSyncBaseAddrImpl(config);
}
}
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_duplicate_impl.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_duplicate_impl.h"
namespace AscendC {
template <typename T> constexpr [aicore] __inline__ __attribute__((always_inline)) void CheckDuplicateSupportedType()
{
    static_assert(SupportType<T, half, bfloat16_t, int16_t, uint16_t, int32_t, uint32_t, float>(), "Failed to check "
        "dtype in Duplicate, current api support dtype combination is src and dst both: half / bfloat16_t / int16_t / "
        "uint16_t / int32_t / uint32_t / float.");
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, T scalarValue, const uint8_t repeatTimes,
    const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    vector_dup(dstLocal, scalarValue, repeatTimes, dstBlockStride, 1, dstRepeatStride, 0);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, repeatTimes, dstBlockStride, dstRepeatStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, repeatTimes, dstBlockStride, dstRepeatStride);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        ResetMask();
        SetMaskNorm();
    }
}
}
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_server.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_server.h"
namespace AscendC {
class KfcCommServer {
public:
    __attribute__((cce_global)) KfcMsg* msgSendHead;
    __attribute__((cce_global)) KfcMsg* msgSendStart;


    __attribute__((cce_global)) KfcMsg* msgRcvHead;
    __attribute__((cce_global)) KfcMsg* msgRcvStart;

    __attribute__((cce_global)) uint8_t* ubAvalidTail;

    uint8_t msgSendPos;
    uint8_t msgRcvPos;
    uint8_t subBlockID;

public:
    [aicore] __inline__ __attribute__((always_inline)) void Init(__attribute__((cce_global)) uint8_t* workspace, int i)
    {

        this->msgRcvStart = (__attribute__((cce_global)) KfcMsg*)GetMsgHead(workspace, i);
        this->msgSendStart = this->msgRcvStart + 64;

        this->msgSendHead = this->msgSendStart;
        this->msgSendPos = 0;
        this->msgRcvHead = this->msgRcvStart;
        this->msgRcvPos = 0;
        this->subBlockID = i;

                                                                             ;

                                                                            ;
        ubAvalidTail = GetUBAvaliedAddr(workspace, i);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg* AllocMessage()
    {
        return AllocMessageImpl(this->msgSendHead, this->msgSendPos, this->msgSendStart);
    }

    [aicore] __inline__ __attribute__((always_inline)) void FreeMessage(__attribute__((cce_global)) KfcMsg* msg)
    {
        FreeMessageImpl(msg);
    }

    [aicore] __inline__ __attribute__((always_inline)) void FreeUB(int32_t addr)
    {
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventID);
        WaitFlag<HardEvent::MTE3_MTE2>(eventID);
        __attribute__((cce_cube_buff)) uint32_t* dst = (__attribute__((cce_cube_buff)) uint32_t*)(TOTAL_L1_SIZE);




        create_cbuf_matrix((__attribute__((cce_cube_buff)) uint32_t*)dst, 0x10001, (uint32_t)addr);

        eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::MTE2_MTE3>(eventID);
        WaitFlag<HardEvent::MTE2_MTE3>(eventID);
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kfc/kfc_comm_server.h"
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)ubAvalidTail, (__attribute__((cce_cube_buff)) void*)dst, 0, 1, 1, 1, 1);
    }

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) KfcMsg* RcvMessage()
    {
        auto msg = (__attribute__((cce_global)) KfcMsg*)RcvMessageImpl(this->msgRcvHead, this->msgRcvPos, this->msgRcvStart);
        return msg;
    }

    [aicore] __inline__ __attribute__((always_inline)) void RollBackMsg()
    {
        RollBackMsgImpl(this->msgRcvHead, this->msgRcvPos);
        return;
    }
};
}
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_cube_group_intf.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_cube_group_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_info.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_info.h"
namespace AscendC {

constexpr uint32_t MAX_MSG_PER_AIV = 4;
constexpr uint32_t BARRIER_SIZE = 64;
constexpr uint32_t BARRIER_MAX_AIV = 50;
constexpr uint16_t CACHE_LINE_LEN = 512;
constexpr uint32_t UB_START_ADDR = TOTAL_UB_SIZE - ONE_BLK_SIZE * BARRIER_MAX_AIV;
constexpr uint16_t CACHELINE_BLKNUM = CACHE_LINE_LEN / ONE_BLK_SIZE;

enum class CubeMsgState : uint8_t {
    FREE = 0,
    VALID,
    QUIT,
    FAKE

};

struct CubeGroupMsgHead {
    volatile CubeMsgState msgState = CubeMsgState::FREE;
    volatile uint8_t aivID;
};

struct BarrierInfo {
    volatile uint32_t head;
    uint32_t buffer[15];
};


enum class PipeMode : uint8_t { SCALAR_MODE = 0, MTE3_MODE = 1, MAX };

template <int32_t ActualFuncId, int32_t ExpectFuncId>
struct IsEqual {};

template <int32_t FuncId>
struct IsEqual<FuncId, FuncId> {
    using Type = void;
};
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_cube_group_intf.h" 2
namespace AscendC {
class KfcWorkspace {
public:
    [aicore] __inline__ __attribute__((always_inline)) KfcWorkspace(__attribute__((cce_global)) uint8_t* workspace);
    [aicore] __inline__ __attribute__((always_inline)) void UpdateKfcWorkspace(uint32_t offset);
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetKfcWorkspace();
    [aicore] __inline__ __attribute__((always_inline)) ~KfcWorkspace();

private:
    friend [aicore] __inline__ __attribute__((always_inline)) uint8_t GetEventId(KfcWorkspace &desc);
    [aicore] __inline__ __attribute__((always_inline)) KfcWorkspace() = delete;
    __attribute__((cce_global)) uint8_t* msgStart;
    uint8_t evtID;
};

template <typename CubeMsgType>
class CubeResGroupHandle {
public:
    [aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle() = default;

    [aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle(
        __attribute__((cce_global)) uint8_t* workspace, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, uint8_t evtIDIn);




    [aicore] __inline__ __attribute__((always_inline)) void AssignQueue(uint8_t queueIdIn);



    template <PipeMode pipeMode = PipeMode::SCALAR_MODE>
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) CubeMsgType *AllocMessage();


    template <PipeMode pipeMode = PipeMode::SCALAR_MODE>
    [aicore] __inline__ __attribute__((always_inline)) uint16_t PostMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput);


    [aicore] __inline__ __attribute__((always_inline)) uint16_t FreeMessage(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] __inline__ __attribute__((always_inline)) uint16_t FreeMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState waitState);


    [aicore] __inline__ __attribute__((always_inline)) uint16_t PostFakeMsg(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] __inline__ __attribute__((always_inline)) void SetQuit(__attribute__((cce_global)) CubeMsgType *msg);



    [aicore] __inline__ __attribute__((always_inline)) void SetSkipMsg(uint8_t skipCnt);



    template <bool sync = true>
    [aicore] __inline__ __attribute__((always_inline)) bool Wait(uint16_t offset);

private:
    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) bool __IsRun(T handle);

    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) uint32_t __GetMsgAreaLen(T handle, uint32_t sizeOfCubeMsgStruct);


    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) void __SetAivQuit(T *handle, uint8_t aivID);


    template <typename U>
    friend [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) U *__RcvMessage(CubeResGroupHandle<U> handle);


    template <typename T>
    friend [aicore] __inline__ __attribute__((always_inline)) void __ReleaseMessage(T *handle);


    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* __GetAicMsgHead(__attribute__((cce_global)) uint8_t* workspace, uint8_t aicStart, uint16_t msgSizePerAic);


    [aicore] __inline__ __attribute__((always_inline)) void __AivUpdateMsgCurrent();


    [aicore] __inline__ __attribute__((always_inline)) void __AicUpdateMsgCurrent();


    [aicore] __inline__ __attribute__((always_inline)) bool __AivIsRun(uint8_t aivID);


    [aicore] __inline__ __attribute__((always_inline)) void __WriteGmCubeMsgByScalar(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] __inline__ __attribute__((always_inline)) void __WriteGmStateByScalar(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState newState);

    [aicore] __inline__ __attribute__((always_inline)) void __WriteGmCubeMsgByDatacopy(__attribute__((cce_global)) CubeMsgType *msgPtr, CubeMsgType &cubeMsgInput);


    [aicore] __inline__ __attribute__((always_inline)) void __CopyCubeMsg(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput);

    uint8_t aicSize = 0;
    uint8_t aivSize = 0;
    uint8_t aivPerAic = 0;
    uint8_t aivNumForCurAic = 0;
    uint8_t queueId = 0;

    __attribute__((cce_global)) CubeMsgType *msgHead;

    __attribute__((cce_global)) CubeMsgType *msgCurrent;

    __attribute__((cce_unif_buff)) CubeMsgType *ubMsg;

    uint16_t msgPos = 0;

    uint16_t msgSize = 0;


    uint64_t aivWorkState = 0;
    uint8_t aivWork = 0;
    uint8_t eventID;
};

template <int groupID, class MatmulApiType, template <class, class> class CallBack, typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle<CubeMsgType> CreateCubeResGroup(
    KfcWorkspace &desc, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, __attribute__((cce_global)) uint8_t* tiling);
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h" 2
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) KfcWorkspace::KfcWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{
    msgStart = workspace;
    if constexpr(g_coreType == AscendC::AIV) {
        evtID = GetTPipePtr()->AllocEventID<HardEvent::MTE3_S>();
        SetFlag<HardEvent::MTE3_S>(evtID);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void KfcWorkspace::UpdateKfcWorkspace(uint32_t offset)
{
    msgStart += offset;
}

[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* KfcWorkspace::GetKfcWorkspace()
{
    return msgStart;
}

[aicore] __inline__ __attribute__((always_inline)) KfcWorkspace::~KfcWorkspace()
{
    if constexpr(g_coreType == AscendC::AIV) {
        WaitFlag<HardEvent::MTE3_S>(evtID);
        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_S>(evtID);
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint8_t GetEventId(KfcWorkspace &kfcWorkspace)
{
    return kfcWorkspace.evtID;
}

template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle<CubeMsgType>::CubeResGroupHandle(
    __attribute__((cce_global)) uint8_t* workspace, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, uint8_t evtIDIn)
{
                                                                                                             ;
                                                                                                          ;
    aicSize = blockSize / MIX_NUM;
    aivSize = msgQueueSize;

    aivPerAic = Ceil(aivSize, aicSize);
    int8_t aivInLastAic = aivSize - (aicSize - 1) * aivPerAic;
                                                                                    ;

    aivNumForCurAic = (GetBlockIdxImpl() == blockStart / MIX_NUM + aicSize - 1) ? aivInLastAic : aivPerAic;
    aivWorkState = (static_cast<uint64_t>(1) << aivNumForCurAic) - 1;

    msgSize = aivPerAic * aicSize * MAX_MSG_PER_AIV;
    msgHead = (__attribute__((cce_global)) CubeMsgType *)__GetAicMsgHead(workspace, blockStart / MIX_NUM, aivPerAic * MAX_MSG_PER_AIV);
    msgCurrent = msgHead;
    if constexpr(g_coreType == AscendC::AIV) {
        eventID = evtIDIn;
        uint32_t ubMsgAddr = TOTAL_UB_SIZE - ONE_BLK_SIZE * AIV_CORE_NUM - sizeof(CubeMsgType);





        ubMsg = reinterpret_cast<__attribute__((cce_unif_buff)) CubeMsgType *>(ubMsgAddr);

    }
}




template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::AssignQueue(uint8_t queueIdIn)
{
                                                                                                                      ;
    if constexpr(g_coreType == AscendC::AIV) {
        uint8_t aicSubgroupID = queueIdIn / aivPerAic;
        uint8_t aivSubgroupID = queueIdIn % aivPerAic;
        queueId = queueIdIn;

                                                                                                                   ;
        if (aicSubgroupID != aicSize - 1) {
            aivNumForCurAic = aivPerAic;
        } else {
            aivNumForCurAic = aivSize - (aicSize - 1) * aivPerAic;
        }



                            ;
        msgHead += aicSubgroupID * aivPerAic * MAX_MSG_PER_AIV + aivSubgroupID;
        msgCurrent = msgHead;
    }
}



template <typename CubeMsgType>
template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) CubeMsgType *CubeResGroupHandle<CubeMsgType>::AllocMessage()
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (pipeMode == PipeMode::MTE3_MODE) {
            WaitFlag<HardEvent::MTE3_S>((event_t)eventID);
        }
        dcci(
            reinterpret_cast<__attribute__((cce_global)) int64_t *>(msgCurrent), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while ((msgCurrent->head).msgState != CubeMsgState::FREE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msgCurrent),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
        }
        __attribute__((cce_global)) CubeMsgType *msgReturn = msgCurrent;
        __AivUpdateMsgCurrent();
        return msgReturn;
    }
    return msgCurrent;
}


template <typename CubeMsgType>
template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::PostMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (pipeMode == PipeMode::SCALAR_MODE) {
            __CopyCubeMsg(msg, msgInput);
            __WriteGmCubeMsgByScalar(msg);
        } else if constexpr (pipeMode == PipeMode::MTE3_MODE) {
            __WriteGmCubeMsgByDatacopy(msg, msgInput);
            SetFlag<HardEvent::MTE3_S>((event_t)eventID);
        } else {
                                                                                                 ;
        }
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::FreeMessage(__attribute__((cce_global)) CubeMsgType *msg)
{
    if constexpr(g_coreType == AscendC::AIC) {
        __WriteGmStateByScalar(msg, CubeMsgState::FREE);
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::FreeMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState waitState)
{
    if constexpr(g_coreType == AscendC::AIC) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while ((msg->head).msgState != waitState) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
        __WriteGmStateByScalar(msg, CubeMsgState::FREE);
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) uint16_t CubeResGroupHandle<CubeMsgType>::PostFakeMsg(__attribute__((cce_global)) CubeMsgType *msg)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __WriteGmStateByScalar(msg, CubeMsgState::FAKE);
    }
    return msg - msgHead;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::SetQuit(__attribute__((cce_global)) CubeMsgType *msg)
{
    uint8_t aivID = queueId % aivPerAic;
    if constexpr(g_coreType == AscendC::AIV) {



                            ;
        (msg->head).aivID = aivID;
        __WriteGmStateByScalar(msg, CubeMsgState::QUIT);
    }
}



template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::SetSkipMsg(uint8_t skipCnt)
{
    if constexpr(g_coreType == AscendC::AIC) {
        aivWork += skipCnt;




                            ;
        msgPos += skipCnt;
    }
}



template <typename CubeMsgType>
template <bool sync>
[aicore] __inline__ __attribute__((always_inline)) bool CubeResGroupHandle<CubeMsgType>::Wait(uint16_t offset)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_global)) CubeMsgType *cubeMsgCur = msgHead + offset;
        dcci(
            reinterpret_cast<__attribute__((cce_global)) int64_t *>(cubeMsgCur), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        if constexpr (sync) {
            while ((cubeMsgCur->head).msgState != CubeMsgState::FREE) {
                dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(cubeMsgCur),
                    cache_line_t::SINGLE_CACHE_LINE,
                    dcci_dst_t::CACHELINE_OUT);
            }
            return true;
        } else {
            return (cubeMsgCur->head).msgState == CubeMsgState::FREE;
        }
    }
    return true;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) bool __IsRun(T handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return handle.aivWorkState;
    }
    return false;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t __GetMsgAreaLen(T handle, uint32_t sizeOfCubeMsgStruct)
{
    return handle.msgSize * sizeOfCubeMsgStruct;
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __SetAivQuit(T *handle, uint8_t aivID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t mask = ~(static_cast<uint64_t>(1) << aivID);
        handle->aivWorkState &= mask;
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __ReleaseMessage(T *handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (handle->aivWork + 1 >= handle->aivNumForCurAic) {
            uint8_t lineCnt = handle->msgPos / handle->aivPerAic;
            handle->msgPos = (lineCnt == MAX_MSG_PER_AIV - 1)
                                 ? 0
                                 : handle->msgPos + (handle->aivPerAic - handle->aivNumForCurAic) + 1;
            handle->aivWork = 0;
        } else {
            handle->msgPos += 1;
            handle->aivWork += 1;
        }
        handle->msgCurrent = handle->msgHead + handle->msgPos;
        if (handle->aivWorkState == 0) {
            return;
        }
        while (!(handle->aivWorkState & (static_cast<uint64_t>(1) << handle->aivWork))) {
            if (handle->aivWork + 1 >= handle->aivNumForCurAic) {
                uint8_t lineCnt = handle->msgPos / handle->aivPerAic;
                handle->msgPos = (lineCnt == MAX_MSG_PER_AIV - 1)
                                     ? 0
                                     : handle->msgPos + (handle->aivPerAic - handle->aivNumForCurAic) + 1;
                handle->aivWork = 0;
            } else {
                handle->msgPos += 1;
                handle->aivWork += 1;
            }
            handle->msgCurrent = handle->msgHead + handle->msgPos;
        }
    }
}

template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) CubeMsgType *__RcvMessage(CubeResGroupHandle<CubeMsgType> handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(handle.msgCurrent),
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
        while ((handle.msgCurrent->head).msgState == CubeMsgState::FREE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(handle.msgCurrent),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
        }
    }
    return handle.msgCurrent;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* CubeResGroupHandle<CubeMsgType>::__GetAicMsgHead(
    __attribute__((cce_global)) uint8_t* workspace, uint8_t aicStart, uint16_t msgSizePerAic)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint8_t aicIndex = GetBlockIdxImpl() - aicStart;
        auto ptr = reinterpret_cast<__attribute__((cce_global)) CubeMsgType *>(workspace);
        return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr[aicIndex * msgSizePerAic]);
    }
    return workspace;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__AivUpdateMsgCurrent()
{
    msgPos = (msgPos == 3) ? 0 : msgPos + 1;
    msgCurrent = msgHead + msgPos * aivPerAic;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__AicUpdateMsgCurrent()
{
    if (aivWork + 1 >= aivNumForCurAic) {
        uint8_t lineCnt = msgPos / aivPerAic;
        msgPos = (lineCnt == MAX_MSG_PER_AIV - 1) ? 0 : msgPos + (aivPerAic - aivNumForCurAic) + 1;
        aivWork = 0;
    } else {
        msgPos += 1;
        aivWork += 1;
    }
    msgCurrent = msgHead + msgPos;
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) bool CubeResGroupHandle<CubeMsgType>::__AivIsRun(uint8_t aivID)
{
    return aivWorkState & (static_cast<uint64_t>(1) << aivID);
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__WriteGmCubeMsgByScalar(__attribute__((cce_global)) CubeMsgType *msg)
{
                                                                                               ;

    for (uint32_t i = 1; i < sizeof(CubeMsgType) / sizeof(int64_t); i++) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg) + sizeof(int64_t) * i,
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
    }
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__WriteGmStateByScalar(
    __attribute__((cce_global)) CubeMsgType *msg, CubeMsgState newState)
{
                                                                                               ;
    (msg->head).msgState = newState;
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
}

template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__WriteGmCubeMsgByDatacopy(
    __attribute__((cce_global)) CubeMsgType *msgPtr, CubeMsgType &cubeMsgInput)
{
                                                                                                      ;
    auto ubData = reinterpret_cast<__attribute__((cce_unif_buff)) uint64_t *>(ubMsg);
    auto msgData = reinterpret_cast<uint64_t *>(&cubeMsgInput);
    for (uint32_t i = 0; i < sizeof(CubeMsgType) / sizeof(uint64_t); i++, ubData++, msgData++) {
        *ubData = *msgData;
    }
    event_t evtID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(evtID);
    WaitFlag<HardEvent::S_MTE3>(evtID);
    PipeBarrier<PIPE_MTE3>();
    copy_ubuf_to_gm((__attribute__((cce_global)) void *)msgPtr, (__attribute__((cce_unif_buff)) void *)ubMsg, 0, 1, sizeof(CubeMsgType) / ONE_BLK_SIZE, 0, 0);
}


template <typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) void CubeResGroupHandle<CubeMsgType>::__CopyCubeMsg(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput)
{
    auto gmPtr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(msg);
    auto msgDataPtr = reinterpret_cast<uint64_t *>(&msgInput);
    for (uint32_t i = 0; i < sizeof(CubeMsgType) / sizeof(int64_t); i++, gmPtr++, msgDataPtr++) {
        *gmPtr = *msgDataPtr;
    }
}

template <int groupID, class MatmulApiType, template <class, class> class CallBack, typename CubeMsgType>
[aicore] __inline__ __attribute__((always_inline)) CubeResGroupHandle<CubeMsgType> CreateCubeResGroup(
    KfcWorkspace &desc, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, __attribute__((cce_global)) uint8_t* tiling)
{

                                                                                                               ;
    CubeResGroupHandle handle =
        CubeResGroupHandle<CubeMsgType>(desc.GetKfcWorkspace(), blockStart, blockSize, msgQueueSize, GetEventId(desc));
    desc.UpdateKfcWorkspace(__GetMsgAreaLen(handle, sizeof(CubeMsgType)));

    if constexpr(g_coreType == AscendC::AIV) {
        return handle;
    }


    auto aicId = GetBlockIdxImpl();
    if ((aicId < blockStart / MIX_NUM) || (aicId >= (blockStart / MIX_NUM + blockSize / MIX_NUM))) {
        return handle;
    }
    MatmulApiType mm;
    CallBack<MatmulApiType, CubeMsgType> obj;
    obj.Init(obj, mm, tiling);
    while (__IsRun(handle)) {
        auto rcvMsg = __RcvMessage(handle);
        if ((rcvMsg->head).msgState == CubeMsgState::QUIT) {
            __SetAivQuit(&handle, (rcvMsg->head).aivID);
        } else {
            obj.Call(mm, rcvMsg, handle);
        }
        __ReleaseMessage(&handle);
    }
    return handle;
}

}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_group_barrier_intf.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/core_mng/roc/kernel_operator_group_barrier_intf.h"
namespace AscendC {
template <PipeMode pipeMode>
class GroupBarrier {
public:
    [aicore] __inline__ __attribute__((always_inline)) GroupBarrier(__attribute__((cce_global)) uint8_t* groupWorkspace, uint32_t arriveSizeIn, uint32_t waitSizeIn);
    [aicore] __inline__ __attribute__((always_inline)) void Arrive(uint32_t arriveIndex);

    [aicore] __inline__ __attribute__((always_inline)) void Wait(uint32_t waitIndex);
    [aicore] __inline__ __attribute__((always_inline)) uint64_t GetWorkspaceLen();

private:
    [aicore] __inline__ __attribute__((always_inline)) void __WriteCurrentValue(__attribute__((cce_global)) BarrierInfo *BarrierInfoAddr);
    [aicore] __inline__ __attribute__((always_inline)) GroupBarrier() = delete;
    __attribute__((cce_global)) BarrierInfo *barrierInfoArrive;
    __attribute__((cce_global)) BarrierInfo *barrierInfoWait;
    uint32_t arriveSize;
    uint32_t waitSize;
    uint32_t counter;
    bool hasArrive;
};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h" 2
namespace AscendC {
template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) GroupBarrier<pipeMode>::GroupBarrier(
    __attribute__((cce_global)) uint8_t* groupWorkspace, uint32_t arriveSizeIn, uint32_t waitSizeIn)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                         ;
                                                                                                                   ;
                                                                                                             ;



                            ;



                            ;
        this->barrierInfoArrive = reinterpret_cast<__attribute__((cce_global)) BarrierInfo *>(groupWorkspace);
        this->barrierInfoWait = reinterpret_cast<__attribute__((cce_global)) BarrierInfo *>(groupWorkspace + BARRIER_SIZE);
        this->arriveSize = arriveSizeIn;
        this->waitSize = waitSizeIn;
        this->counter = 1;
        this->hasArrive = false;






        __attribute__((cce_unif_buff)) int32_t *dst = reinterpret_cast<__attribute__((cce_unif_buff)) int32_t *>(UB_START_ADDR);

        for (uint32_t i = 0; i < BARRIER_MAX_AIV; i++) {
            *(__attribute__((cce_unif_buff)) uint32_t *)(dst + DEFAULT_BLK_NUM * i) = 1;
        }
    }
}

template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) void GroupBarrier<pipeMode>::Arrive(uint32_t arriveIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (counter > 1) {
            uint32_t expectedWaitNum = (counter - 1) * waitSize;
            GlobalTensor<uint32_t> barrierInfoWaitGlobal;
            __attribute__((cce_global)) BarrierInfo *BarrierInfoAddr =
                barrierInfoWait + (CACHE_LINE_LEN / sizeof(BarrierInfo)) * arriveIndex;
            dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            while (BarrierInfoAddr->head != expectedWaitNum) {
                dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            }
        }
        __WriteCurrentValue(barrierInfoArrive);
        counter += 1;
        hasArrive = true;
    }
}


template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) void GroupBarrier<pipeMode>::Wait(uint32_t waitIndex)
{



    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t waitCounter = (hasArrive) ? counter - 1 : counter;
        uint32_t expectedArriveNum = waitCounter * arriveSize;
        __attribute__((cce_global)) BarrierInfo *BarrierInfoAddr = barrierInfoArrive + (CACHE_LINE_LEN / sizeof(BarrierInfo)) * waitIndex;
        dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while (BarrierInfoAddr->head < expectedArriveNum) {
            dcci((__attribute__((cce_global)) uint64_t *)BarrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
        __WriteCurrentValue(barrierInfoWait);
        counter =
            (hasArrive) ? counter : counter + 1;
        hasArrive = false;
    }
}

template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) uint64_t GroupBarrier<pipeMode>::GetWorkspaceLen()
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                        ;
                                                                                                  ;

                                                                                                                ;

                                                                                                                    ;
        return (arriveSize > waitSize) ? arriveSize * CACHE_LINE_LEN : waitSize * CACHE_LINE_LEN;
    }
}

template <PipeMode pipeMode>
[aicore] __inline__ __attribute__((always_inline)) void GroupBarrier<pipeMode>::__WriteCurrentValue(__attribute__((cce_global)) BarrierInfo *BarrierInfoAddr)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t num = (arriveSize >= waitSize) ? arriveSize : waitSize;
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventID);
        WaitFlag<HardEvent::S_MTE3>(eventID);
        SetAtomicAddImpl<int32_t>();







        __attribute__((cce_unif_buff)) int32_t *dst = (__attribute__((cce_unif_buff)) int32_t *)(UB_START_ADDR);

        copy_ubuf_to_gm((__attribute__((cce_global)) void *)BarrierInfoAddr, (__attribute__((cce_unif_buff)) void *)dst, 0, num, 1, 0, CACHELINE_BLKNUM - 1);

        SetAtomicNoneImpl();
    }
}
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h" 2







namespace AscendC {
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_common_intf.h"
template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void IBSet(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                  int32_t blockIdx, int32_t eventID);

template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void IBWait(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                   int32_t blockIdx, int32_t eventID);







template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                 const int32_t usedCores = 0);

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockIdx();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockIdx();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetTaskRation();

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitOutput(GlobalTensor<T> gmWorkspaceAddr, uint32_t size, T value = 0);

template <bool isAIVOnly = true>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll();

enum class AtomicDtype { ATOMIC_NONE = 0, ATOMIC_F32, ATOMIC_F16, ATOMIC_S16, ATOMIC_S32, ATOMIC_S8, ATOMIC_BF16 };

enum class AtomicOp { ATOMIC_SUM = 0 };

template <AtomicDtype type, AtomicOp op>
[aicore] __inline__ __attribute__((always_inline)) void SetStoreAtomicConfig();

[aicore] __inline__ __attribute__((always_inline)) void GetStoreAtomicConfig(uint16_t& atomicType, uint16_t& atomicOp);

template <uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void CrossCoreSetFlag(uint16_t flagId);

[aicore] __inline__ __attribute__((always_inline)) void CrossCoreWaitFlag(uint16_t flagId);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCachePreload(const GlobalTensor<uint64_t>& srcTensor, const T cacheOffset);

[aicore] __inline__ __attribute__((always_inline)) int64_t GetICachePreloadStatus();

[aicore] __inline__ __attribute__((always_inline)) void ICachePreLoad(const int64_t preFetchLen);

[aicore] __inline__ __attribute__((always_inline)) void CheckLocalMemoryIA(const CheckLocalMemoryIAParam& checkParams);
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 1
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_proposal_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_proposal_impl.h"
namespace AscendC {
constexpr uint32_t singleSortElementCountV220 = 32;
constexpr uint32_t singleSortElementCountV200 = 16;
constexpr uint32_t regionProposalDataSize = 8;

template <typename T>
[[deprecated("NOTICE: MrgSort4 is not deprecated. Currently, MrgSort4 is an unsupported API on current device."
             "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void Vmrgsort4Cal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* addrArray[MRG_SORT_ELEMENT_LEN], uint64_t config)
{
                                                 ;
}

template <typename T>
[[deprecated("NOTICE: RpSort16 is not deprecated. Currently, RpSort16 is an unsupported API on current device."
             "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void VbitsortCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                 ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VbitsortCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) uint32_t* src1Local,
    const ProposalIntriParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vbitsort(dstLocal, src0Local, src1Local, intriParams.repeat);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Vmrgsort4Cal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* addrArray[MRG_SORT_ELEMENT_LEN], uint64_t src1,
    uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vmrgsort4(dstLocal, addrArray, src1, config);
    }
}

template <typename T>
[[deprecated(
    "NOTICE: ProposalConcat is not deprecated. Currently, ProposalConcat is an unsupported API on current device."
    "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void VconcatCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                       ;
}

template <typename T>
[[deprecated(
    "NOTICE: ProposalExtract is not deprecated. Currently, ProposalExtract is an unsupported API on current device."
    "Please check your code!")]]
[aicore] __inline__ __attribute__((always_inline)) void VextractCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                        ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSortCal(const LocalTensor<T> &dstLocal, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTimes)
{
    MrgSort4Info mrgSortInfo(elementCountList, false, validBit, (uint16_t)repeatTimes);





    uint64_t config = 0;
    config |= (mrgSortInfo.repeatTimes & 0xFF);
    config |= (uint64_t(mrgSortInfo.validBit & 0xF) << 8);
    config |= (uint64_t(mrgSortInfo.ifExhaustedSuspension & 0x1) << 12);

    uint64_t src1 = 0;
    src1 |= (uint64_t(mrgSortInfo.elementLengths[0] & 0xFFFF));
    src1 |= (uint64_t(mrgSortInfo.elementLengths[1] & 0xFFFF) << 16);
    src1 |= (uint64_t(mrgSortInfo.elementLengths[2] & 0xFFFF) << 32);
    src1 |= (uint64_t(mrgSortInfo.elementLengths[3] & 0xFFFF) << 48);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)sortList.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src4.GetPhyAddr()};

    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), addrArray, src1, config);
}

[aicore] __inline__ __attribute__((always_inline)) void GetMrgSortResultImpl(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4)
{
    int64_t mrgSortResult = get_vms4_sr();
    constexpr uint64_t resMask = 0xFFFF;

    mrgSortList1 = static_cast<uint64_t>(mrgSortResult) & resMask;
    constexpr uint64_t sortList2Bit = 16;

    mrgSortList2 = (static_cast<uint64_t>(mrgSortResult) >> sortList2Bit) & resMask;
    constexpr uint64_t sortList3Bit = 32;

    mrgSortList3 = (static_cast<uint64_t>(mrgSortResult) >> sortList3Bit) & resMask;
    constexpr uint64_t sortList4Bit = 48;

    mrgSortList4 = (static_cast<uint64_t>(mrgSortResult) >> sortList4Bit) & resMask;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FullSortInnerLoop(const LocalTensor<T> &dstLocal, const LocalTensor<T> &tmpLocal,
    const uint32_t baseOffset, const uint16_t singleMergeTmpElementCount, const int32_t mergeTmpRepeatTimes)
{
    if (mergeTmpRepeatTimes <= 0) {
        return;
    }
    MrgSortSrcList sortList =
        MrgSortSrcList(tmpLocal[0], tmpLocal[baseOffset], tmpLocal[2 * baseOffset], tmpLocal[3 * baseOffset]);
    const uint16_t elementCountList[MRG_SORT_ELEMENT_LEN] = {singleMergeTmpElementCount, singleMergeTmpElementCount,
        singleMergeTmpElementCount, singleMergeTmpElementCount};
    uint32_t sortedNum[MRG_SORT_ELEMENT_LEN];
    MrgSortCal<T>(dstLocal, sortList, elementCountList, sortedNum, 0b1111, mergeTmpRepeatTimes);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FullSortInnerLoopTail(const LocalTensor<T> &dstLocal, const LocalTensor<T> &tmpLocal,
    const uint32_t baseOffset, const uint16_t singleMergeTmpElementCount, const uint32_t elementCountTail,
    const int32_t mergeTmpRepeatTimes, int32_t mergeTmpTailQueNum)
{
    if (mergeTmpTailQueNum <= 0) {
        return;
    }
    uint32_t offset0Tail = MRG_SORT_ELEMENT_LEN * baseOffset * mergeTmpRepeatTimes;
    uint32_t offset1Tail, offset2Tail, offset3Tail;
    uint16_t validBitTail;
    uint16_t elementCountListTail[MRG_SORT_ELEMENT_LEN] = {singleMergeTmpElementCount, singleMergeTmpElementCount,
    singleMergeTmpElementCount, singleMergeTmpElementCount};
    if (mergeTmpTailQueNum == 2) {
        offset1Tail = offset0Tail + baseOffset;
        elementCountListTail[1] = elementCountTail;
        offset2Tail = 0;
        elementCountListTail[2] = 0;
        offset3Tail = 0;
        elementCountListTail[3] = 0;
        validBitTail = 0b0011;
    } else if (mergeTmpTailQueNum == 3) {
        offset1Tail = offset0Tail + baseOffset;
        offset2Tail = offset0Tail + 2 * baseOffset;
        elementCountListTail[2] = elementCountTail;
        offset3Tail = 0;
        elementCountListTail[3] = 0;
        validBitTail = 0b0111;
    } else {
        offset1Tail = offset0Tail + baseOffset;
        offset2Tail = offset0Tail + 2 * baseOffset;
        offset3Tail = offset0Tail + 3 * baseOffset;
        elementCountListTail[3] = elementCountTail;
        validBitTail = 0b1111;
    }
    if (mergeTmpTailQueNum > 1) {
        MrgSortSrcList sortListTail = MrgSortSrcList(tmpLocal[offset0Tail], tmpLocal[offset1Tail],
            tmpLocal[offset2Tail], tmpLocal[offset3Tail]);
        uint32_t sortedNumTail[MRG_SORT_ELEMENT_LEN];
        MrgSortCal<T>(dstLocal[offset0Tail], sortListTail, elementCountListTail, sortedNumTail, validBitTail,
            1);
    } else {
        if constexpr (IsSameType<T, half>::value) {
            DataCopy(dstLocal[offset0Tail], tmpLocal[offset0Tail], elementCountTail * 4);
        } else {
            DataCopy(dstLocal[offset0Tail], tmpLocal[offset0Tail], elementCountTail * 2);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint32_t GetFullSortInnerLoopTimes(const int32_t repeatTimes)
{
    uint32_t loopi = 0;
    int32_t queNum = repeatTimes;
    while (queNum > 1) {
        queNum = Ceil(queNum, MRG_SORT_ELEMENT_LEN);
        loopi++;
    }
    return loopi;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DoFullSort(const LocalTensor<T> &dstLocal, const LocalTensor<T> &concatLocal,
    const LocalTensor<uint32_t> &indexLocal, LocalTensor<T> &tmpLocal, const int32_t repeatTimes)
{
    uint32_t elementCount = concatLocal.GetSize();
    uint32_t singleMergeElementCount = singleSortElementCountV220;
    uint32_t loopi = GetFullSortInnerLoopTimes(repeatTimes);
    uint16_t singleMergeTmpElementCount = singleMergeElementCount;
    uint32_t srcLocalElementCount = repeatTimes * singleMergeElementCount;
    uint32_t dstLocalElementCount = srcLocalElementCount * regionProposalDataSize / sizeof(T);
    int32_t mergeTmpTotalQueNum = repeatTimes;
    int32_t mergeTmpTailQueNum = repeatTimes % MRG_SORT_ELEMENT_LEN;
    int32_t mergeTmpQueNum = mergeTmpTotalQueNum - mergeTmpTailQueNum;
    int32_t mergeTmpRepeatTimes = repeatTimes / MRG_SORT_ELEMENT_LEN;
    DataCopy(tmpLocal, dstLocal, dstLocalElementCount);
    PipeBarrier<PIPE_V>();
    for (int i = 0; i < loopi; i++) {
        uint32_t baseOffset;
        baseOffset = singleMergeTmpElementCount * regionProposalDataSize / sizeof(T);
        FullSortInnerLoop(dstLocal, tmpLocal, baseOffset, singleMergeTmpElementCount, mergeTmpRepeatTimes);
        PipeBarrier<PIPE_V>();
        uint16_t elementCountTail = srcLocalElementCount % singleMergeTmpElementCount ?
            srcLocalElementCount % singleMergeTmpElementCount :
            singleMergeTmpElementCount;
        FullSortInnerLoopTail(dstLocal, tmpLocal, baseOffset, singleMergeTmpElementCount, elementCountTail,
            mergeTmpRepeatTimes, mergeTmpTailQueNum);
        PipeBarrier<PIPE_V>();
        DataCopy(tmpLocal, dstLocal, dstLocalElementCount);
        PipeBarrier<PIPE_V>();
        singleMergeTmpElementCount *= MRG_SORT_ELEMENT_LEN;
        mergeTmpTotalQueNum = mergeTmpTotalQueNum % MRG_SORT_ELEMENT_LEN ?
            mergeTmpTotalQueNum / MRG_SORT_ELEMENT_LEN + 1 :
            mergeTmpTotalQueNum / MRG_SORT_ELEMENT_LEN;
        mergeTmpTailQueNum = mergeTmpTotalQueNum % MRG_SORT_ELEMENT_LEN;
        if (mergeTmpTailQueNum == 0 && elementCountTail != singleMergeTmpElementCount) {
            mergeTmpTailQueNum = MRG_SORT_ELEMENT_LEN;
        }
        mergeTmpQueNum = mergeTmpTotalQueNum - mergeTmpTailQueNum;
        mergeTmpRepeatTimes = mergeTmpQueNum / MRG_SORT_ELEMENT_LEN;
    }
}
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_mask_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_mask_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(__attribute__((cce_unif_buff)) uint16_t* dst, __attribute__((cce_unif_buff)) uint16_t* src0, __attribute__((cce_unif_buff)) uint16_t* src1,
    const uint8_t PatternMode, const bool reduceMode, const uint32_t mask, const GatherMaskParams& reducev2Params,
    uint64_t& rsvdCnt)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (reduceMode) {
            SetMaskCount();
        } else {
            SetMaskNorm();
        }







        set_vector_mask(0, mask);

        vreducev2(dst, src0, src1, reducev2Params.repeatTimes, reducev2Params.src0BlockStride, PatternMode,
            reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        rsvdCnt = AscendCUtils::GetRsvdCnt();
        SetMaskNorm();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(__attribute__((cce_unif_buff)) uint32_t* dst, __attribute__((cce_unif_buff)) uint32_t* src0, __attribute__((cce_unif_buff)) uint32_t* src1,
    const uint8_t PatternMode, const bool reduceMode, const uint32_t mask, const GatherMaskParams& reducev2Params,
    uint64_t& rsvdCnt)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (reduceMode) {
            SetMaskCount();
        } else {
            SetMaskNorm();
        }







        set_vector_mask(0, mask);

        vreducev2(dst, src0, src1, reducev2Params.repeatTimes, reducev2Params.src0BlockStride, PatternMode,
            reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        rsvdCnt = AscendCUtils::GetRsvdCnt();
        SetMaskNorm();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint8_t PatternMode,
    const GatherMaskParams& reducev2Params)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                                                        ;
        if (sizeof(T) == sizeof(uint16_t)) {
            vreducev2(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0),
                reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src1), reducev2Params.repeatTimes, reducev2Params.src0BlockStride,
                PatternMode, reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        } else {
            vreducev2(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0),
                reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src1), reducev2Params.repeatTimes, reducev2Params.src0BlockStride,
                PatternMode, reducev2Params.src0RepeatStride, reducev2Params.src1RepeatStride);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskImpl(
    __attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t PatternMode, const GatherMaskParams& reducev2Params)
{
                                                                             ;
    __attribute__((cce_unif_buff)) T* nullsrc1 = ONE_REPEAT_BYTE_SIZE * sizeof(T) + src0;
    GatherMaskImpl(dst, src0, nullsrc1, PatternMode, reducev2Params);
}

template <typename T, GatherMaskMode mode = defaultGahterMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint16_t* src1, const bool reduceMode,
    const uint32_t mask, const GatherMaskParams& reducev2Params, uint64_t& rsvdCnt)
{
                                                                                                               ;


                                                           ;
    GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0), src1, 0,
        reduceMode, mask, reducev2Params, rsvdCnt);
}

template <typename T, GatherMaskMode mode = defaultGahterMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint32_t* src1, const bool reduceMode,
    const uint32_t mask, const GatherMaskParams& reducev2Params, uint64_t& rsvdCnt)
{
                                                                                                               ;


                                         ;
    GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0), src1, 0,
        reduceMode, mask, reducev2Params, rsvdCnt);
}

template <typename T, GatherMaskMode mode = defaultGahterMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t src1Pattern,
    const bool reduceMode, const uint32_t mask, const GatherMaskParams& reducev2Params, uint64_t& rsvdCnt)
{
                                                                                                               ;
                                                                             ;


                                                                                                    ;
    __attribute__((cce_unif_buff)) T* nullsrc1 = ONE_REPEAT_BYTE_SIZE * sizeof(T) + src0;
    if (sizeof(T) == sizeof(uint16_t)) {
        GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0),
            reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(nullsrc1), src1Pattern, reduceMode, mask, reducev2Params, rsvdCnt);
    } else {
        GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0),
            reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(nullsrc1), src1Pattern, reduceMode, mask, reducev2Params, rsvdCnt);
    }
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetGatherMaskRemainCountImpl()
{
    return get_rsvd_cnt();
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h" 2
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort4(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params);
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RpSort16(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes);
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params);
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sort32(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& src1Local, const int32_t repeatTimes);
# 114 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalConcat(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber);
# 127 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalExtract(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber);
# 140 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Concat(LocalTensor<T> &concatLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &tmpLocal, const int32_t repeatTimes);
# 153 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Extract(const LocalTensor<T> &dstValueLocal, const LocalTensor<uint32_t> &dstIndexLocal,
    const LocalTensor<T> &sortedLocal, const int32_t repeatTimes);
# 168 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T, bool isExhaustedSuspension = false>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T> &dstLocal, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTimes);
# 182 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_proposal_intf.h"
template <typename T, bool isFullSort>
[aicore] __inline__ __attribute__((always_inline)) void Sort(const LocalTensor<T> &dstLocal, const LocalTensor<T> &concatLocal,
    const LocalTensor<uint32_t> &indexLocal, LocalTensor<T> &tmpLocal, const int32_t repeatTimes);







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortOffset(const uint32_t elemOffset);







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortLen(const uint32_t elemCount);
#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetMrgSortResult(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4);
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_determine_compute_sync_intf.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_determine_compute_sync_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h" 1
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride);
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_duplicate_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, const int32_t& calCount);
}
#pragma end_pipe
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h" 2



namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void InitDetermineComputeWorkspaceCalc(GlobalTensor<int32_t> &gmWorkspace,
    LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockNum = GetBlockNum();
        auto blockIdx = GetBlockIdx();
        if (GetBlockIdx() == 0) {
            Duplicate(ubWorkspace, 0, B32_DATA_NUM_PER_BLOCK * blockNum);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace, ubWorkspace, B32_DATA_NUM_PER_BLOCK * blockNum);
        }
        ubWorkspace.SetValue(blockNum * B32_DATA_NUM_PER_BLOCK, 1);
        PipeBarrier<PIPE_ALL>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) bool CheckUBWorkspace(LocalTensor<int32_t> &ubWorkspace, int64_t blockIdx, int64_t blockNum)
{
    int32_t repeatTime = ubWorkspace.GetValue(blockNum * B32_DATA_NUM_PER_BLOCK);
    int64_t offset = 0;


    for (; offset < blockIdx * B32_DATA_NUM_PER_BLOCK; offset += B32_DATA_NUM_PER_BLOCK) {
        if (ubWorkspace.GetValue(offset) != repeatTime) {
            return false;
        }
    }
    for (; offset < blockNum * B32_DATA_NUM_PER_BLOCK; offset += B32_DATA_NUM_PER_BLOCK) {
        if (ubWorkspace.GetValue(offset) != 0) {
            return false;
        }
    }
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void WaitPreBlockCalc(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockIdx = GetBlockIdx();
        auto blockNum = GetBlockNum();
        bool matchFlag;
        do {
            DataCopy(ubWorkspace, gmWorkspace, blockNum * B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
            SetFlag<HardEvent::MTE2_S>(eventID);
            WaitFlag<HardEvent::MTE2_S>(eventID);
            matchFlag = CheckUBWorkspace(ubWorkspace, blockIdx, blockNum);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE2));
            SetFlag<HardEvent::S_MTE2>(eventID);
            WaitFlag<HardEvent::S_MTE2>(eventID);
        } while (!matchFlag);
        PipeBarrier<PIPE_ALL>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NotifyNextBlockCalc(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockIdx = GetBlockIdx();
        auto blockNum = GetBlockNum();
        int32_t repeatTime = ubWorkspace.GetValue(blockNum * B32_DATA_NUM_PER_BLOCK);
        if (blockIdx + 1 == blockNum) {
            Duplicate(ubWorkspace, 0, blockNum * B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace, ubWorkspace, blockNum * B32_DATA_NUM_PER_BLOCK);
        } else {
            auto offset = blockIdx * B32_DATA_NUM_PER_BLOCK;
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            SetFlag<HardEvent::S_V>(eventID);
            WaitFlag<HardEvent::S_V>(eventID);
            Duplicate(ubWorkspace[offset], repeatTime, B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace[offset], ubWorkspace[offset], B32_DATA_NUM_PER_BLOCK);
        }
        ubWorkspace.SetValue(blockNum * B32_DATA_NUM_PER_BLOCK, repeatTime + 1);
        PipeBarrier<PIPE_ALL>();
    }
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_determine_compute_sync_intf.h" 2






namespace AscendC {





[aicore] __inline__ __attribute__((always_inline)) void InitDetermineComputeWorkspace(GlobalTensor<int32_t>& gmWorkspace,
    LocalTensor<int32_t>& ubWorkspace);





[aicore] __inline__ __attribute__((always_inline)) void WaitPreBlock(GlobalTensor<int32_t>& gmWorkspace, LocalTensor<int32_t>& ubWorkspace);






[aicore] __inline__ __attribute__((always_inline)) void NotifyNextBlock(GlobalTensor<int32_t>& gmWorkspace, LocalTensor<int32_t>& ubWorkspace);
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_transpose_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_transpose_impl.h"
namespace AscendC {
constexpr int8_t TWO_NUM = 2;
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dstList[16], __attribute__((cce_unif_buff)) float* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dstList[16], __attribute__((cce_unif_buff)) int32_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint32_t* dstList[16], __attribute__((cce_unif_buff)) uint32_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dstList[16], __attribute__((cce_unif_buff)) int16_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint16_t* dstList[16], __attribute__((cce_unif_buff)) uint16_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dstList[16], __attribute__((cce_unif_buff)) half* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDB8IntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == false)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, false);
    } else if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, true);
    } else if ((transDataTo5HDParams.dstHighHalf == true) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, true);
    } else {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, false);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dstList[16], __attribute__((cce_unif_buff)) int8_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
}

[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dstList[16], __attribute__((cce_unif_buff)) uint8_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
                                                                                            ;
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<float>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<int32_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<uint32_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<int16_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<uint16_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<half>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDB8IntrinsicsImpl(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == false)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, false);
    } else if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, true);
    } else if ((transDataTo5HDParams.dstHighHalf == true) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, true);
    } else {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, false);
    }
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<int8_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl<int8_t>(dstList, srcList, transDataTo5HDParams);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDIntrinsicsImpl<uint8_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl<uint8_t>(dstList, srcList, transDataTo5HDParams);
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void SetVaReg(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16])
{
    uint64_t vaRegArray1[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray2[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray3[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray4[VA_REG_ARRAY_LEN];

    for (int32_t i = 0; i < VA_REG_ARRAY_LEN; i++) {
        vaRegArray1[i] = (uint64_t)dstList[i];
        vaRegArray2[i] = (uint64_t)dstList[VA_REG_ARRAY_LEN + i];
        vaRegArray3[i] = (uint64_t)srcList[i];
        vaRegArray4[i] = (uint64_t)srcList[VA_REG_ARRAY_LEN + i];
    }

    set_va_reg_sb(VA0, vaRegArray1);
    set_va_reg_sb(VA1, vaRegArray2);
    set_va_reg_sb(VA2, vaRegArray3);
    set_va_reg_sb(VA3, vaRegArray4);
}

[aicore] __inline__ __attribute__((always_inline)) void SetVaReg(uint64_t dst[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t src[NCHW_CONV_ADDR_LIST_SIZE])
{
    set_va_reg_sb(VA0, dst);
    set_va_reg_sb(VA1, dst + VA_REG_ARRAY_LEN);
    set_va_reg_sb(VA2, src);
    set_va_reg_sb(VA3, src + VA_REG_ARRAY_LEN);
}

[aicore] __inline__ __attribute__((always_inline)) void VldVaReg(__attribute__((cce_unif_buff)) uint64_t* dst, __attribute__((cce_unif_buff)) uint64_t* src)
{
    vld_va_reg(VA0, dst, L128);
    vld_va_reg(VA1, dst, H128);
    vld_va_reg(VA2, src, L128);
    vld_va_reg(VA3, src, H128);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDImpl(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetVaReg(dstList, srcList);
        TransDataTo5HDIntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDImpl(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& transDataTo5HDParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetVaReg(dstList, srcList);
        TransDataTo5HDIntrinsicsImpl<T>(dstList, srcList, transDataTo5HDParams);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HDVldVaRegImpl(
    __attribute__((cce_unif_buff)) uint64_t* dst, __attribute__((cce_unif_buff)) uint64_t* src, const TransDataTo5HDParams& transDataTo5HDParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        VldVaReg(dst, src);
        uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE] = { 0 };
        uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE] = { 0 };
        TransDataTo5HDIntrinsicsImpl<T>(dstList, srcList, transDataTo5HDParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TransposeIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src)
{
    vtranspose((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src);
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void TransposeImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src)
{
    if constexpr(g_coreType == AscendC::AIV) {
        TransposeIntrinsicsImpl((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src);
    }
}

template <typename T> struct Transpose4dParams {
    [aicore] Transpose4dParams(){};

    uint8_t blockSize = 1;
    uint16_t tmp1RemainRowCount = 0;
    uint16_t tmp1CopyCount = 0;
    uint32_t tmp1NeedRowCount = 0;
    uint16_t tmp2Count = 0;
    uint16_t tmp2NeedRowCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t tmp3RemainRowCount = 0;
    uint16_t copyCIndex = 0;
    uint16_t srcBlockIndex = 0;
    uint32_t preCinnerOffset = 0;
    uint32_t preCoffset = 0;
    uint16_t dstBlockNum = 0;
    uint16_t dstNeedBlockNum = 0;
    uint16_t imageSize = 0;
    uint32_t oneChwSize = 0;
    uint16_t transRowCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t copyColCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t transLen = 0;
    uint32_t preTmpLen = B16_TMP_ELE_LEN;
    uint32_t dstAllBlockNum = 0;
    uint32_t imageBlockNum = 0;


    TransDataTo5HDParams transDataParams1;

    __attribute__((cce_unif_buff)) T* dstList1[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList1[NCHW_CONV_ADDR_LIST_SIZE];

    DataCopyParams dataCopyParams1;
    DataCopyParams dataCopyParams2;


    TransDataTo5HDParams transDataParams2;

    __attribute__((cce_unif_buff)) T* dstList2[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList2[NCHW_CONV_ADDR_LIST_SIZE];
};


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransBroadCastForB8Cal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
    }
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? ONE_BLK_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
    params.transDataParams1.dstHighHalf = false;
    params.transDataParams1.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();
    params.transDataParams1.dstHighHalf = true;
    params.transDataParams1.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
    }
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? ONE_BLK_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
    params.transDataParams1.dstHighHalf = false;
    params.transDataParams1.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();
    params.transDataParams1.dstHighHalf = true;
    params.transDataParams1.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransFracForB8Cal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = false;
    params.transDataParams2.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = false;
    params.transDataParams2.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[B8_TRANS_FRACTAL + n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = true;
    params.transDataParams2.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[B8_TRANS_FRACTAL + n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = true;
    params.transDataParams2.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransBroadCastCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
            params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    } else if constexpr (sizeof(T) == sizeof(uint8_t)) {
        TransBroadCastForB8Cal(dstLocal, srcLocal, params);
    } else {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
            params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
            params.dstList1[m + 1] =
                (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[params.srcBlockIndex * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransFracCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
            params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[m * params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    } else if constexpr (sizeof(T) == sizeof(uint8_t)) {
        TransFracForB8Cal(dstLocal, srcLocal, params);
    } else {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
            params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
            params.dstList2[m + 1] =
                (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransLastFracCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
        params.dstList2[m + 1] =
            (__attribute__((cce_unif_buff)) T *)dstLocal[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / TWO_NUM; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
        params.srcList2[n + NCHW_CONV_ADDR_LIST_SIZE / TWO_NUM] =
            (__attribute__((cce_unif_buff)) T *)srcLocal[n * params.blockSize].GetPhyAddr();
    }
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CopyFirstBlockCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params, DataCopyParams &dataCopyParams)
{
    if ((params.dstNeedBlockNum != 0) && (params.tmp3RemainRowCount != 0)) {
        DataCopy(dstLocal[params.dstBlockNum * (params.blockSize)], srcLocal, dataCopyParams);
        params.dstNeedBlockNum -= params.tmp3RemainRowCount;
        params.dstBlockNum += params.tmp3RemainRowCount;
        params.tmp3RemainRowCount = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UpdataCopyToTmp2ParamCal(Transpose4dParams<T> &params, const TransposeParamsExt &transposeParams)
{
    params.copyCIndex += 1;
    params.tmp2NeedRowCount -= 1;
    params.tmp1CopyCount += 1;
    params.tmp2Count += 1;
    params.tmp1RemainRowCount -= 1;
    if (params.copyCIndex == transposeParams.cSize) {
        params.copyCIndex = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UpdataTransToTmp3ParamCal(Transpose4dParams<T> &params, const uint16_t cSize,
    const TransposeType transposeType)
{
    if (transposeType == TransposeType::TRANSPOSE_NCHW2NHWC) {
        params.tmp3RemainRowCount = 1;
        params.tmp2Count = 0;
        if (params.dstNeedBlockNum == 1) {
            params.tmp2NeedRowCount = params.imageSize % params.transRowCount == 0 ?
                params.transRowCount :
                (params.imageSize % params.transRowCount);
        } else {
            params.tmp2NeedRowCount = params.transRowCount;
        }
        if ((params.dstNeedBlockNum > 1) && (sizeof(T) == sizeof(float))) {
            params.tmp3RemainRowCount = TWO_NUM;
        }
    } else if (transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        params.tmp3RemainRowCount = 1;
        params.tmp2Count = 0;
        params.tmp2NeedRowCount = cSize * params.transRowCount;

        if (sizeof(T) == sizeof(float) && (params.dstNeedBlockNum != 1)) {
            params.tmp3RemainRowCount = TWO_NUM;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UpdataTransToTmp1ParamCal(Transpose4dParams<T> &params)
{

    params.srcBlockIndex += 1;

    if (params.dstNeedBlockNum == 1) {

        params.tmp1RemainRowCount = params.oneChwSize % params.tmp1NeedRowCount == 0 ?
            params.tmp1NeedRowCount :
            (params.oneChwSize % params.tmp1NeedRowCount);
    } else {
        params.tmp1RemainRowCount = params.tmp1NeedRowCount;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CopyTodstForChwCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint16_t cSize, Transpose4dParams<T> &params)
{
    params.dataCopyParams2.blockCount = cSize;
    params.dataCopyParams2.blockLen = params.tmp3RemainRowCount;
    params.dataCopyParams2.srcStride =
        params.preTmpLen * sizeof(T) / ONE_BLK_SIZE - params.tmp3RemainRowCount;
    params.dataCopyParams2.dstStride = params.imageBlockNum - params.tmp3RemainRowCount;

    if ((params.dstNeedBlockNum != 0) && (params.tmp3RemainRowCount != 0)) {
        DataCopy(dstLocal[params.dstBlockNum * (params.blockSize)], srcLocal, params.dataCopyParams2);
        params.dstNeedBlockNum -= params.tmp3RemainRowCount;
        params.dstBlockNum += params.tmp3RemainRowCount;
        params.dstAllBlockNum = params.tmp3RemainRowCount * cSize;
        params.tmp3RemainRowCount = 0;
    }
    if (sizeof(T) == sizeof(float) && (params.dstNeedBlockNum == 1) && (params.dstAllBlockNum != 0) &&
        (params.srcBlockIndex % params.dstAllBlockNum == 0)) {
        params.tmp2NeedRowCount = cSize * (params.imageSize % NCHW_CONV_ADDR_LIST_SIZE);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose2HwcCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &sharedTmpBuffer, const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    LocalTensor<T> tempTensorA = sharedTmpBuffer;
    LocalTensor<T> tempTensorB = sharedTmpBuffer[transposeParams.cSize * params.preTmpLen];
    LocalTensor<T> tempTensorC = sharedTmpBuffer[(transposeParams.cSize + 1) * params.preTmpLen];

    while (params.dstNeedBlockNum != 0) {

        if (params.tmp1RemainRowCount == 0) {
            TransBroadCastCal(tempTensorA, srcLocal, params);

            params.srcBlockIndex += 1;

            if (params.dstNeedBlockNum == 1) {

                params.tmp1RemainRowCount = params.oneChwSize % params.tmp1NeedRowCount == 0 ?
                    transposeParams.cSize * params.tmp1NeedRowCount :
                    (params.oneChwSize % params.tmp1NeedRowCount);
            } else {
                params.tmp1RemainRowCount = transposeParams.cSize * params.tmp1NeedRowCount;
            }
        }
        PipeBarrier<PIPE_V>();

        while (params.tmp2NeedRowCount != 0) {
            params.dataCopyParams1.blockCount = 1;
            params.dataCopyParams1.blockLen = 1;

            params.preCinnerOffset = (params.copyColCount * (params.tmp1CopyCount / transposeParams.cSize));
            params.preCoffset = (params.transLen) * (params.copyCIndex % transposeParams.cSize);
            DataCopy(tempTensorB[(params.tmp2Count % params.transRowCount) * (params.blockSize)],
                tempTensorA[params.preCoffset + params.preCinnerOffset], params.dataCopyParams1);

            UpdataCopyToTmp2ParamCal(params, transposeParams);

            if (params.tmp1RemainRowCount == 0) {
                params.tmp1CopyCount = 0;
                break;
            }
            PipeBarrier<PIPE_V>();
        }

        if (params.tmp2NeedRowCount == 0) {
            TransFracCal(tempTensorC, tempTensorB, params);

            UpdataTransToTmp3ParamCal(params, transposeParams.cSize, TransposeType::TRANSPOSE_NCHW2NHWC);
        }
        PipeBarrier<PIPE_V>();

        params.dataCopyParams1.blockCount = 1;
        params.dataCopyParams1.blockLen = params.tmp3RemainRowCount;
        CopyFirstBlockCal(dstLocal, tempTensorC, transposeParams, params, params.dataCopyParams1);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose2ChwCal(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &sharedTmpBuffer, const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    LocalTensor<T> tempTensorA = sharedTmpBuffer;
    LocalTensor<T> tempTensorB = sharedTmpBuffer[params.preTmpLen];

    LocalTensor<T> tempTensorC = sharedTmpBuffer[(transposeParams.cSize + 1) * params.preTmpLen];
    while (params.dstNeedBlockNum != 0) {
        if (params.tmp1RemainRowCount == 0) {
            TransBroadCastCal(tempTensorA, srcLocal, params);

            UpdataTransToTmp1ParamCal(params);
        }
        PipeBarrier<PIPE_V>();

        while (params.tmp1RemainRowCount != 0) {
            params.dataCopyParams1.blockCount = 1;
            params.dataCopyParams1.blockLen = 1;
            params.preCinnerOffset = (params.copyColCount * (params.tmp2Count / transposeParams.cSize));
            params.preCoffset = params.transLen * (params.copyCIndex % transposeParams.cSize);
            DataCopy(tempTensorB[params.preCoffset + params.preCinnerOffset],
                tempTensorA[(params.tmp2Count % params.blockSize) * params.transRowCount], params.dataCopyParams1);

            UpdataCopyToTmp2ParamCal(params, transposeParams);

            if (params.tmp1RemainRowCount == 0) {
                params.tmp1CopyCount = 0;
                break;
            }
            PipeBarrier<PIPE_V>();
        }

        if (params.tmp2NeedRowCount == 0) {
            if ((params.imageSize % NCHW_CONV_ADDR_LIST_SIZE != 0) && (params.dstNeedBlockNum == 1)) {
                for (int16_t k = 0; k < transposeParams.cSize; k++) {
                    TransLastFracCal(tempTensorC[k * params.preTmpLen], tempTensorB[k * params.preTmpLen], params);
                }
            } else {
                for (int16_t k = 0; k < transposeParams.cSize; k++) {
                    TransFracCal(tempTensorC[k * params.preTmpLen], tempTensorB[k * params.preTmpLen], params);
                }
            }
            UpdataTransToTmp3ParamCal(params, transposeParams.cSize, TransposeType::TRANSPOSE_NHWC2NCHW);
        }
        PipeBarrier<PIPE_V>();

        CopyTodstForChwCal(dstLocal, tempTensorC, transposeParams.cSize, params);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetTransposeParamCal(const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    params.imageSize = transposeParams.hSize * transposeParams.wSize;
    params.imageBlockNum = params.imageSize * sizeof(T) / ONE_BLK_SIZE;
    params.oneChwSize = params.imageSize * transposeParams.cSize;
    params.blockSize = ONE_BLK_SIZE / sizeof(T);
    params.tmp1NeedRowCount = sizeof(T) == 1 ? ONE_BLK_SIZE : params.blockSize;

    params.transDataParams1.repeatTimes = transposeParams.cSize;
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? NCHW_CONV_ADDR_LIST_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;

    params.transLen = BYTE_PER_FRACTAL / sizeof(T);
    if constexpr (sizeof(T) == sizeof(uint8_t)) {
        params.transRowCount = B8_TRANS_ROW;
        params.copyColCount = B8_COPY_COL;
        params.preTmpLen = B8_TMP_ELE_LEN;
        params.transLen = B8_TRANS_LEN / sizeof(T);
    } else if constexpr (sizeof(T) == sizeof(float)) {
        params.preTmpLen = B32_TMP_ELE_LEN;
        if (transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
            params.copyColCount = ONE_BLK_SIZE / sizeof(T);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose4DImpl(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams)
{
    LocalTensor<T> stackBuffer = sharedTmpBuffer.ReinterpretCast<T>();
                                                                                                                    ;

    Transpose4dParams<T> params;
    GetTransposeParamCal(transposeParams, params);

    if (transposeParams.transposeType == TransposeType::TRANSPOSE_NCHW2NHWC) {
        for (int16_t i = 0; i < transposeParams.nSize; i++) {
            params.dstBlockNum = 0;
            params.srcBlockIndex = 0;
            params.copyCIndex = 0;
            params.tmp1RemainRowCount = 0;
            params.tmp1CopyCount = 0;
            params.dstNeedBlockNum = params.oneChwSize * sizeof(T) / ONE_BLK_SIZE;
            params.tmp2NeedRowCount =
                params.dstNeedBlockNum == 1 ? (params.imageSize % params.transRowCount) : params.transRowCount;
            Transpose2HwcCal(dstLocal[i * params.oneChwSize], srcLocal[i * params.oneChwSize], stackBuffer,
                transposeParams, params);
        }
    } else if (transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        for (int16_t i = 0; i < transposeParams.nSize; i++) {
            params.transDataParams1.repeatTimes = 1;
            params.transDataParams1.dstRepStride =
                params.transDataParams1.repeatTimes > 1 ? NCHW_CONV_ADDR_LIST_SIZE : 0;
            params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
            params.dstBlockNum = 0;
            params.dstAllBlockNum = 0;
            params.srcBlockIndex = 0;
            params.dstNeedBlockNum =
                params.imageBlockNum;

            params.tmp2NeedRowCount = params.oneChwSize * sizeof(T) / ONE_BLK_SIZE == 1 ?
                (params.imageSize % params.transRowCount) :
                transposeParams.cSize * params.transRowCount;


            if ((sizeof(T) == sizeof(float)) && (params.dstNeedBlockNum == 1)) {
                params.tmp2NeedRowCount = transposeParams.cSize * params.blockSize;
            }
            Transpose2ChwCal(dstLocal[i * params.oneChwSize], srcLocal[i * params.oneChwSize], stackBuffer,
                transposeParams, params);
        }
    }
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal);
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_transpose_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HD(const LocalTensor<T> (&dstLocalList)[NCHW_CONV_ADDR_LIST_SIZE],
    const LocalTensor<T> (&srcLocalList)[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HD(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType,
    const TransposeParamsExt &transposeParams);
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void TransDataTo5HD(const LocalTensor<uint64_t>& dstLocal, const LocalTensor<uint64_t>& srcLocal,
    const TransDataTo5HDParams& nchwconvParams);
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_gather_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherbImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint32_t* offset,
    const uint32_t srcLength, uint8_t repeatTimes, const GatherRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                        ;
        ResetMask();
        uint16_t dstRptStd = repeatParams.dstRepStride;
        uint8_t dstBlkStd = repeatParams.dstBlkStride;
        uint32_t offsetAddr = (uint64_t)src0;




        vgatherb(dst, offset, offsetAddr, dstRptStd, dstBlkStd, repeatTimes);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* srcOffsetLocal,
    const uint32_t srcLength, const uint32_t srcBaseAddr, const uint64_t mask, const uint8_t repeatTimes,
    const uint16_t dstRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t offsetAddr = (uint64_t)srcLocal + srcBaseAddr;




        AscendCUtils::SetMask<T>(mask);
        if constexpr (sizeof(T) == sizeof(uint16_t)) {
            vgather((__attribute__((cce_unif_buff)) uint16_t *)dstLocal, srcOffsetLocal, offsetAddr, dstRepStride, repeatTimes);
        } else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            vgather((__attribute__((cce_unif_buff)) uint32_t *)dstLocal, srcOffsetLocal, offsetAddr, dstRepStride, repeatTimes);
        } else {
                                                                                                            ;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* srcOffsetLocal,
    const uint32_t srcLength, const uint32_t srcBaseAddr, const uint64_t mask[], const uint8_t repeatTimes,
    const uint16_t dstRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t offsetAddr = (uint64_t)srcLocal + srcBaseAddr;




        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        if constexpr (sizeof(T) == sizeof(uint16_t)) {
            vgather((__attribute__((cce_unif_buff)) uint16_t *)dstLocal, (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal, offsetAddr, dstRepStride,
                repeatTimes);
        } else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            vgather((__attribute__((cce_unif_buff)) uint32_t *)dstLocal, (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal, offsetAddr, dstRepStride,
                repeatTimes);
        } else {
                                                                                                            ;
        }
    }
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h" 2


#pragma begin_pipe(V)
namespace AscendC {
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gatherb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& offsetLocal, const uint8_t repeatTimes, const GatherRepeatParams& repeatParams);
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstRepStride);
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstRepStride);
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint32_t count);
}
#pragma end_pipe
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_scatter_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_scatter_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ScatterImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* dstOffsetLocal,
    const uint32_t dstLength, const uint32_t dstBaseAddr, const uint64_t mask, const uint8_t repeatTimes,
    const uint8_t srcRepStride)
{
                                                ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ScatterImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* dstOffsetLocal,
    const uint32_t dstLength, const uint32_t dstBaseAddr, const uint64_t mask[], const uint8_t repeatTimes,
    const uint8_t srcRepStride)
{
                                                ;
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h" 2






#pragma begin_pipe(V)
namespace AscendC {
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint8_t srcRepStride);
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint8_t srcRepStride);
# 80 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint32_t count);
}
#pragma end_pipe
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h" 1
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_brcb_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_brcb_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BrcbImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t repeatTimes,
    const BrcbRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        ResetMask();
        if constexpr(sizeof(T) == B16_BYTE_SIZE) {
            vbrcb((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src0, repeatParams.dstBlkStride,
                repeatParams.dstRepStride, repeatTimes);
        } else if constexpr(sizeof(T) == B32_BYTE_SIZE) {
            vbrcb((__attribute__((cce_unif_buff)) uint32_t*)dst, (__attribute__((cce_unif_buff)) uint32_t*)src0, repeatParams.dstBlkStride,
                repeatParams.dstRepStride, repeatTimes);
        } else {


                               ;
        }
    }
}
}
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h" 2



#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_brcb_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Brcb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local, const uint8_t repeatTimes,
    const BrcbRepeatParams& repeatParams);
}
#pragma end_pipe
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h" 1
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 118 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 140 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 158 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 180 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 220 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                 const LocalTensor<U>& src1Local, const uint64_t mask[], const uint8_t repeatTimes,
                                 const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                 const LocalTensor<U>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                 const BinaryRepeatParams& repeatParams);
# 238 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                 const LocalTensor<U>& src1Local, const int32_t& calCount);
# 260 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 278 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 300 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 318 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 340 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                           const BinaryRepeatParams& repeatParams);
# 358 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                           const LocalTensor<T>& src1Local, const int32_t& calCount);
# 380 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                          const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                          const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                          const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                          const BinaryRepeatParams& repeatParams);
# 398 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                          const LocalTensor<T>& src1Local, const int32_t& calCount);
# 420 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);
# 438 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, const int32_t& calCount);
# 460 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<half>& dstLocal, const LocalTensor<int32_t>& src0Local,
                                  const LocalTensor<int32_t>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                  const LocalTensor<U>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);

template <bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<half>& dstLocal, const LocalTensor<int32_t>& src0Local,
                                  const LocalTensor<int32_t>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                  const LocalTensor<U>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                  const BinaryRepeatParams& repeatParams);
# 487 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<half>& dstLocal, const LocalTensor<int32_t>& src0Local,
                                  const LocalTensor<int32_t>& src1Local, const int32_t& calCount);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
                                  const LocalTensor<U>& src1Local, const int32_t& calCount);
# 512 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                   const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                   const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                   const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                   const BinaryRepeatParams& repeatParams);
# 530 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                   const LocalTensor<T>& src1Local, const int32_t& calCount);
# 552 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                       const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                                       const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                       const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                                       const BinaryRepeatParams& repeatParams);
# 570 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                                       const LocalTensor<T>& src1Local, const int32_t& calCount);
# 592 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes,
                               const BinaryRepeatParams& repeatParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
                               const LocalTensor<T>& src1Local, const int32_t& calCount);
}
#pragma end_pipe
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_scalar_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_binary_scalar_impl.h"
namespace AscendC {



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Adds, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vadds(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            AddsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        AddsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MulsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Muls, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmuls(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MulsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        MulsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MaxsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Maxs, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmaxs(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride, false, false);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MaxsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MaxsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MaxsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        MaxsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MinsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Mins, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmins(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride, false, false);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MinsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MinsIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MinsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        MinsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, uint16_t, uint32_t, int16_t, int32_t>(), "Failed to check dtype in ShiftLeft, current "
        "api support dtype combination is src and dst both: uint16_t / uint32_t / int16_t / int32_t.");
    vshl(dst, src, (uint32_t)scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride,
        (uint16_t)repeatParams.srcBlkStride, (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            ShiftLeftIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams, bool roundEn)
{


                                ;
    if (roundEn) {
        if constexpr (SupportType<T, int16_t, int32_t>()) {
            vshr(dst, src, (int32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, true);
        } else {
            vshr(dst, src, (uint32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, true);
        }
    } else {
        if constexpr (SupportType<T, int16_t, int32_t>()) {
            vshr(dst, src, (int32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, false);
        } else {
            vshr(dst, src, (uint32_t)scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride, false);
        }
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams, roundEn);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams, roundEn);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            ShiftRightIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE }, false);
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE }, false);
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in LeakyRelu, current api support dtype "
        "combination is src and dst both: half / float.");
    vlrelu(dst, src, scalarValue, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint16_t)repeatParams.dstRepStride, (uint16_t)repeatParams.srcRepStride);
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            LeakyReluIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, calCount);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h" 2





#pragma begin_pipe(V)
namespace AscendC {
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 108 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 134 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 159 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 185 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 210 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 236 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 261 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 287 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 312 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn = false);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn);
# 338 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
# 363 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 389 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount);

template <typename T, typename U, bool isSetMask = true,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
}
#pragma end_pipe
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmpsel_impl.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_cmpsel_impl.h"
namespace AscendC {




template <typename U> [aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImplPre(__attribute__((cce_unif_buff)) U* sel, SELMODE selMode)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        set_cmpmask(sel);
        PipeBarrier<PIPE_V>();
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {







        uint32_t selAddr = static_cast<uint32_t>(reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel)));
        __attribute__((cce_unif_buff)) uint32_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint32_t>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<uint32_t>(ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, selAddr, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<uint32_t>(tempBuf);

    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    (void)sel;
    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImplPre(T src1)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_unif_buff)) T* tempBuf = AscendCUtils::GetTemporaryBufferAddr<T>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<T>(ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<T>(tempBuf);
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, T src1, SELMODE selMode,
    int32_t repeat, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        (void)src1;
        vsel(dst, src0, sel, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}

template <typename T, SELMODE selMode = SELMODE::VSEL_CMPMASK_SPR>
[aicore] __inline__ __attribute__((always_inline)) void SelectCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, int32_t repeat,
    const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if constexpr (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
    } else if constexpr (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {
        vsel(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SelectCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, int32_t repeat,
    const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vsel(dst, src0, sel, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(SELMODE::VSEL_TENSOR_SCALAR_MODE));
    }
}






template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, SELMODE selMode,
    uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    BinaryRepeatParams repeatParams;
    SetMaskCount();

    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        set_cmpmask(sel);
        PipeBarrier<PIPE_V>();

        AscendCUtils::SetMask<U>(0, calCount);
        vsel(dst, src0, src1, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
        PipeBarrier<PIPE_V>();
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {







        uint32_t selAddr = static_cast<uint32_t>(reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel)));
        __attribute__((cce_unif_buff)) uint32_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint32_t>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<U>(0, ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, selAddr, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<uint32_t>(tempBuf);

        AscendCUtils::SetMask<U>(0, calCount);
        vsel(dst, src0, src1, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
        PipeBarrier<PIPE_V>();
    }

    ResetMask();
    SetMaskNorm();
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, T src1, SELMODE selMode,
    uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SetMaskCount();
    __attribute__((cce_unif_buff)) T* tempBuf = AscendCUtils::GetTemporaryBufferAddr<T>(TMP_UB_OFFSET, ONE_BLK_SIZE);

    AscendCUtils::SetMask<U>(0, ONE_BLK_SIZE);
    DuplicateIntrinsicsImpl(tempBuf, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    set_cmpmask(tempBuf);
    PipeBarrier<PIPE_V>();

    AscendCUtils::FreeTemporaryBuffer<T>(tempBuf);

    AscendCUtils::SetMask<U>(0, calCount);
    BinaryRepeatParams repeatParams;
    vsel(dst, src0, sel, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
        repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
        static_cast<uint8_t>(selMode));

    ResetMask();
    SetMaskNorm();
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) T* src1Local,
    SELMODE selMode, const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(selMask, selMode);

        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) T* src1Local,
    SELMODE selMode, const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(selMask, selMode);

        AscendCUtils::SetMask<T>(mask);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, T src1Local,
    SELMODE selMode, const uint64_t mask[], const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(src1Local);

        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, T src1Local,
    SELMODE selMode, const uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(src1Local);

        AscendCUtils::SetMask<T>(mask);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTimes, repeatParams);
    }
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h" 2






#pragma begin_pipe(V)
namespace AscendC {
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask[],
    const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask,
    const BinaryRepeatParams& repeatParams);
# 90 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, uint32_t calCount);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetCmpMask(const LocalTensor<T>& dst);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetCmpMask(const LocalTensor<T>& src);
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);
# 136 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, uint32_t calCount);
# 165 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask[],
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);

template <typename T, SELMODE selMode>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    SelectCal<T, selMode>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), repeatTimes, repeatParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    SelectCal<T, U>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), repeatTimes, repeatParams);
}
# 203 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint32_t calCount);
# 226 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);
# 248 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint32_t calCount);
}
#pragma end_pipe
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_reduce_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_reduce_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgadd(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgmax(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgmin(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcpadd(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t srcBlkStride, const int32_t dstRepStride, const int32_t srcRepStride)
{
    vcadd(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride, 0);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(maskCount);
        BlockReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(maskCount);
        BlockReduceMaxIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(maskCount);
        BlockReduceMinIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(maskCount);
        PairReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceMaxIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceMinIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        PairReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeat,
    const int32_t elemsInOneRepeat, const int32_t dstBlkStride, const int32_t srcBlkStride, const int32_t dstRepStride,
    const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(elemsInOneRepeat);
        RepeatReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeat, srcBlkStride, dstRepStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            set_mask_count();
            set_vector_mask(0, count);
        }
        vcadd(dst, src, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, 1);
        auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        int64_t accVal = get_acc_val();
        *(dst) = *(reinterpret_cast<T*>(&accVal));
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        if constexpr (isSetMask) {
            set_mask_norm();
            set_vector_mask((uint64_t)-1, (uint64_t)-1);
        }
    }
}


template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceOrder order)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        if (order == ReduceOrder::ORDER_VALUE_INDEX) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::VALUE_INDEX);
        } else if (order == ReduceOrder::ORDER_INDEX_VALUE) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::INDEX_VALUE);
        } else if (order == ReduceOrder::ORDER_ONLY_VALUE) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_VALUE);
        } else {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_INDEX);
        }
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMaxImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t mask,
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMaxImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceOrder order)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        if (order == ReduceOrder::ORDER_VALUE_INDEX) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::VALUE_INDEX);
        } else if (order == ReduceOrder::ORDER_INDEX_VALUE) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::INDEX_VALUE);
        } else if (order == ReduceOrder::ORDER_ONLY_VALUE) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_VALUE);
        } else {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_INDEX);
        }
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMinImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t mask,
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMinImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        vcadd(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride, 0);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceSumImpl<T, isSetMask>(dstLocal, srcLocal, params);
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint32_t mask,
    const int32_t repeat, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    struct ReduceRepeatParams params(mask, repeat, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceSumImpl<T, isSetMask>(dstLocal, srcLocal, params);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride)
{
    vcmax(workLocal, srcLocal, repeatTimes, 1, 1, srcRepStride, Order_t::VALUE_INDEX);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride)
{
    vcmin(workLocal, srcLocal, repeatTimes, 1, 1, srcRepStride, Order_t::VALUE_INDEX);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride)
{
    vcadd(workLocal, srcLocal, repeatTimes, 1, 1, srcRepStride, 0);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumSecondStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = params.repeatTimes / elementNumPerRep;
    int32_t leftData = params.repeatTimes % elementNumPerRep;

    uint64_t highMask = 0;
    uint64_t lowMask = 0;


    lowMask = params.repeatTimes;



    SetMaskCount();

    AscendCUtils::SetMask<T>(highMask, lowMask);
    ReduceSumIntrinsicsImpl<T>(workLocal, workLocal, 1, DEFAULT_REPEAT_STRIDE);

    SetMaskNorm();
# 360 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_reduce_impl.h"
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateSpecialFormatMask(const int32_t& maskLen, uint64_t& highMask, uint64_t& lowMask)
{

    int32_t halfLen = HLAF_MASK_LEN / 2;
    for (int32_t i = 0; i < maskLen - halfLen; i++) {
        highMask = highMask << 2;
        highMask = highMask | 1;
    }
    int32_t lowMaskRange = maskLen >= halfLen ? halfLen : maskLen;
    for (int32_t i = 0; i < lowMaskRange; i++) {
        lowMask = lowMask << 2;
        lowMask = lowMask | 1;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceOperation(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTimes,
    const int32_t srcRepStride, const uint64_t& highMask, const uint64_t& lowMask, const ReduceMode& mode)
{
    AscendCUtils::SetMask<T>(highMask, lowMask);
    switch (mode) {
        case ReduceMode::REDUCE_MAX:
            ReduceMaxIntrinsicsImpl(workLocal, srcLocal, repeatTimes, srcRepStride);
            break;
        case ReduceMode::REDUCE_MIN:
            ReduceMinIntrinsicsImpl(workLocal, srcLocal, repeatTimes, srcRepStride);
            break;
        case ReduceMode::REDUCE_SUM:
            ReduceSumIntrinsicsImpl(workLocal, srcLocal, repeatTimes, srcRepStride);
            break;
        default:
            break;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplFirstStep(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode, int32_t& curData)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = index * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, MAX_REPEAT_TIMES, params.srcRepStride,
            params.highMask, params.lowMask, mode);
    }
    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = range * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, leftRepeatTimes, params.srcRepStride,
            params.highMask, params.lowMask, mode);
    }
    curData = VREDUCE_PER_REP_OUTPUT * params.repeatTimes;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplSecondStep(__attribute__((cce_unif_buff)) T* workLocal, const ReduceMode& mode, int32_t& curData,
    int32_t preStartPos, int32_t secondStartPos)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t newMaskLen = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    int32_t bodyOutputCount = 0;
    int32_t tailOutputCount = 0;

    if (newRepeatTimes >= 1) {
        highMask = (sizeof(T) == sizeof(half)) ? 0x5555555555555555 : 0;
        lowMask = 0x5555555555555555;

        ReduceOperation<T>(workLocal + secondStartPos, workLocal + preStartPos, newRepeatTimes, DEFAULT_REPEAT_STRIDE,
            highMask, lowMask, mode);
        bodyOutputCount = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
    }
    highMask = 0;
    lowMask = 0;

    if (leftData > 0) {
        newMaskLen = leftData / VREDUCE_PER_REP_OUTPUT;

        CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);

        dstOffset = secondStartPos + bodyOutputCount;
        srcOffset = preStartPos + newRepeatTimes * elementNumPerRep;
        ReduceOperation<T>(workLocal + dstOffset, workLocal + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        tailOutputCount = VREDUCE_PER_REP_OUTPUT;
    }

    curData = bodyOutputCount + tailOutputCount;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetIndex(__attribute__((cce_unif_buff)) T* workLocal, int32_t secondStartPos, int32_t& secondIndex,
    int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + secondStartPos + 1);
                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + thirdIndex + 1);
                                                                                                  ;
    } else {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + secondStartPos + 1);
                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + thirdIndex + 1);
                                                                                                  ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetIndex(__attribute__((cce_unif_buff)) T* workLocal, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + thirdStartPos + 1);
                                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + secondStartPos + thirdIndex + 1);
                                                                                                                  ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal +
            elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex + 1);
                                                                                                                ;
    } else {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + thirdStartPos + 1);
                                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + secondStartPos + thirdIndex + 1);
                                                                                                                  ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal +
            elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex + 1);
                                                                                                                ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetIndex(__attribute__((cce_unif_buff)) T* workLocal, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t fourthStartPos, int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex, int32_t& fourthIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        fourthIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + fourthStartPos + 1);



          ;
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + thirdStartPos + fourthIndex + 1);



          ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);



          ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(workLocal +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);



          ;
    } else {
        fourthIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + fourthStartPos + 1);



          ;
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + thirdStartPos + fourthIndex + 1);



          ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);



          ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(workLocal +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);



          ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplThirdStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal, const int32_t srcRepStride,
    const ReduceMode& mode, int32_t& curData, int32_t& secondStartPos, int32_t& thirdStartPos)
{
    int32_t preNum = 0;
    int32_t firstIndex = 0;
    int32_t secondIndex = 0;
    int32_t thirdIndex = 0;
    int32_t fourthIndex = 0;
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;

    int32_t offsetNumPerRep = ONE_BLK_SIZE / sizeof(T) * srcRepStride;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (curData == VREDUCE_PER_REP_OUTPUT) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        GetIndex<T>(workLocal, secondStartPos, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT);
        int32_t redultIndex = secondIndex + preNum;
        *dstLocal = *(workLocal + secondStartPos);
        *(dstLocal + 1) = *reinterpret_cast<T*>(&redultIndex);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        return;
    }

    int32_t newMaskLen = curData / VREDUCE_PER_REP_OUTPUT;
    CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);
    if (curData > elementNumPerRep) {
        ReduceImplSecondStep<T>(workLocal, mode, curData, secondStartPos, thirdStartPos);

        int32_t fourthStartPos =
            (((thirdStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
        dstOffset = fourthStartPos;
        srcOffset = thirdStartPos;
        PipeBarrier<PIPE_V>();
        ReduceOperation<T>(workLocal + dstOffset, workLocal + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        *dstLocal = *(workLocal + dstOffset);

        GetIndex<T>(workLocal, secondStartPos, thirdStartPos, fourthStartPos, firstIndex, secondIndex, thirdIndex,
            fourthIndex);
        preNum = offsetNumPerRep *
            (elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT + secondIndex) / VREDUCE_PER_REP_OUTPUT;
    } else {
        dstOffset = thirdStartPos;
        srcOffset = secondStartPos;

        ReduceOperation<T>(workLocal + dstOffset, workLocal + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        *dstLocal = *(workLocal + thirdStartPos);

        GetIndex<T>(workLocal, secondStartPos, thirdStartPos, firstIndex, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex) /
            VREDUCE_PER_REP_OUTPUT;
    }

    int32_t redultIndex = firstIndex + preNum;
    *(dstLocal + 1) = *reinterpret_cast<T*>(&redultIndex);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumFirstStep(__attribute__((cce_unif_buff)) T* workLocal, __attribute__((cce_unif_buff)) T* srcLocal,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES;
        srcOffset = index * MAX_REPEAT_TIMES * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, MAX_REPEAT_TIMES, params.srcRepStride,
            params.highMask, params.lowMask, ReduceMode::REDUCE_SUM);
    }

    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES;
        srcOffset = range * MAX_REPEAT_TIMES * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        ReduceOperation<T>(workLocal + dstOffset, srcLocal + srcOffset, leftRepeatTimes, params.srcRepStride,
            params.highMask, params.lowMask, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumFinalStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal, int32_t& secondResultNum)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    if (secondResultNum == 1) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *(dstLocal) = *(workLocal);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    } else {
        highMask = (secondResultNum > HLAF_MASK_LEN) ? ((((uint64_t)1) << (secondResultNum - HLAF_MASK_LEN)) - 1) : 0;
        lowMask = (secondResultNum > HLAF_MASK_LEN) ? FULL_MASK : ((((uint64_t)1) << secondResultNum) - 1);
        ReduceOperation<T>(dstLocal, workLocal, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params)
{
    ReduceSumFirstStep<T>(workLocal, srcLocal, params);
    PipeBarrier<PIPE_V>();
    ReduceSumSecondStep<T>(dstLocal, workLocal, params);
    PipeBarrier<PIPE_V>();
    int32_t secondResultNum =
        (params.repeatTimes + ONE_REPEAT_BYTE_SIZE / sizeof(T) - 1) / (ONE_REPEAT_BYTE_SIZE / sizeof(T));
    ReduceSumFinalStep<T>(dstLocal, workLocal, secondResultNum);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplSecondStepNoIndex(__attribute__((cce_unif_buff)) T* workLocal, const ReduceMode& mode, int32_t& curData)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0, lowMask = 0;
    if (newRepeatTimes != 0) {
        CreateSpecialFormatMask<T>(elementNumPerRep / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        ReduceOperation<T>(workLocal, workLocal, newRepeatTimes, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
    }
    highMask = 0;
    lowMask = 0;
    if (leftData > 0) {
        CreateSpecialFormatMask<T>(leftData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        ReduceOperation<T>(workLocal + newRepeatTimes * VREDUCE_PER_REP_OUTPUT,
            workLocal + newRepeatTimes * elementNumPerRep, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
        newRepeatTimes += 1;
    }
    curData = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplThirdStepNoIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* workLocal, const ReduceMode& mode,
    int32_t& curData)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;

    CreateSpecialFormatMask<T>(curData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
    ReduceOperation<T>(workLocal, workLocal, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    *dstLocal = *workLocal;
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplWithIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (params.repeatTimes == 1) {
            ReduceOperation<T>(dstLocal, srcLocal, 1, params.srcRepStride, params.highMask, params.lowMask, mode);
        } else {
            int32_t curData = 0;
            ReduceImplFirstStep<T>(workLocal, srcLocal, params, mode, curData);
            PipeBarrier<PIPE_V>();
            int32_t secondStartPos =
                ((curData * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
            ReduceImplSecondStep<T>(workLocal, mode, curData, 0, secondStartPos);
            PipeBarrier<PIPE_V>();
            int32_t thirdStartPos =
                (((secondStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
            ReduceImplThirdStep<T>(dstLocal, workLocal, params.srcRepStride, mode, curData, secondStartPos,
                thirdStartPos);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImplNoIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (params.repeatTimes == 1) {
            ReduceOperation<T>(workLocal, srcLocal, 1, params.srcRepStride, params.highMask, params.lowMask, mode);
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);
            *dstLocal = *workLocal;
            event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_V>(eventIdSToV);
            WaitFlag<HardEvent::S_V>(eventIdSToV);
            SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        } else {
            if (mode == ReduceMode::REDUCE_SUM) {
                ReduceSumImpl<T>(dstLocal, srcLocal, workLocal, params);
            } else {
                int32_t curData = 0;
                ReduceImplFirstStep<T>(workLocal, srcLocal, params, mode, curData);
                PipeBarrier<PIPE_V>();
                ReduceImplSecondStepNoIndex<T>(workLocal, mode, curData);

                int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
                if (curData <= elementNumPerRep) {
                    PipeBarrier<PIPE_V>();
                    ReduceImplThirdStepNoIndex<T>(dstLocal, workLocal, mode, curData);
                    return;
                }
                PipeBarrier<PIPE_V>();
                ReduceImplSecondStepNoIndex<T>(workLocal, mode, curData);
                if (curData <= elementNumPerRep) {
                    PipeBarrier<PIPE_V>();
                    ReduceImplThirdStepNoIndex<T>(dstLocal, workLocal, mode, curData);
                }
            }
        }
    }
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* workLocal,
    struct ReduceRepeatParams& params, bool calIndex, const ReduceMode& mode)
{
    if (calIndex) {
        ReduceImplWithIndex<T>(dstLocal, srcLocal, workLocal, params, mode);
    } else {
        ReduceImplNoIndex<T>(dstLocal, srcLocal, workLocal, params, mode);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceTailCompute(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex, const ReduceMode& mode)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t repeatTimes = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    T bodyValue = dstLocal.GetValue(0);
    T bodyIndex = dstLocal.GetValue(1);

    struct ReduceRepeatParams tailParams(tailCount, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(elementNumPerRep * repeatTimes), (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(),
        tailParams, calIndex, mode);
    eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    T tailValue = dstLocal.GetValue(0);
    T tailIndex = dstLocal.GetValue(1);


    struct ReduceRepeatParams lastParams(2, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    workLocal.SetValue(0, bodyValue);
    workLocal.SetValue(1, tailValue);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), lastParams, calIndex, mode);
    if (calIndex) {
        eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        T lastIndexVal = dstLocal.GetValue(1);
        uint32_t newIndex = 0;
        uint32_t lastIndex = 0;
        if (sizeof(T) == sizeof(half)) {
            lastIndex = *reinterpret_cast<uint16_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTimes + *reinterpret_cast<uint16_t*>(&tailIndex);
        } else {
            lastIndex = *reinterpret_cast<uint32_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTimes + *reinterpret_cast<uint32_t*>(&tailIndex);
        }
        if (lastIndex == 1) {
            dstLocal.SetValue(1, *reinterpret_cast<T*>(&newIndex));
        } else {
            dstLocal.SetValue(1, bodyIndex);
        }
        eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{
    int64_t maxMinCnt = get_max_min_cnt();
    if constexpr (IsSameType<T, half>::value) {
        constexpr uint64_t valueMask = 0xffff;
        maxMinValue = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    } else {
        constexpr uint64_t valueMask = 0xffffffff;
        maxMinValue = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    }
    constexpr uint64_t indexBit = 32;
    maxMinIndex = (static_cast<uint64_t>(maxMinCnt) >> indexBit);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(uint32_t &maxMinValue)
{
                                                                                   ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(T &maxMinValue, T &maxMinIndex)
{
    int64_t maxMinCnt = get_max_min_cnt();
    uint32_t maxVal = 0;
    uint32_t maxIdx = 0;
    if constexpr (IsSameType<T, half>::value) {
        constexpr uint64_t valueMask = 0xffff;
        maxVal = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    } else {
        constexpr uint64_t valueMask = 0xffffffff;
        maxVal = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    }
    maxMinValue = *(reinterpret_cast<T*>(&maxVal));

    constexpr uint64_t indexBit = 32;
    maxIdx = (static_cast<uint64_t>(maxMinCnt) >> indexBit);
    maxMinIndex = *(reinterpret_cast<T*>(&maxIdx));
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCountImpl(T &maxMinValue)
{
                                                                                   ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) T GetAccValImpl()
{
    int64_t accVal = get_acc_val();
    return *(reinterpret_cast<T*>(&accVal));
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h" 2
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 103 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t elemsInOneRepeat, const int32_t dstBlkStride, const int32_t srcBlkStride,
    const int32_t dstRepStride, const int32_t srcRepStride);
# 145 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 161 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
# 177 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
# 207 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
# 223 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
# 238 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex = 0);
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride);
# 263 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex = 0);
# 276 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex = 0);
# 288 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count);
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue, T &maxMinIndex);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) T GetAccVal();
}
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_mask_intf.h" 1
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_gather_mask_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
template <typename T, typename U, GatherMaskMode mode = defaultGahterMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<U>& src1Pattern, const bool reduceMode, const uint32_t mask,
    const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt);

template <typename T, GatherMaskMode mode = defaultGahterMaskMode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const uint8_t src1Pattern, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt);
#pragma end_pipe

[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) int64_t GetGatherMaskRemainCount();
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_mulcast_intf.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_mulcast_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams);

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint32_t calCount);
}
#pragma end_pipe
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_bilinearinterpalation_intf.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_bilinearinterpalation_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer);
}
#pragma end_pipe
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_createvecindex_intf.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_createvecindex_intf.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask, uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask[], uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> dstLocal, const T &firstValue,
    uint32_t calCount);
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_ternary_scalar_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_ternary_scalar_impl.h"
namespace AscendC {

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, U scalarValue, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{


                                                        ;
    vaxpy(dst, src, scalarValue, repeatTimes, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
        repeatParams.dstRepStride, repeatParams.srcRepStride);
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if (sizeof(T) > sizeof(U)) {
                AscendCUtils::SetMask<T>(mask);
            } else {
                AscendCUtils::SetMask<U>(mask);
            }
        }
        AxpyIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AxpyIntrinsicsImpl(dst, src, scalarValue, repeatTimes, repeatParams);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetMaskCount();
        AscendCUtils::SetMask<U>(0, calCount);
        if constexpr (sizeof(T) > sizeof(U)) {
            AxpyIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
        } else {
            AxpyIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        ResetMask();
        SetMaskNorm();
    }
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h" 2
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_ternary_scalar_intf.h"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    const int32_t& calCount);
}
#pragma end_pipe
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h" 1
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_unary_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_unary_impl.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int32_t>(), "Failed to check dtype in Relu, current api support dtype "
        "combination is src and dst both: half / float / int32_t.");
    vrelu(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ExpIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Exp, current api support dtype combination "
        "is src and dst both: half / float.");
    vexp(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LnIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Ln, current api support dtype combination "
        "is src and dst both: half / float.");
    vln(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AbsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Abs, current api support dtype combination "
        "is src and dst both: half / float.");
    vabs(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReciprocalIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Reciprocal, current api support dtype "
        "combination is src and dst both: half / float.");
    vrec(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RsqrtIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Rsqrt, current api support dtype "
        "combination is src and dst both: half / float.");
    vrsqrt(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SqrtIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Sqrt, current api support dtype "
        "combination is src and dst both: half / float.");
    vsqrt(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void NotIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in Not, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vnot(dst, src, repeatTimes, (uint16_t)repeatParams.dstBlkStride, (uint16_t)repeatParams.srcBlkStride,
        (uint8_t)repeatParams.dstRepStride, (uint8_t)repeatParams.srcRepStride);
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ReluIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ReluIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vrelu(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ExpIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ExpIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vexp(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        LnIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        LnIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vln(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AbsIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AbsIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vabs(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ReciprocalIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ReciprocalIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vrec(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        RsqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        RsqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vrsqrt(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SqrtIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vsqrt(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}



template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        NotIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        NotIntrinsicsImpl(dst, src, repeatTimes, repeatParams);
    }
}


template <typename T> [aicore] __inline__ __attribute__((always_inline)) void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        vnot(dst, src, 1, (uint16_t)DEFAULT_BLK_STRIDE, (uint16_t)DEFAULT_BLK_STRIDE,
            (uint8_t)DEFAULT_REPEAT_STRIDE, (uint8_t)DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h" 2






#pragma begin_pipe(V)
namespace AscendC {
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 73 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 103 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 133 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 149 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 163 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 179 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 193 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t& calCount);
# 210 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 240 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 254 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
# 270 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 284 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount);
}
#pragma end_pipe
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vconv_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vconv_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    vconv_deq(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride, repeatParams.dstRepStride,
        repeatParams.srcRepStride);
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int8_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s82f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) uint8_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_u82f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s322f32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s322f32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s322f32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s322f32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s322f32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_s322f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_f162f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s8r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s8f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s8c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s8a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s8z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162s8(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162u8r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162u8f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162u8c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162u8a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162u8z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162u8(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322f16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322f16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322f16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322f16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322f16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:
            vconv_f322f16o(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f322f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
                                                                          ;
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
                                                                         ;
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s162f16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s162f16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s162f16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s162f16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s162f16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_s162f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322f32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322f32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322f32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322f32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322f32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) bfloat16_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322bf16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322bf16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322bf16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322bf16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322bf16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:


              ;
            break;
        case RoundMode::CAST_NONE:


              ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int64_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s64r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s64f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s64c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s64a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s64z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) bfloat16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_bf162f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) bfloat16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_bf162s32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_bf162s32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_bf162s32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_bf162s32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_bf162s32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:


              ;
            break;
        case RoundMode::CAST_NONE:


              ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s16r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s16f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s16c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s16a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s16z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s162f32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s322s16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int64_t* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s322s64(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int64_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s642f32r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s642f32f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s642f32c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s642f32a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s642f32z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) int64_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s642s32(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int4b_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s4r(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s4f(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s4c(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s4a(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s4z(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162s4(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                        ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int4b_t* src, const RoundMode& roundMode,
    uint8_t repeat, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s42f16(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}


template <typename U, typename T>
[aicore] static __inline__ __attribute__((always_inline)) void CheckCastDatatype() {
# 810 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vconv_impl.h"
                                                       ;
}


template <typename U, typename T>
[aicore] __inline__ __attribute__((always_inline)) void CastImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src, const RoundMode& roundMode,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<U, T>();
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(U) > sizeof(T)) {
            if constexpr (IsSameType<T, int4b_t>::value) {
                CastIntrinsicsImpl(
                    dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, ONE_FOURTH_DEFAULT_REPEAT_STRIDE});
            } else {
                CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2});
            }
        } else if constexpr (sizeof(U) < sizeof(T)) {
            if constexpr (IsSameType<U, int4b_t>::value) {
                CastIntrinsicsImpl(
                    dst, src, roundMode, 1, {1, 1, ONE_FOURTH_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            } else {
                CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE});
            }
        } else {
            CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename U, typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CastImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src, const RoundMode& roundMode,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<U, T>();
        if constexpr (isSetMask) {
            if (sizeof(U) >= sizeof(T)) {
                AscendCUtils::SetMask<T>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<U>(mask[1], mask[0]);
            }
        }
        CastIntrinsicsImpl(dst, src, roundMode, repeatTimes, repeatParams);
    }
}


template <typename U, typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void CastImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src, const RoundMode& roundMode,
    const uint64_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<U, T>();
        if constexpr (isSetMask) {
            if (sizeof(U) >= sizeof(T)) {
                AscendCUtils::SetMask<T>(mask);
            } else {
                AscendCUtils::SetMask<U>(mask);
            }
        }
        CastIntrinsicsImpl(dst, src, roundMode, repeatTimes, repeatParams);
    }
}

template <typename T, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) int16_t* src, uint8_t repeat,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr (halfBlock) {
        vconv_deqs162b8h(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    } else {
        vconv_deqs162b8l(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    }
}

template <typename T, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastVDeqIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) int16_t* src, uint8_t repeat,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr (halfBlock) {
        vconv_vdeqs162b8h(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    } else {
        vconv_vdeqs162b8l(dst, src, repeat, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    }
}


template <typename U, typename T, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        struct UnaryRepeatParams repeatParams;
        if constexpr (isVecDeq) {
            CastVDeqIntrinsicsImpl<U, halfBlock>(dst, src, 1, repeatParams);
        } else {
            CastDeqIntrinsicsImpl<U, halfBlock>(dst, src, 1, repeatParams);
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename U, typename T, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        if constexpr (isVecDeq) {
            CastVDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        } else {
            CastDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        }
    }
}


template <typename U, typename T, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src,
    const int32_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        if constexpr (isVecDeq) {
            CastVDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        } else {
            CastDeqIntrinsicsImpl<U, halfBlock>(dst, src, repeatTimes, repeatParams);
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeat,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<U, T>, Tuple<half, int8_t>, Tuple<float, half>, Tuple<int16_t, int8_t>>(),
        "Failed to check dtype in AddReluCast, current api support dtype combination is src: half, dst: int8_t; src: "
        "float, dst: half; src: int16_t, dst: int8_t.");
    if constexpr (SupportType<Tuple<U, T>, Tuple<half, int8_t>>()) {
        vaddreluconv_f162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else if constexpr (SupportType<Tuple<U, T>, Tuple<float, half>>()) {
        vaddreluconv_f322f16(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else {
        vaddreluconv_s162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask);
            }
        }
        AddReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask[], uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask[1], mask[0]);
            }
        }
        AddReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename DST_TYPE, typename SRC_TYPE>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(DST_TYPE) > sizeof(SRC_TYPE)) {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE / HALF_FACTOR, DEFAULT_REPEAT_STRIDE / HALF_FACTOR});
        } else if constexpr (sizeof(DST_TYPE) < sizeof(SRC_TYPE)) {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE / HALF_FACTOR,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        } else {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeat,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<U, T>, Tuple<half, int8_t>, Tuple<float, half>, Tuple<int16_t, int8_t>>(),
        "Failed to check dtype in SubReluCast, current api support dtype combination is src: half, dst: int8_t; src: "
        "float, dst: half; src: int16_t, dst: int8_t.");
    if constexpr (SupportType<Tuple<U, T>, Tuple<half, int8_t>>()) {
        vsubreluconv_f162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else if constexpr (SupportType<Tuple<U, T>, Tuple<float, half>>()) {
        vsubreluconv_f322f16(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else {
        vsubreluconv_s162s8(dst, src0, src1, repeat, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    }
}


template <typename DST_TYPE, typename SRC_TYPE>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (sizeof(DST_TYPE) > sizeof(SRC_TYPE)) {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE / HALF_FACTOR, DEFAULT_REPEAT_STRIDE / HALF_FACTOR});
        } else if constexpr (sizeof(DST_TYPE) < sizeof(SRC_TYPE)) {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE / HALF_FACTOR,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        } else {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask((uint64_t)-1, (uint64_t)-1);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask, uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask);
            }
        }
        SubReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}


template <typename DST_TYPE, typename SRC_TYPE, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCastImpl(__attribute__((cce_unif_buff)) DST_TYPE* dst, __attribute__((cce_unif_buff)) SRC_TYPE* src0, __attribute__((cce_unif_buff)) SRC_TYPE* src1,
    const uint64_t mask[], uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(DST_TYPE) >= sizeof(SRC_TYPE)) {
                AscendCUtils::SetMask<SRC_TYPE>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<DST_TYPE>(mask[1], mask[0]);
            }
        }
        SubReluCastIntrinsicsImpl(dst, src0, src1, repeatTimes, repeatParams);
    }
}

[aicore] __inline__ __attribute__((always_inline)) uint64_t MakeDeqScaleConfig(float scale, int16_t offset, bool signMode)
{
    constexpr uint64_t signModeBit = 46;
    constexpr uint64_t offsetMask = 0x1ff;
    constexpr uint64_t offsetBit = 37;
    uint64_t config = ((static_cast<uint64_t>(signMode) << signModeBit) | ((offset & offsetMask) << offsetBit) |
                       *(reinterpret_cast<uint32_t *>(&scale)));
    return config;
}

[aicore] __inline__ __attribute__((always_inline)) void SetDeqScaleImpl(float scale, int16_t offset, bool signMode)
{
    set_deqscale(MakeDeqScaleConfig(scale, offset, signMode));
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScaleImpl(const LocalTensor<T>& vdeqTensor, const VdeqInfo& vdeqInfo)
{
    for (uint8_t i = 0; i < VDEQ_TENSOR_SIZE; i++) {
        float scale = vdeqInfo.vdeqScale[i];
        int16_t offset = vdeqInfo.vdeqOffset[i];
        bool signMode = vdeqInfo.vdeqSignMode[i];
        vdeqTensor.SetValue(i, static_cast<T>(MakeDeqScaleConfig(scale, offset, signMode)));
    }



    constexpr uint64_t deqAddr = 5;
    set_deqscale(((uint64_t)vdeqTensor.GetPhyAddr()) >> deqAddr);

}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScaleImpl(T config)
{
    set_deqscale(config);
    gDeqValue = config;
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);


template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint32_t calCount);
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true, bool isVecDeq = true, bool halfBlock = true>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);

template <typename T1, typename T2, bool isSetMask = true, bool isVecDeq = true, bool halfBlock = true>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const int32_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams);
# 104 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isVecDeq = true, bool halfBlock = true>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint32_t calCount);
# 127 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);


template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);
# 145 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount);
# 168 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams);


template <typename T1, typename T2, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams);
# 186 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount);

#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(half scale);

[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(float scale, int16_t offset, bool signMode);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(const LocalTensor<T>& vdeqTensor, const VdeqInfo& vdeqInfo);
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vpadding_intf.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vpadding_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams);

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint32_t calCount);
}
#pragma end_pipe
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_scalar_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_scalar_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_scalar.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/kernel_scalar.h"
namespace AscendC {
template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetCountOfValueImpl(uint64_t valueIn)
{
    if constexpr (countValue == 1) {
        return bcnt1(valueIn);
    } else if constexpr (countValue == 0) {
        return bcnt0(valueIn);
    } else {
        static_assert(((countValue == 0) || (countValue == 1)) && "countValue must be 1 or 0");
        return 0;
    }
}

[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarCountLeadingZeroImpl(uint64_t valueIn)
{
    return clz(valueIn);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t CountBitsCntSameAsSignBitImpl(int64_t valueIn)
{
    return sflbits(valueIn);
}

template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetSFFValueImpl(uint64_t valueIn)
{
    if constexpr (countValue == 1) {
        return sff1(valueIn);
    } else if constexpr (countValue == 0) {
        return sff0(valueIn);
    } else {
        static_assert(((countValue == 0) || (countValue == 1)) && "countValue must be 1 or 0");
        return 0;
    }
}

template <RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) half ScalarCastF322F16Impl(float valueIn)
{
    switch (roundMode) {
        case RoundMode::CAST_ODD:
            return conv_f322f16o(valueIn);
        default:

                                                                                                                      ;
            return 0;
    }
}

template <RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) int32_t ScalarCastF322S32Impl(float valueIn)
{
    switch (roundMode) {
        case RoundMode::CAST_ROUND:
            return conv_f322s32a(valueIn);
        case RoundMode::CAST_CEIL:
            return conv_f322s32c(valueIn);
        case RoundMode::CAST_FLOOR:
            return conv_f322s32f(valueIn);
        case RoundMode::CAST_RINT:
            return conv_f322s32r(valueIn);
        default:

                                                                                                                      ;
            return 0;
    }
}

template <typename srcT, typename dstT, RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) dstT ScalarCastImpl(srcT valueIn)
{

    if constexpr (std::is_same<dstT, half>::value) {
        return ScalarCastF322F16Impl<roundMode>(valueIn);
    } else if constexpr (std::is_same<dstT, int32_t>::value) {
        return ScalarCastF322S32Impl<roundMode>(valueIn);
    } else {
        static_assert(((sizeof(dstT) == sizeof(half)) || (sizeof(dstT) == sizeof(int32_t))),
            "dstT only support half or int32_t");
        return 0;
    }




}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_scalar_intf.h" 2

namespace AscendC {
template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetCountOfValue(uint64_t valueIn);

[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarCountLeadingZero(uint64_t valueIn);

[aicore] __inline__ __attribute__((always_inline)) int64_t CountBitsCntSameAsSignBit(int64_t valueIn);

template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetSFFValue(uint64_t valueIn);

template <typename srcT, typename dstT, RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) dstT ScalarCast(srcT valueIn);
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_sys_var_intf.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_sys_var_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sys_var_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_sys_var_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockNumImpl()
{



    return get_subblockdim();

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSystemCycleImpl()
{







    uint64_t sysCnt = 0;
    asm volatile("MOV %0, SYS_CNT\n" : "+l"(sysCnt));
    return (int64_t)(sysCnt);

}

[aicore] __inline__ __attribute__((always_inline)) void GetArchVersionImpl(uint32_t& coreVersion)
{
    const int32_t coreVersionOffset = 32;
    coreVersion = (uint32_t)((uint64_t)(get_arch_ver() >> coreVersionOffset) & 0xFFF);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetProgramCounterImpl()
{
    const int32_t pcOffset = 16;
    int64_t pc = (get_pc() >> pcOffset) & 0xFFFFFFFFFFFF;
    return pc;
}

[aicore] __inline__ __attribute__((always_inline)) void TrapImpl()
{
    trap();
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_sys_var_intf.h" 2





namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GetArchVersion(uint32_t& coreVersion);

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockNum();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetProgramCounter();

[aicore] __inline__ __attribute__((always_inline)) void Trap();

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSystemCycle();
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_c220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_c220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_membase_impl.h" 2


namespace AscendC {
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void DropOutBitModeInit(const LocalTensor<T>& sharedTmpBuffer)
{

                                                                                        ;

    ResetMask();
    LocalTensor<int16_t> stackBuffer = sharedTmpBuffer.template ReinterpretCast<int16_t>();


    UnaryRepeatParams unaryParams;
    Muls<int16_t, false>(stackBuffer, stackBuffer, 0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetCmpMask<int16_t>(stackBuffer);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutBitModeInit()
{
    LocalTensor<int16_t> stackBuffer;
    bool ans = PopStackBuffer<int16_t, TPosition::LCM>(stackBuffer);

                                                                                                                      ;
    DropOutBitModeInit(stackBuffer);
}

template <typename T, bool isInitBitMode = false>
[aicore] __inline__ __attribute__((always_inline)) void DropOutBitMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const uint32_t dataSize)
{
    if constexpr (isInitBitMode == false) {
        DropOutBitModeInit(sharedTmpBuffer);
    }

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, dataSize);

    const BinaryRepeatParams binaryParams;
    Select<T, uint8_t>(dstLocal, maskLocal, srcLocal, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstLocal, dstLocal, static_cast<T>(divValue), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeCalc(const LocalTensor<half>& dstLocal, const LocalTensor<half>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const half divValue, const DropOutParams<half, float>& params)
{
    const LocalTensor<half>& stackBuffer = params.firstLocal;

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, uint8_t, false>(stackBuffer, maskLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<half, false>(dstLocal, stackBuffer, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    Muls<half, false>(dstLocal, dstLocal, divValue, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeCalc(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float divValue, const DropOutParams<half, float>& params)
{
    const LocalTensor<half>& firstLocal = params.firstLocal;
    const LocalTensor<float>& secondLocal = params.secondLocal;

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, uint8_t, false>(firstLocal, maskLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(secondLocal, firstLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<float, false>(dstLocal, secondLocal, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    Muls<float, false>(dstLocal, dstLocal, divValue, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeSetTmpBuffer(LocalTensor<half>& firstLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, DropOutParams<half, float>& params)
{
    firstLocal = sharedTmpBuffer.ReinterpretCast<half>();

    params.stackBufferSize = firstLocal.GetSize();
    params.stackBufferSize = params.stackBufferSize / ONE_BLK_SIZE * ONE_BLK_SIZE;

    params.maxRepeatSize = MAX_REPEAT_HALF_SIZE;
    params.oneRepeatSize = ONE_REPEAT_HALF_SIZE;
}

[aicore] __inline__ __attribute__((always_inline)) void DropOutByteModeSetTmpBuffer(LocalTensor<half>& firstLocal, LocalTensor<float>& secondLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, DropOutParams<half, float>& params)
{
    uint32_t popBufferLen = sharedTmpBuffer.GetSize();
    uint32_t cutBufLen = sizeof(float) + sizeof(half);
    params.stackBufferSize = popBufferLen / cutBufLen / ONE_BLK_SIZE * ONE_BLK_SIZE;

    firstLocal = sharedTmpBuffer.ReinterpretCast<half>();
    firstLocal.SetSize(params.stackBufferSize);

    secondLocal = sharedTmpBuffer[params.stackBufferSize * sizeof(half)].ReinterpretCast<float>();
    secondLocal.SetSize(params.stackBufferSize);

    params.maxRepeatSize = MAX_REPEAT_FLOAT_SIZE;
    params.oneRepeatSize = ONE_REPEAT_FLOAT_SIZE;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DropOutByteMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const uint32_t dataSize)
{
    DropOutParams<half, float> params;
    params.dataSize = dataSize;

    if constexpr (sizeof(T) == sizeof(half)) {
        DropOutByteModeSetTmpBuffer(params.firstLocal, sharedTmpBuffer, params);
    } else {
        DropOutByteModeSetTmpBuffer(params.firstLocal, params.secondLocal, sharedTmpBuffer, params);
    }

    const uint32_t round = params.dataSize / params.stackBufferSize;
    const uint32_t tail = params.dataSize % params.stackBufferSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackBufferSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        DropOutByteModeCalc(dstLocal[offset], srcLocal[offset], maskLocal[offset], divValue, params);
        offset = offset + params.stackBufferSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);
        DropOutByteModeCalc(dstLocal[offset], srcLocal[offset], maskLocal[offset], divValue, params);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_c220_impl.h" 2

namespace AscendC {
template <typename T, bool isInitBitMode = false>
[aicore] __inline__ __attribute__((always_inline)) void DropOutBitMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const DropOutShapeInfo& info)
{
    if constexpr (isInitBitMode == false) {
        DropOutBitModeInit(sharedTmpBuffer);
    }

    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = info.firstAxis;
    reducev2Params.src0RepeatStride = info.maskLastAxis / ONE_BLK_SIZE;

    LocalTensor<uint16_t> maskTmpLocal = maskLocal.ReinterpretCast<uint16_t>();

    const uint32_t mask = info.srcLastAxis / ONE_BYTE_BIT_SIZE / sizeof(uint16_t);
    uint64_t rsvdCnt = 0;

    GatherMask<uint16_t>(maskTmpLocal, maskTmpLocal, REDUCEV2_MODE_SEVEN, true, mask, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();

    DropOutBitMode<T, true>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, divValue,
        info.firstAxis * info.srcLastAxis);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DropOutByteMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const DropOutShapeInfo& info)
{
    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = info.firstAxis;
    reducev2Params.src0RepeatStride = info.maskLastAxis / ONE_BLK_SIZE;

    LocalTensor<uint16_t> maskTmpLocal = maskLocal.ReinterpretCast<uint16_t>();

    const uint32_t mask = info.srcLastAxis / sizeof(uint16_t);
    uint64_t rsvdCnt = 0;

    GatherMask<uint16_t>(maskTmpLocal, maskTmpLocal, REDUCEV2_MODE_SEVEN, true, mask, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();

    DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, divValue, info.firstAxis * info.srcLastAxis);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/../../impl/filter/dropout/dropout_impl.h" 2




namespace AscendC {
#pragma begin_pipe(V)
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOutOpt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
    float divValue = 1.0;
    divValue = divValue / keepProb;

    const uint32_t dataSize = info.firstAxis * info.srcLastAxis;

    if constexpr (dropOutMode == DROPOUT_MODE_BYTE_MISALIGN) {
        DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, static_cast<T>(divValue), info);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BYTE_ALIGN) {
        DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, static_cast<T>(divValue), dataSize);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BIT_ALIGN) {
        DropOutBitMode<T, isInitBitMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, static_cast<T>(divValue),
            dataSize);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BIT_MISALIGN) {
        DropOutBitMode<T, isInitBitMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, static_cast<T>(divValue),
            info);
    }
}

template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOutImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
                                 ;
                                                                                                   ;
                                                                                                       ;
                                                                                                         ;

                                                                                        ;

    if constexpr (dropOutMode != 0) {
        DropOutOpt<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
    } else if (info.srcLastAxis < info.maskLastAxis) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BYTE_MISALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer,
            keepProb, info);
    } else if (info.srcLastAxis == info.maskLastAxis) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BYTE_ALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb,
            info);
    } else if (info.srcLastAxis == (info.maskLastAxis * ONE_BYTE_BIT_SIZE)) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BIT_ALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb,
            info);
    } else {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BIT_MISALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer,
            keepProb, info);
    }
                                ;
}

template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOutImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float keepProb, const DropOutShapeInfo& info)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
}
#pragma end_pipe
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h"
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOut(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/filter/dropout.h"
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DropOut(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float keepProb, const DropOutShapeInfo& info)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, keepProb, info);
}
#pragma end_pipe
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_tiling/kernel_tiling.h" 1








#pragma pack(push, 8)
struct LogSoftMaxTiling {
    uint32_t srcM = 0;
    uint32_t srcK = 0;
    uint32_t srcSize = 0;
    uint32_t outMaxM = 0;
    uint32_t outMaxK = 0;
    uint32_t outMaxSize = 0;
    uint32_t splitM = 0;
    uint32_t splitK = 0;
    uint32_t splitSize = 0;
    uint32_t reduceM = 0;
    uint32_t reduceK = 0;
    uint32_t reduceSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t tailSplitSize = 0;
    uint32_t tailReduceSize = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct SoftMaxTiling {
    uint32_t srcM = 0;
    uint32_t srcK = 0;
    uint32_t srcSize = 0;
    uint32_t outMaxM = 0;
    uint32_t outMaxK = 0;
    uint32_t outMaxSize = 0;
    uint32_t splitM = 0;
    uint32_t splitK = 0;
    uint32_t splitSize = 0;
    uint32_t reduceM = 0;
    uint32_t reduceK = 0;
    uint32_t reduceSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t tailSplitSize = 0;
    uint32_t tailReduceSize = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2ServerCfg {
    uint32_t version = 0;
    uint8_t debugMode = 0;
    uint8_t sendArgIndex = 0;
    uint8_t recvArgIndex = 0;
    uint8_t commOutArgIndex = 0;
    uint8_t reserved[8] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2HcommCfg {
    uint8_t skipLocalRankCopy = 0;
    uint8_t skipBufferWindowCopy = 0;
    uint8_t stepSize = 0;
    char reserved[13] = {};
    char groupName[128] = {};
    char algConfig[128] = {};
    uint32_t opType = 0;
    uint32_t reduceType = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TCubeTiling {
    int32_t usedCoreNum = 0;
    int32_t M = 0;
    int32_t N = 0;
    int32_t Ka = 0;
    int32_t Kb = 0;
    int32_t singleCoreM = 0;
    int32_t singleCoreN = 0;
    int32_t singleCoreK = 0;
    int32_t baseM = 0;
    int32_t baseN = 0;
    int32_t baseK = 0;
    int32_t depthA1 = 0;
    int32_t depthB1 = 0;
    int32_t stepM = 0;
    int32_t stepN = 0;
    int32_t isBias = 0;
    int32_t transLength = 0;
    int32_t iterateOrder = 0;
    int32_t shareMode = 0;
    int32_t shareL1Size = 0;
    int32_t shareL0CSize = 0;
    int32_t shareUbSize = 0;
    int32_t batchM = 0;
    int32_t batchN = 0;
    int32_t singleBatchM = 0;
    int32_t singleBatchN = 0;
    int32_t stepKa = 0;
    int32_t stepKb = 0;
    int32_t depthAL1CacheUB = 0;
    int32_t depthBL1CacheUB = 0;
    int32_t dbL0A = 0;
    int32_t dbL0B = 0;
    int32_t dbL0C = 0;
    int32_t ALayoutInfoB = 0;
    int32_t ALayoutInfoS = 0;
    int32_t ALayoutInfoN = 0;
    int32_t ALayoutInfoG = 0;
    int32_t ALayoutInfoD = 0;
    int32_t BLayoutInfoB = 0;
    int32_t BLayoutInfoS = 0;
    int32_t BLayoutInfoN = 0;
    int32_t BLayoutInfoG = 0;
    int32_t BLayoutInfoD = 0;
    int32_t CLayoutInfoB = 0;
    int32_t CLayoutInfoS1 = 0;
    int32_t CLayoutInfoN = 0;
    int32_t CLayoutInfoG = 0;
    int32_t CLayoutInfoS2 = 0;
    int32_t BatchNum = 0;
    int32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct BatchNormTiling {
    uint32_t originalBLength = 0;
    uint32_t meanVarSize = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t shCurLength = 0;
    float firstDimValueBack = 0;
    uint32_t castHalfRepStride = 0;
    uint32_t shCurLengthBlockNum = 0;
    uint32_t castHalfOutRepStride = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct DeepNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float lastDimValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormGradBetaTiling {
    uint32_t stackBufferSize = 0;
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t bshLength = 0;
    uint32_t bsLength = 0;
    uint32_t oneCalSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t bsTailSize = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    uint32_t gammaTempTensorPos = 0;
    uint32_t betaTempTensorPos = 0;
    uint32_t inputDyTmpTensorPos = 0;
    uint32_t resForGammaTmpTensorPos = 0;
    uint32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormGradTiling {
    uint32_t stackBufferSize = 0;
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t oneCalSize = 0;
    uint32_t nohCalSize = 0;
    uint32_t loopNum = 0;
    uint32_t tailSize = 0;
    uint32_t nohTailSize = 0;
    uint32_t tmpTensorBSHPos = 0;
    uint32_t tmpTensorBSHSize = 0;
    uint32_t pdVarTensorPos = 0;
    uint32_t pdVarTensorSize = 0;
    uint32_t pdMeanTensorPos = 0;
    uint32_t pdMeanTensorSize = 0;
    uint32_t x1TensorPos = 0;
    uint32_t x1TensorSize = 0;
    uint32_t x2TensorPos = 0;
    uint32_t x2TensorSize = 0;
    uint32_t x3TensorPos = 0;
    uint32_t x3TensorSize = 0;
    uint32_t tmpTensorPos = 0;
    uint32_t tmpTensorSize = 0;
    uint32_t tmpTensor1Pos = 0;
    uint32_t tmpTensor1Size = 0;
    uint32_t tmpTensor2Pos = 0;
    uint32_t tmpTensor2Size = 0;
    uint32_t lastDimValueBack = 0;
    uint32_t lastDimValueBackMulTwo = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float lastDimValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct RmsNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    float reciprocalOfHLength = 0;
    uint32_t mainBshLength = 0;
    uint32_t mainBsLength = 0;
    uint32_t mainBsLengthAlign = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailPos = 0;
    uint32_t tailBshLength = 0;
    uint32_t tailBsLength = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct UnPadTiling {
    uint32_t srcHeight = 0;
    uint32_t srcWidth = 0;
    uint32_t tmpBuffer1BlockNum = 0;
    uint32_t tmpBuffer1RowNum = 0;
    uint32_t tmpBuffer2Offset = 0;
    uint32_t widthTiling = 0;
    uint32_t widthFractal = 0;
    uint32_t widthFractalTail = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct PadTiling {
    uint32_t srcHeight = 0;
    uint32_t srcWidth = 0;
    uint32_t srcOriWidth = 0;
    uint32_t widthWithoutLastBlock = 0;
    uint32_t blocksPerRow = 0;
    uint32_t heightTiling = 0;
    uint32_t heightFractal = 0;
    uint32_t heightFractalTail = 0;
    uint32_t mainLoopOffset = 0;
    uint32_t tailBlockOffset = 0;
    uint32_t tmpBuffer1BlockNum = 0;
    uint32_t tmpBuffer1RowNum = 0;
    uint32_t tmpBuffer2Offset = 0;
    uint32_t widthTiling = 0;
    uint32_t widthFractal = 0;
    uint32_t widthFractalTail = 0;
    uint32_t widthFractalTailAlingned = 0;
    uint32_t brcbTiling = 0;
    uint32_t brcbFractal = 0;
    uint32_t brcbFractalTail = 0;
    uint32_t maxRepeatTimes = 0;
    uint32_t brcbTilingRepeatTimes = 0;
    uint32_t brcbTilingRepeatTimesTail = 0;
    uint32_t brcbFractalTailRepeatTimes = 0;
    uint32_t brcbFractalTailRepeatTimesTail = 0;
    uint32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TopkTiling {
    int32_t tmpLocalSize = 0;
    int32_t allDataSize = 0;
    int32_t innerDataSize = 0;
    uint32_t sortRepeat = 0;
    int32_t mrgSortRepeat = 0;
    int32_t kAlignFourBytes = 0;
    int32_t kAlignTwoBytes = 0;
    int32_t maskOffset = 0;
    int32_t maskVreducev2FourBytes = 0;
    int32_t maskVreducev2TwoBytes = 0;
    int32_t mrgSortSrc1offset = 0;
    int32_t mrgSortSrc2offset = 0;
    int32_t mrgSortSrc3offset = 0;
    int32_t mrgSortTwoQueueSrc1Offset = 0;
    int32_t mrgFourQueueTailPara1 = 0;
    int32_t mrgFourQueueTailPara2 = 0;
    int32_t srcIndexOffset = 0;
    uint32_t copyUbToUbBlockCount = 0;
    int32_t topkMrgSrc1MaskSizeOffset = 0;
    int32_t topkNSmallSrcIndexOffset = 0;
    uint32_t vreduceValMask0 = 0;
    uint32_t vreduceValMask1 = 0;
    uint32_t vreduceIdxMask0 = 0;
    uint32_t vreduceIdxMask1 = 0;
    uint16_t vreducehalfValMask0 = 0;
    uint16_t vreducehalfValMask1 = 0;
    uint16_t vreducehalfValMask2 = 0;
    uint16_t vreducehalfValMask3 = 0;
    uint16_t vreducehalfValMask4 = 0;
    uint16_t vreducehalfValMask5 = 0;
    uint16_t vreducehalfValMask6 = 0;
    uint16_t vreducehalfValMask7 = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct ConfusionTransposeTiling {
    uint32_t param0 = 0;
    uint32_t param1 = 0;
    uint32_t param2 = 0;
    uint32_t param3 = 0;
    uint32_t param4 = 0;
    uint32_t param5 = 0;
    uint32_t param6 = 0;
    uint32_t param7 = 0;
    uint32_t param8 = 0;
    uint32_t param9 = 0;
    uint32_t param10 = 0;
    uint32_t param11 = 0;
    uint32_t param12 = 0;
    uint32_t param13 = 0;
    uint32_t param14 = 0;
    uint32_t param15 = 0;
    uint32_t param16 = 0;
    uint32_t param17 = 0;
};
#pragma pack(pop)
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_v220_impl.h"
namespace AscendC {
template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidIntrinsicsImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& stackBuffer)
{
    struct UnaryRepeatParams repeatParams;
    struct BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, src, static_cast<T>(-1.0), MASK_PLACEHOLDER, 1, repeatParams);
    PipeBarrier<PIPE_V>();
    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, repeatParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(dst, dst, static_cast<T>(1), MASK_PLACEHOLDER, 1, repeatParams);
    Duplicate<T, false>(stackBuffer, static_cast<T>(1.0), MASK_PLACEHOLDER, 1,
        DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, stackBuffer, dst, MASK_PLACEHOLDER, 1, binaryRepeatParams);
    PipeBarrier<PIPE_V>();
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }



                                                                        ;



      ;

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(T);


      ;

    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        SigmoidIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        SigmoidIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }

    SetMaskNorm();
    ResetMask();
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/sigmoid/sigmoid_common_impl.h" 2






namespace AscendC {
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SigmoidImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    LocalTensor<uint8_t> stackBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackBuffer);

                                                                          ;

    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, stackBuffer, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sigmoid<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/sigmoid.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sigmoid<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_common.h"
namespace AscendC {
constexpr uint8_t SOFTMAX_BASIC_TILE_NUM = 8;
constexpr uint8_t SOFTMAX_COMPUTE_DIM = 2;
constexpr uint8_t SOFTMAXGRAD_COMPUTE_DIM = 3;
constexpr uint8_t SOFTMAXFLASH_COMPUTE_DIM = 4;
constexpr uint8_t SOFTMAX_INNER_SHAPE_DIM = 2;
constexpr uint32_t FLOAT_NUM_PER_BLK = ONE_BLK_SIZE / B32_BYTE_SIZE;
constexpr uint32_t HALF_NUM_PER_BLK = ONE_BLK_SIZE / B16_BYTE_SIZE;
constexpr uint32_t HALF_REPEAT_STRIDE = DEFAULT_REPEAT_STRIDE / B16_BYTE_SIZE;
constexpr uint32_t SCALAR_STACK_DEPTH = 8;
constexpr uint32_t SOFTMAX_SHAPE_NZ_BASIC_COUNT = 16;
constexpr uint32_t SOFTMAX_MAX_REPEAT_STRIDE = MAX_REPEAT_TIMES * DEFAULT_REPEAT_STRIDE;
constexpr uint32_t SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
constexpr uint32_t SOFTMAX_MAX_REPEAT_CLC_HALF_NUM = MAX_REPEAT_TIMES * HALF_REPEAT_SIZE;
constexpr uint32_t SOFTMAX_SPECIAL_BASICBLOCK_LEN = FLOAT_REPEAT_SIZE * SOFTMAX_BASIC_TILE_NUM * SOFTMAX_COMPUTE_DIM;
constexpr uint32_t SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE = 192;
constexpr uint32_t SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN = DEFAULT_BLOCK_SIZE * HALF_FACTOR;
struct LastAxisShapeND {
    uint32_t m;
    uint32_t k;
};

struct ReduceLastND {
    uint32_t originalSrcM;
    uint32_t originalSrcK;
    uint32_t srcM;
    uint32_t srcK;
    uint32_t dstM;
    uint32_t dstK;
};

struct BroadCastLastND {
    uint32_t dstM;
    uint32_t dstK;
    uint32_t srcM;
    uint32_t srcK;
};

struct SoftMaxShapeInfo {
    uint32_t srcM{ 0 };
    uint32_t srcK{ 0 };
    uint32_t oriSrcM{ 0 };
    uint32_t oriSrcK{ 0 };
};
struct SoftmaxConfig {
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn)
    {
        isCheckTiling = isCheckTilingIn;
    }
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn, const uint32_t oriSrcMIn, const uint32_t oriSrcKIn)
    {
        isCheckTiling = isCheckTilingIn;
        oriSrcM = oriSrcMIn;
        oriSrcK = oriSrcKIn;
    }

    bool isCheckTiling = true;
    uint32_t oriSrcM = 0;
    uint32_t oriSrcK = 0;
};

constexpr SoftmaxConfig SOFTMAX_DEFAULT_CFG = { true, 0, 0 };

[aicore] __inline__ __attribute__((always_inline)) LastAxisShapeND GetLastAxisShapeND(const ShapeInfo& shapeInfo)
{
    uint32_t calculateSize = 1;
    LastAxisShapeND ndinfo;
    for (uint32_t i = 0; i < shapeInfo.shapeDim; i++) {
        calculateSize *= shapeInfo.shape[i];
    }


                                                                             ;
    ndinfo.k = shapeInfo.shape[shapeInfo.shapeDim - 1];

                                                                   ;
    ndinfo.m = calculateSize / ndinfo.k;
    return ndinfo;
}

[aicore] __inline__ __attribute__((always_inline)) LastAxisShapeND GetLastAxisOriginShapeND(const ShapeInfo& srcShapeInfo)
{
    uint32_t calculateSize = 1;
    LastAxisShapeND ndinfo;
    for (uint32_t i = 0; i < srcShapeInfo.originalShapeDim; i++) {
        calculateSize *= srcShapeInfo.originalShape[i];
    }


                                                                                                    ;
    ndinfo.k = srcShapeInfo.originalShape[srcShapeInfo.originalShapeDim - 1];

                                                                   ;
    ndinfo.m = calculateSize / ndinfo.k;
    return ndinfo;
}
[aicore] __inline__ __attribute__((always_inline)) constexpr uint32_t CalculateNDSplitM(const uint32_t workLocalSize, const uint32_t dataTypeSize,
    const uint32_t reduceK, const LastAxisShapeND& ndinfo, bool isBasicBlock = false)
{
    uint32_t splitM = 0;
    if (dataTypeSize == B16_BYTE_SIZE) {
        splitM = workLocalSize / (reduceK + ndinfo.k + FLOAT_REPEAT_SIZE);
    } else {
        splitM = workLocalSize / (reduceK + FLOAT_REPEAT_SIZE);
    }

    splitM = splitM < ndinfo.m ? splitM : ndinfo.m;

    if (isBasicBlock && (splitM > SOFTMAX_BASIC_TILE_NUM) && (ndinfo.m % SOFTMAX_BASIC_TILE_NUM == 0)) {
        splitM = splitM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (ndinfo.m % splitM != 0) {
            splitM -= SOFTMAX_BASIC_TILE_NUM;
        }

        while (splitM * ndinfo.k >= FLOAT_REPEAT_SIZE * DEFAULT_BLOCK_SIZE) {
            splitM = splitM / HALF_FACTOR;
        }
    }
    return splitM;
}
[aicore] __inline__ __attribute__((always_inline)) bool SoftMaxTilingFunc(const uint32_t workLocalSize, const SoftMaxShapeInfo& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, bool isBasicBlock = false,
    bool isDataFormatNZ = false)
{

                                                                                              ;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    const uint32_t srcM = ndinfo.srcM;
    const uint32_t srcK = ndinfo.srcK;
    const uint32_t oriSrcM = ndinfo.oriSrcM;

    softmaxTiling.srcM = srcM;
    softmaxTiling.srcK = srcK;
    softmaxTiling.srcSize = srcM * srcK;
    softmaxTiling.outMaxM = srcM;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = srcM * elementNumPerBlk;
    if (isDataFormatNZ) {
        softmaxTiling.reduceM = workLocalSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT + srcK);
    } else {
        softmaxTiling.reduceM = CalculateNDSplitM(workLocalSize, dataTypeSize1, elementNumPerBlk, {srcM, srcK},
            isBasicBlock);
    }

    if (softmaxTiling.reduceM < oriSrcM && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < oriSrcM ? softmaxTiling.reduceM : oriSrcM;
    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = srcK;
    softmaxTiling.splitSize = softmaxTiling.reduceM * srcK;

                                                                                              ;
    softmaxTiling.rangeM = oriSrcM / softmaxTiling.reduceM;
    softmaxTiling.tailM = oriSrcM % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * srcK;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) bool SoftMaxFlashTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t elementNumPerBlk, bool isUpdate = false, bool isBasicBlock = false)
{
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;

    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;

    if (!isUpdate) {
        softmaxTiling.reduceM =
            workLocalSize / (elementNumPerBlk * SOFTMAX_COMPUTE_DIM + ndinfo.k * SOFTMAX_COMPUTE_DIM);
    } else {
        softmaxTiling.reduceM =
            workLocalSize / (elementNumPerBlk * SOFTMAXFLASH_COMPUTE_DIM + ndinfo.k * SOFTMAX_COMPUTE_DIM);
    }

    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;

    if (isBasicBlock && (softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) &&
        (softmaxTiling.srcM % SOFTMAX_BASIC_TILE_NUM == 0)) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (softmaxTiling.srcM % softmaxTiling.reduceM != 0) {
            softmaxTiling.reduceM -= SOFTMAX_BASIC_TILE_NUM;
        }
    }

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                                   ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) bool SoftMaxGradTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t elementNumPerBlk, bool isFront = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false)
{
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;

    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;

    if (elementNumPerBlk != ONE_BYTE_BIT_SIZE) {
        softmaxTiling.reduceM = workLocalSize /
            (elementNumPerBlk * SOFTMAX_COMPUTE_DIM + ndinfo.k * SOFTMAXGRAD_COMPUTE_DIM + FLOAT_REPEAT_SIZE);
    } else {
        if (isFront && !isDataFormatNZ) {
            softmaxTiling.reduceM = workLocalSize / (elementNumPerBlk + ndinfo.k + FLOAT_REPEAT_SIZE);
        } else {
            softmaxTiling.reduceM =
                workLocalSize / (ndinfo.k + elementNumPerBlk * SOFTMAX_COMPUTE_DIM + FLOAT_REPEAT_SIZE);
        }
    }
    if (softmaxTiling.reduceM < ndinfo.m && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;

    if (isBasicBlock && isFront && (softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) &&
        (softmaxTiling.srcM % SOFTMAX_BASIC_TILE_NUM == 0)) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (softmaxTiling.srcM % softmaxTiling.reduceM != 0) {
            softmaxTiling.reduceM -= SOFTMAX_BASIC_TILE_NUM;
        }

        while (softmaxTiling.reduceM * ndinfo.k >= FLOAT_REPEAT_SIZE * DEFAULT_BLOCK_SIZE) {
            softmaxTiling.reduceM = softmaxTiling.reduceM / B16_BYTE_SIZE;
        }
    }

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                                  ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void TailMaxImpl(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = HALF_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Max(dst[i * HALF_REPEAT_SIZE], dst[i * HALF_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK], mask,
                1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Max(dst[i * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Max(dst[range * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
[aicore] __inline__ __attribute__((always_inline)) void TailMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = FLOAT_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Max(dst[i * FLOAT_REPEAT_SIZE], dst[i * FLOAT_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK],
                mask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Max(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Max(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
[aicore] __inline__ __attribute__((always_inline)) void TailAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = FLOAT_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Add(dst[i * FLOAT_REPEAT_SIZE], dst[i * FLOAT_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK],
                mask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Add(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Add(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AlignedBrcbImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t brcbCount)
{
    T scalarList[SCALAR_STACK_DEPTH] = {0};

    SetVectorMask<T>(brcbCount);
    for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
        scalarList[j] = srcLocal.GetValue(j);
    }
    for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
        Duplicate<T, false>(dstLocal[k * brcbCount], scalarList[k], MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ContinusColumnBrcbImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t& repeat, const uint32_t& brcbCount)
{
    float scalarList[SCALAR_STACK_DEPTH] = {0};
    SetVectorMask<float>(brcbCount);
    const uint32_t rangeM = repeat / SCALAR_STACK_DEPTH;
    const uint32_t tailM = repeat % SCALAR_STACK_DEPTH;
    uint32_t offset = 0;

    for (uint32_t i = 0; i < rangeM; i++) {
        offset = i * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j);
        }
        for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailM != 0) {
        offset = rangeM * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j);
        }
        for (uint32_t k = 0; k < tailM; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedColumnBrcbImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t& repeat, const uint32_t& brcbCount)
{
    float scalarList[SCALAR_STACK_DEPTH] = {0};
    SetVectorMask<float>(brcbCount);
    const uint32_t rangeM = repeat / SCALAR_STACK_DEPTH;
    const uint32_t tailM = repeat % SCALAR_STACK_DEPTH;
    uint32_t offset = 0;

    for (uint32_t i = 0; i < rangeM; i++) {
        offset = i * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j * brcbCount);
        }
        for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailM != 0) {
        offset = rangeM * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j * brcbCount);
        }
        for (uint32_t k = 0; k < tailM; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BroadCastNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t srcM)
{
    uint8_t repeat = srcM / DEFAULT_REPEAT_STRIDE;
    for (uint8_t i = 0; i < repeat; i++) {
        Muls<float, false>(dst[i * B16_BYTE_SIZE * FLOAT_REPEAT_SIZE], src[i * B16_BYTE_SIZE * FLOAT_REPEAT_SIZE], 1.0,
            MASK_PLACEHOLDER, B16_BYTE_SIZE, { 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
    PipeBarrier<PIPE_V>();

    uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE];
    for (int32_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (uint64_t)dst[i * FLOAT_NUM_PER_BLK].GetPhyAddr();
        srcList[i] = (uint64_t)src[i * FLOAT_NUM_PER_BLK].GetPhyAddr();
    }
    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = repeat;
    if (transDataParams.repeatTimes > 1) {
        transDataParams.dstRepStride = B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE;
        transDataParams.srcRepStride = B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE;
    }
    TransDataTo5HD<float>(dstList, srcList, transDataParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastLastCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const BroadCastLastND& brcParam, const uint32_t scalarStackDepth, const uint32_t index)
{
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    const uint32_t rangeK = brcParam.dstK / elementNumPerRep;
    const uint32_t tailK = brcParam.dstK % elementNumPerRep;
    T scalarList[SCALAR_STACK_DEPTH] = {0};

    for (uint32_t j = 0; j < scalarStackDepth; j++) {
        scalarList[j] = src[(index * SCALAR_STACK_DEPTH + j) * brcParam.srcK].GetValue(0);
    }
    for (uint32_t j = 0; j < rangeK; j++) {
        for (uint32_t k = 0; k < scalarStackDepth; k++) {
            Duplicate(dst[j * elementNumPerRep + (index * SCALAR_STACK_DEPTH + k) * brcParam.dstK], scalarList[k],
                elementNumPerRep, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailK != 0) {
        for (uint32_t k = 0; k < scalarStackDepth; k++) {
            Duplicate(dst[rangeK * elementNumPerRep + (index * SCALAR_STACK_DEPTH + k) * brcParam.dstK], scalarList[k],
                tailK, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CreateSpecialFormatMask(uint64_t& lowMask, const uint32_t& maskLen, const uint32_t& nzBlockCount)
{



                                                                              ;

                                                                                  ;

                                                                                   ;
    uint64_t defaultMask = 0xFFFF >> (SOFTMAX_SHAPE_NZ_BASIC_COUNT - maskLen);
    lowMask = defaultMask;

    for (uint32_t i = 0; i < nzBlockCount - 1; i++) {
        lowMask = lowMask << SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        lowMask = lowMask | defaultMask;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BinaryComputeWithSpecialMask(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, uint64_t mask[2], const uint32_t& lastBlockMaskLen, const uint32_t& splitCount,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const LocalTensor<float>&, uint64_t*,
    const uint8_t, const BinaryRepeatParams&))
{
    uint32_t repeat = splitCount / FLOAT_REPEAT_SIZE;
    uint32_t tail = splitCount % FLOAT_REPEAT_SIZE;

    uint32_t repeatRange = repeat / MAX_REPEAT_TIMES;
    uint32_t repeatTail = repeat % MAX_REPEAT_TIMES;
    const auto offsetCount = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
    uint32_t dstOffset = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;

    for (uint32_t i = 0; i < repeatRange; i++) {
        func(dst[i * offsetCount], src0[i * offsetCount], src1[i * offsetCount], mask, MAX_REPEAT_TIMES,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    if (repeatTail != 0) {
        func(dst[repeatRange * offsetCount], src0[repeatRange * offsetCount], src1[repeatRange * offsetCount], mask,
            repeatTail, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    if (tail != 0) {
        uint64_t tailMask[2] = { 0, 0 };
        CreateSpecialFormatMask(tailMask[0], lastBlockMaskLen, tail / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        func(dst[repeat * FLOAT_REPEAT_SIZE], src0[repeat * FLOAT_REPEAT_SIZE], src1[repeat * FLOAT_REPEAT_SIZE],
            tailMask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void UnaryComputeWithSpecialMask(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    uint64_t mask[2], const uint32_t& lastBlockMaskLen, const uint32_t& splitCount,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, uint64_t*, const uint8_t,
    const UnaryRepeatParams&))
{
    uint32_t repeat = splitCount / FLOAT_REPEAT_SIZE;
    uint32_t tail = splitCount % FLOAT_REPEAT_SIZE;

    uint32_t repeatRange = repeat / MAX_REPEAT_TIMES;
    uint32_t repeatTail = repeat % MAX_REPEAT_TIMES;
    const auto offsetCount = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
    uint32_t dstOffset = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;

    for (uint32_t i = 0; i < repeatRange; i++) {
        func(dst[i * offsetCount], src[i * offsetCount], mask, MAX_REPEAT_TIMES,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    if (repeatTail != 0) {
        func(dst[repeatRange * offsetCount], src[repeatRange * offsetCount], mask, repeatTail,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    if (tail != 0) {
        uint64_t tailMask[2] = { 0, 0 };
        CreateSpecialFormatMask(tailMask[0], lastBlockMaskLen, tail / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        func(dst[repeat * FLOAT_REPEAT_SIZE], src[repeat * FLOAT_REPEAT_SIZE], tailMask, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NextBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint8_t splitM, const uint8_t srcRepstride, const uint32_t splitBlock, const uint32_t srcK)
{
    if (splitM > splitBlock) {
        for (uint32_t i = HALF_FACTOR; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepstride });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Max<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[HALF_REPEAT_SIZE + j * srcK], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}
[aicore] __inline__ __attribute__((always_inline)) void NextBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint8_t splitM, const uint8_t srcRepstride, const uint32_t splitBlock, const uint32_t srcK)
{
    if (splitM > splitBlock) {
        for (uint32_t i = HALF_FACTOR; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepstride });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[HALF_REPEAT_SIZE + j * srcK], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);

    Max<float, false>(dst, src, src[FLOAT_NUM_PER_BLK], 1, 1,
        { B16_BYTE_SIZE, B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
    PipeBarrier<PIPE_V>();
    BlockReduceMax<float, false>(dst, dst, reduceParam.srcM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE);

    SetMaskNorm();
    ResetMask();
}
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);

    Add<float, false>(dst, src, src[FLOAT_NUM_PER_BLK], 1, 1,
        { B16_BYTE_SIZE, B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
    PipeBarrier<PIPE_V>();
    BlockReduceSum<float, false>(dst, dst, reduceParam.srcM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE);

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src, uint8_t splitM,
    uint8_t offset, const uint32_t splitBlock)
{
    Max<float, false>(dst, src, src[FLOAT_REPEAT_SIZE], 1, splitM, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
    if (splitM > splitBlock) {
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Max<float, false>(dst[j * FLOAT_REPEAT_SIZE],
                src[HALF_FACTOR * FLOAT_REPEAT_SIZE + splitBlock * FLOAT_REPEAT_SIZE * j], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src, uint8_t splitM,
    uint8_t offset, const uint32_t splitBlock)
{
    Add<float, false>(dst, src, src[FLOAT_REPEAT_SIZE], 1, splitM, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
    if (splitM > splitBlock) {
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Add<float, false>(dst[j * FLOAT_REPEAT_SIZE],
                src[HALF_FACTOR * FLOAT_REPEAT_SIZE + splitBlock * FLOAT_REPEAT_SIZE * j], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BigBlockReduceMax(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    for (uint32_t i = 0; i < splitM; i++) {
        BlockReduceMax<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * splitK], FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
            1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitBlock - FLOAT_NUM_PER_BLK;
    if (remainRepeat == 0) {
        return;
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitM; ++j) {
        Max<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * splitK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BigBlockReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    for (uint32_t i = 0; i < splitM; i++) {
        BlockReduceSum<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * splitK], FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
            1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitBlock - FLOAT_NUM_PER_BLK;
    if (remainRepeat == 0) {
        return;
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitM; ++j) {
        Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * splitK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GenericSubNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Sub<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Sub<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GenericDivNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Div<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Div<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GenericMulNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Mul<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Mul<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void TransDivToMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    const uint32_t curReduceSize = originalSrcM * srcReduceK;
    Duplicate(tmpbuffer, (float)1.0, curReduceSize);
    PipeBarrier<PIPE_V>();
    Div(tmpbuffer, tmpbuffer, src, curReduceSize);
    PipeBarrier<PIPE_V>();
    GenericMulNDImpl(dst, dst, tmpbuffer, originalSrcM, srcK, srcReduceK);
}
};
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_common_impl.h"
namespace AscendC {
constexpr RoundMode FLOAT2HALF_ROUND_MODE = RoundMode::CAST_ROUND;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastLastImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const struct BroadCastLastND& brcParam)
{


    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint64_t lowMask =
        brcParam.srcM * elementNumPerBlk;
    uint16_t repeat = brcParam.dstK / elementNumPerBlk;
    uint16_t srcBlkStride = brcParam.srcK / elementNumPerBlk;
    uint64_t mask[2] = { lowMask, 0 };

    uint32_t range = repeat / MAX_REPEAT_TIMES;
    uint32_t tail = repeat % MAX_REPEAT_TIMES;

    SetMaskCount();
    for (uint32_t i = 0; i < range; i++) {
        Copy<T>(dst[i * elementNumPerBlk * MAX_REPEAT_TIMES], src, mask, MAX_REPEAT_TIMES,
            { repeat, srcBlkStride, 1, 0 });
    }
    if (tail != 0) {
        Copy<T>(dst[range * elementNumPerBlk * MAX_REPEAT_TIMES], src, mask, tail, { repeat, srcBlkStride, 1, 0 });
    }

    SetMaskNorm();
    ResetMask();
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_common_impl.h"
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxLastNDSplitImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam, uint64_t mask, uint32_t splitNum)
{
    uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;

    for (uint32_t i = 0; i < range; i++) {
        WholeReduceMax(dst[i * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK, ReduceOrder::ORDER_ONLY_VALUE);
    }
    if (tail != 0) {
        WholeReduceMax(dst[range * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK, ReduceOrder::ORDER_ONLY_VALUE);
    }
}
[aicore] __inline__ __attribute__((always_inline)) void AlignedReduceMaxNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceMaxParam, const uint32_t splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_REPEAT_SIZE);
    BlockReduceMax<float, false>(tmpTensor, src, 1, MASK_PLACEHOLDER, 1, 1, reduceMaxParam.srcK / FLOAT_NUM_PER_BLK);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    DataCopy(dst, tmpTensor, { 1, (uint16_t)reduceMaxParam.srcM, 0, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 1; i < splitCount; i++) {
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_REPEAT_SIZE);
        BlockReduceMax<float, false>(tmpTensor, src[i * FLOAT_REPEAT_SIZE], 1, MASK_PLACEHOLDER, 1, 1,
            reduceMaxParam.srcK / FLOAT_NUM_PER_BLK);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
        Max<float, false>(dst, dst, tmpTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
    BlockReduceMax<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedReduceSumNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam, const uint32_t splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_REPEAT_SIZE);
    BlockReduceSum<float, false>(tmpTensor, src, 1, MASK_PLACEHOLDER, 1, 1, reduceParam.srcK / FLOAT_NUM_PER_BLK);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    DataCopy(dst, tmpTensor, { 1, (uint16_t)reduceParam.srcM, 0, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 1; i < splitCount; i++) {
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_REPEAT_SIZE);
        BlockReduceSum<float, false>(tmpTensor, src[i * FLOAT_REPEAT_SIZE], 1, MASK_PLACEHOLDER, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
        Add<float, false>(dst, dst, tmpTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
    BlockReduceSum<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SingleBlockBroadCastImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam)
{
    BrcbRepeatParams brcbParams;
    brcbParams.dstBlkStride = 1;
    brcbParams.dstRepStride = BRCB_BROADCAST_NUMBER;
    const uint32_t range = reduceParam.originalSrcM / BRCB_BROADCAST_NUMBER;
    const uint32_t tail = reduceParam.originalSrcM % BRCB_BROADCAST_NUMBER;

    if (range != 0) {
        if (reduceParam.dstK == BRCB_BROADCAST_NUMBER * HALF_FACTOR) {
            brcbParams.dstBlkStride = HALF_FACTOR;
            brcbParams.dstRepStride = BRCB_BROADCAST_NUMBER * HALF_FACTOR;
            Brcb(dst[0], src, range, brcbParams);
            Brcb(dst[BRCB_BROADCAST_NUMBER], src, range, brcbParams);
        } else {
            Brcb(dst, src, range, brcbParams);
        }
    }

    if (tail != 0) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        float scalarList[SCALAR_STACK_DEPTH] = {0};
        for (uint32_t j = 0; j < tail; j++) {
            scalarList[j] = src[(range * BRCB_BROADCAST_NUMBER + j)].GetValue(0);
        }

        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        for (uint32_t k = 0; k < tail; k++) {
            Duplicate(dst[(range * SCALAR_STACK_DEPTH + k) * reduceParam.dstK], scalarList[k], reduceParam.dstK, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxLastNDImpl(const LocalTensor<float>& dstMax, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceMaxParam)
{
    const uint32_t splitCount = reduceMaxParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceMaxParam.originalSrcK % FLOAT_REPEAT_SIZE;
    if (splitCount > 0) {
        AlignedReduceMaxNDImpl(tmpTensor, src, dstMax, reduceMaxParam, splitCount);
    }
    if (tailSrcK != 0) {
        ReduceMaxLastNDSplitImpl(dstMax, src, reduceMaxParam, tailSrcK, splitCount);
        PipeBarrier<PIPE_V>();
        if (splitCount == 0) {
            DataCopy(tmpTensor, dstMax, { 1, (uint16_t)reduceMaxParam.srcM, 0, 0 });
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
            Max<float, false>(tmpTensor, tmpTensor, dstMax, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            SetMaskNorm();
            ResetMask();
        }
    }

    PipeBarrier<PIPE_V>();
    SingleBlockBroadCastImpl(dstMax, tmpTensor, reduceMaxParam);
}

[aicore] __inline__ __attribute__((always_inline)) void AlignedBroadCastImpl(const LocalTensor<float>& dst, const LocalTensor<float>& tmpbuffer,
    const struct ReduceLastND& reduceParam)
{
    const uint32_t repeat = (reduceParam.originalSrcM + BRCB_BROADCAST_NUMBER - 1) / BRCB_BROADCAST_NUMBER;

    if (reduceParam.dstK == BRCB_BROADCAST_NUMBER * HALF_FACTOR) {
        if (reduceParam.originalSrcM != 1) {
            Brcb(tmpbuffer, dst, (uint8_t)repeat, { HALF_FACTOR, BRCB_BROADCAST_NUMBER * HALF_FACTOR });
            Brcb(tmpbuffer[BRCB_BROADCAST_NUMBER], dst, (uint8_t)repeat,
                { HALF_FACTOR, BRCB_BROADCAST_NUMBER * HALF_FACTOR });
        } else {
            Brcb(tmpbuffer, dst, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
            PipeBarrier<PIPE_V>();
            DataCopy(tmpbuffer[DEFAULT_REPEAT_STRIDE], tmpbuffer, { 1, 1, 0, 0 });
        }
    } else {
        Brcb(tmpbuffer, dst, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
    }
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * reduceParam.dstK);
    Copy<float, false>(dst, tmpbuffer, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void FirstBlockCopyImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t srcM, const uint32_t srcK, const uint16_t dstRepeatStride, const uint16_t srcRepeatStride)
{

    const uint32_t range = srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = srcM % MAX_REPEAT_TIMES;
    for (uint32_t i = 0; i < range; i++) {
        Copy<float, false>(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], src[i * MAX_REPEAT_TIMES * srcK],
            MASK_PLACEHOLDER, MAX_REPEAT_TIMES, { 1, 1, dstRepeatStride, srcRepeatStride });
    }
    if (tail != 0) {
        Copy<float, false>(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], src[range * MAX_REPEAT_TIMES * srcK],
            MASK_PLACEHOLDER, tail, { 1, 1, dstRepeatStride, srcRepeatStride });
    }





}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t reduceK)
{
    if (originalSrcM == 1) {
        WholeReduceMax<float, false>(dst, src, MASK_PLACEHOLDER, DEFAULT_REPEAT_STRIDE, 1, 1, 0,
            ReduceOrder::ORDER_ONLY_VALUE);
        if (reduceK == DEFAULT_REPEAT_STRIDE * HALF_FACTOR) {
            PipeBarrier<PIPE_V>();
            DataCopy(dst[DEFAULT_REPEAT_STRIDE], dst, { 1, 1, 0, 0 });
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_REPEAT_SIZE);
        BlockReduceMax<float, false>(src, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        BlockReduceMax<float, false>(dst, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NewReduceMaxLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    if (reduceParam.originalSrcK < FLOAT_REPEAT_SIZE) {
        ReduceMaxLastNDSplitImpl(dst, src, reduceParam, reduceParam.originalSrcK, 0);
        ResetMask();
    } else {
        if (reduceParam.originalSrcK >= SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN) {
            BigBlockReduceMax(tmpTensor, src, splitCount, reduceParam.originalSrcM, reduceParam.srcK);
        } else if (reduceParam.originalSrcK >= HALF_REPEAT_SIZE) {
            Max<float, false>(tmpTensor, src, src[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(reduceParam.originalSrcM),
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, (uint8_t)srcRepeatStride, (uint8_t)srcRepeatStride });
            NextBlockMaxImpl(tmpTensor, src, (uint8_t)(reduceParam.originalSrcM), srcRepeatStride, splitCount,
                reduceParam.srcK);
        } else {
            FirstBlockCopyImpl(tmpTensor, src, reduceParam.originalSrcM, reduceParam.srcK, DEFAULT_REPEAT_STRIDE,
                srcRepeatStride);
        }

        if (tailSrcK != 0) {
            PipeBarrier<PIPE_V>();
            TailMaxImpl(tmpTensor, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
            ResetMask();
        }
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(dst, tmpTensor, reduceParam.originalSrcM, reduceParam.dstK);
    }
    if (reduceParam.originalSrcM != 1 || reduceParam.originalSrcK <= FLOAT_REPEAT_SIZE) {
        PipeBarrier<PIPE_V>();
        AlignedBroadCastImpl(dst, tmpTensor, reduceParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumLastNDSplitImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam, uint64_t mask, uint32_t dstRepStride, uint32_t splitNum)
{
    uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;

    for (uint32_t i = 0; i < range; i++) {
        WholeReduceSum(dst[i * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
            dstRepStride, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceSum(dst[range * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail, dstRepStride,
            1, reduceParam.srcK / FLOAT_NUM_PER_BLK);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    if (splitCount > 0) {
        AlignedReduceSumNDImpl(tmpTensor, src, dst, reduceParam, splitCount);
    }

    if (tailSrcK != 0) {
        ReduceSumLastNDSplitImpl(dst, src, reduceParam, tailSrcK, 1, splitCount);
        PipeBarrier<PIPE_V>();
        if (splitCount == 0) {
            DataCopy(tmpTensor, dst, { 1, (uint16_t)reduceParam.srcM, 0, 0 });
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
            Add<float, false>(tmpTensor, tmpTensor, dst, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            SetMaskNorm();
            ResetMask();
        }
    }

    PipeBarrier<PIPE_V>();
    SingleBlockBroadCastImpl(dst, tmpTensor, reduceParam);
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t reduceK)
{
    if (originalSrcM == 1) {
        WholeReduceSum<float, false>(dst, src, MASK_PLACEHOLDER, DEFAULT_REPEAT_STRIDE, 1, 1, 0);
        if (reduceK == DEFAULT_REPEAT_STRIDE * HALF_FACTOR) {
            PipeBarrier<PIPE_V>();
            DataCopy(dst[DEFAULT_REPEAT_STRIDE], dst, { 1, 1, 0, 0 });
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_REPEAT_SIZE);
        BlockReduceSum<float, false>(src, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        BlockReduceSum<float, false>(dst, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void NewReduceSumLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    if (reduceParam.originalSrcK < FLOAT_REPEAT_SIZE) {
        ReduceSumLastNDSplitImpl(dst, src, reduceParam, reduceParam.originalSrcK, 1, 0);
        ResetMask();
    } else {
        if (reduceParam.originalSrcK >= SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN) {
            BigBlockReduceSum(tmpTensor, src, splitCount, reduceParam.originalSrcM, reduceParam.srcK);
        } else if (reduceParam.originalSrcK >= HALF_REPEAT_SIZE) {
            Add<float, false>(tmpTensor, src, src[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(reduceParam.originalSrcM),
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, (uint8_t)srcRepeatStride, (uint8_t)srcRepeatStride });
            NextBlockAddImpl(tmpTensor, src, (uint8_t)(reduceParam.originalSrcM), srcRepeatStride, splitCount,
                reduceParam.srcK);
        } else {
            FirstBlockCopyImpl(tmpTensor, src, reduceParam.originalSrcM, reduceParam.srcK, DEFAULT_REPEAT_STRIDE,
                srcRepeatStride);
        }

        if (tailSrcK != 0) {
            PipeBarrier<PIPE_V>();
            TailAddImpl(tmpTensor, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
            ResetMask();
        }
        PipeBarrier<PIPE_V>();
        BasicBlockReduceSumImpl(dst, tmpTensor, reduceParam.originalSrcM, reduceParam.dstK);
    }
    if (reduceParam.originalSrcM != 1 || reduceParam.originalSrcK <= FLOAT_REPEAT_SIZE) {
        PipeBarrier<PIPE_V>();
        AlignedBroadCastImpl(dst, tmpTensor, reduceParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxSingleBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint64_t& mask, const ReduceLastND& reduceParam)
{
    const uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;
    for (uint32_t j = 0; j < range; j++) {
        WholeReduceMax(dst[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, MAX_REPEAT_TIMES, DEFAULT_REPEAT_STRIDE, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceMax(dst[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, tail, DEFAULT_REPEAT_STRIDE, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SingleUnAlignedReduceMaxNZImpl(const LocalTensor<float>& tmpBuffer1,
    const LocalTensor<float>& tmpBuffer0, const uint32_t lastBlockMaskLen, const ReduceLastND& reduceParam)
{
    ReduceMaxSingleBlockNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    AlignedColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceMaxLastNZImpl(const LocalTensor<float>& tmpBuffer1, const LocalTensor<float>& tmpBuffer0,
    uint64_t mask[2], const ReduceLastND& reduceParam)
{
    const uint32_t splitNZBlockCount = reduceParam.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = reduceParam.dstM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitCount = reduceParam.originalSrcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    if (__builtin_expect(!!(splitNZBlockCount == 1 && lastBlockMaskLen != SOFTMAX_SHAPE_NZ_BASIC_COUNT), 0)) {
        SingleUnAlignedReduceMaxNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);
    } else {
        if (__builtin_expect(!!(splitNZBlockCount == 1), 0)) {
            ReduceMaxBlockNZImpl(tmpBuffer1, tmpBuffer0, reduceParam);
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
            Muls<float, false>(tmpBuffer1, tmpBuffer0, 1.0, MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            PipeBarrier<PIPE_V>();
            for (uint32_t j = 1; j < splitNZBlockCount - 1; j++) {
                Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
            }
            SetMaskNorm();
            ResetMask();

            BinaryComputeWithSpecialMask(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * (splitNZBlockCount - 1)],
                mask, lastBlockMaskLen, splitCount, Max<float>);

            PipeBarrier<PIPE_V>();
            ReduceMaxBlockNZImpl(tmpBuffer1, tmpBuffer1, reduceParam);
        }

        if (reduceParam.originalSrcM % DEFAULT_REPEAT_STRIDE == 0) {
            PipeBarrier<PIPE_V>();
            BroadCastNZImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM);
        } else {
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ContinusColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);
            ResetMask();
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumSingleBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint64_t& mask, const ReduceLastND& reduceParam)
{
    const uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;
    for (uint32_t j = 0; j < range; j++) {
        WholeReduceSum(dst[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, MAX_REPEAT_TIMES,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT, 1, SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceSum(dst[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, tail, SOFTMAX_SHAPE_NZ_BASIC_COUNT, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SingleUnAlignedReduceSumNZImpl(const LocalTensor<float>& tmpBuffer1,
    const LocalTensor<float>& tmpBuffer0, const uint32_t lastBlockMaskLen, const ReduceLastND& reduceParam)
{
    ReduceSumSingleBlockNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    AlignedColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumLastNZImpl(const LocalTensor<float>& tmpBuffer1, const LocalTensor<float>& tmpBuffer0,
    uint64_t mask[2], const struct ReduceLastND& reduceParam)
{
    const uint32_t splitOffset = reduceParam.dstM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitCount = reduceParam.originalSrcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = reduceParam.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    if (__builtin_expect(!!(splitNZBlockCount == 1 && lastBlockMaskLen != SOFTMAX_SHAPE_NZ_BASIC_COUNT), 0)) {
        SingleUnAlignedReduceSumNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);
    } else {
        if (__builtin_expect(!!(splitNZBlockCount == 1), 0)) {
            ReduceSumBlockNZImpl(tmpBuffer1, tmpBuffer0, reduceParam);
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
            Muls<float, false>(tmpBuffer1, tmpBuffer0, 1.0, MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            PipeBarrier<PIPE_V>();
            for (uint32_t j = 1; j < splitNZBlockCount - 1; j++) {
                Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
            }
            SetMaskNorm();
            ResetMask();

            BinaryComputeWithSpecialMask(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * (splitNZBlockCount - 1)],
                mask, lastBlockMaskLen, splitCount, Add<float>);

            PipeBarrier<PIPE_V>();
            ReduceSumBlockNZImpl(tmpBuffer1, tmpBuffer1, reduceParam);
        }

        if (reduceParam.originalSrcM % DEFAULT_REPEAT_STRIDE == 0) {
            PipeBarrier<PIPE_V>();
            BroadCastNZImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM);
        } else {
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ContinusColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);
            ResetMask();
        }
    }
}
};
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_impl.h" 2

namespace AscendC {
template <bool isFlashV2 = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    if constexpr (!isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
            mask, lastBlockMaskLen, splitCount, Div<float>);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <bool isFlashV2 = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            splitCount);
    }
    PipeBarrier<PIPE_V>();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    if constexpr (isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount; j++) {
            DataCopy(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
                splitCount);
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
                tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(
            dst[offset1 + (splitNZBlockCount - 1) * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1, mask, lastBlockMaskLen, splitCount, Div<float>);
    }
}

template <bool isFlashV2 = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();
    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();
    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    if constexpr (!isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
            mask, lastBlockMaskLen, splitCount, Div<float>);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNZImpl(const LocalTensor<T1>& dst, const LocalTensor<T1>& sumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNZImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_BLK_NUM, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], reduceSumBuffer, splitCeilM,
            { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        } else {
            BasicBlockAddImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], reduceSumBuffer, splitCeilM,
            { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_BLK_NUM });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxBasicBlock(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, src[offset1], (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, src[offset1], (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer2, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(dst[offset1], dst[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, dst[offset1], (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockAddImpl(tmpBuffer1, dst[offset1], (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer2, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(sumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], dst[offset1 + FLOAT_REPEAT_SIZE * j],
                sumTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericDivNDImpl(dst[offset1], dst[offset1], sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitSize + tiling.reduceSize];
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);

    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);
    PipeBarrier<PIPE_V>();

    Cast(sumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();
    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDExtImpl(const LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDImpl(const LocalTensor<T1>& dst, const LocalTensor<T1>& sumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    PipeBarrier<PIPE_V>();
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftMaxNDExtImpl<T1>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T1, half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T1, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftMaxNDExtImpl<T1>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    uint8_t stride = (uint8_t)(tiling.splitK / FLOAT_NUM_PER_BLK);
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(maxTensor[offset2], reduceSumBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockAddImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(sumTensor[offset2], reduceSumBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], sumTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDExtImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    PipeBarrier<PIPE_V>();
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftMaxNDExtImpl(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftMaxNDExtImpl(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SingleSoftMaxImpl(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const uint32_t& offset, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];

    Cast(tmpBuffer0, src[offset], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer1, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer1, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(tmpBuffer1, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer1, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SingleSoftMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const uint32_t& offset, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDImpl(tmpBuffer0, src[offset], tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(dst[offset], src[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    if constexpr(isReuseSource) {
        Exp(src[offset], dst[offset], splitSize);
        PipeBarrier<PIPE_V>();
        NewReduceSumLastNDImpl(tmpBuffer0, src[offset], tmpBuffer1, reduceParam);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset], src[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    } else {
        Exp(dst[offset], dst[offset], splitSize);
        PipeBarrier<PIPE_V>();
        NewReduceSumLastNDImpl(tmpBuffer0, dst[offset], tmpBuffer1, reduceParam);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset], dst[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    }
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    uint32_t offset = 0;
    uint32_t splitSize = tiling.splitSize;
    ReduceLastND reduceParam;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM,
            tiling.reduceK };
    } else {
        uint32_t splitK = 0;
        if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
            splitK = config.oriSrcK;
        } else {
            splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
        }
        if constexpr (SupportType<T, half>()) {
            reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
        } else if constexpr (SupportType<T, float>()) {
            reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
        }
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SingleSoftMaxImpl<isReuseSource>(dst, src, workLocal, tiling, offset, splitSize, reduceParam);
        offset += tiling.splitSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_base_impl.h" 2




namespace AscendC {
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
            newTiling, sizeof(T), sizeof(float), isBasicBlock);
        SoftMaxNDImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, originalSrcShape, newTiling);
    } else {
        SoftMaxNDImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, originalSrcShape, tiling);
    }
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, tiling, softmaxShapeInfo);
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {

        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || originalSrcShape.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), false, isDataFormatNZ);
            SoftMaxNZImpl<T1, T2, isBasicBlock>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, newTiling);
        } else {
            SoftMaxNZImpl<T1, T2, isBasicBlock>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling);
        }
    } else {

        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), isBasicBlock);
            SoftMaxNDImpl<T1, T2, isBasicBlock, config>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape,
                newTiling);
        } else {
            SoftMaxNDImpl<T1, T2, isBasicBlock, config>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape,
                tiling);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ, config>(dst, sumTensor, maxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ, config>(dst, sumTensor, maxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxResNZImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    bool isUpdateNeedCheck = false;
    const uint32_t splitNZBlockCount = softmaxShapeInfo.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetVectorMask<float>(SOFTMAX_SHAPE_NZ_BASIC_COUNT);
    for (uint32_t j = 0; j < softmaxShapeInfo.srcM; j++) {
        uint32_t offset = j * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        uint32_t splitCount = softmaxShapeInfo.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if constexpr (sizeof(T2) == sizeof(float)) {
            T2 maxValue = maxTensor.GetValue(j * ONE_BLK_FLOAT_NUM);
            uint32_t checkValue = *reinterpret_cast<uint32_t*>(&maxValue);
            if (checkValue == from) {
                for (uint32_t k = 0; k < splitNZBlockCount; k++) {
                    Duplicate<T1, false>(softMaxRes[offset + splitCount * k], to, MASK_PLACEHOLDER, 1, 1,
                        DEFAULT_REPEAT_STRIDE);
                }
                isUpdateNeedCheck = true;
            }
        } else {
            T2 maxValue = maxTensor.GetValue(j * ONE_BLK_HALF_NUM);
            uint16_t checkValue = *reinterpret_cast<uint16_t*>(&maxValue);
            if (checkValue == (uint16_t)from) {
                for (uint32_t k = 0; k < splitNZBlockCount; k++) {
                    Duplicate<T1, false>(softMaxRes[offset + splitCount * k], to, MASK_PLACEHOLDER, 1, 1,
                        DEFAULT_REPEAT_STRIDE);
                }
                isUpdateNeedCheck = true;
            }
        }
    }
    ResetMask();
    return isUpdateNeedCheck;
}

template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxResNDImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    bool isUpdateNeedCheck = false;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, softmaxShapeInfo.srcK);
    for (uint32_t j = 0; j < softmaxShapeInfo.srcM; j++) {
        if constexpr (sizeof(T2) == sizeof(float)) {
            T2 maxValue = maxTensor.GetValue(j * ONE_BLK_FLOAT_NUM);
            uint32_t checkValue = *reinterpret_cast<uint32_t*>(&maxValue);
            if (checkValue == from) {
                Duplicate<T1, false>(softMaxRes[j * softmaxShapeInfo.srcK], to, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                isUpdateNeedCheck = true;
            }
        } else {
            T2 maxValue = maxTensor.GetValue(j * ONE_BLK_HALF_NUM);
            uint16_t checkValue = *reinterpret_cast<uint16_t*>(&maxValue);
            if (checkValue == (uint16_t)from) {
                Duplicate<T1, false>(softMaxRes[j * softmaxShapeInfo.srcK], to, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                isUpdateNeedCheck = true;
            }
        }
    }
    SetMaskNorm();
    ResetMask();
    return isUpdateNeedCheck;
}

template <typename T1, typename T2, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxResImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    bool isUpdateNeedCheck = false;
    if constexpr (isDataFormatNZ) {
        isUpdateNeedCheck = AdjustSoftMaxResNZImpl<T1, T2>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
    } else {
        isUpdateNeedCheck = AdjustSoftMaxResNDImpl<T1, T2>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
    }
    return isUpdateNeedCheck;
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h" 2
#pragma begin_pipe(V)

namespace AscendC {
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor, tiling,
        softmaxShapeInfo);
                                ;
}
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<half, float, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor, tiling,
        softmaxShapeInfo);
                                ;
}
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dstTensor, srcTensor, tiling, softmaxShapeInfo);
                                ;
}
# 122 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dstTensor, srcTensor, sharedTmpBuffer, tiling,
        softmaxShapeInfo);
                                ;
}
# 154 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ,config>(dstTensor, sumTensor, maxTensor, srcTensor, sharedTmpBuffer,
        tiling, softmaxShapeInfo);
                                ;
}
# 185 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<half, float, isBasicBlock, isDataFormatNZ,config>(dstTensor, sumTensor, maxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
                                ;
}
# 213 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmax.h"
template <typename T1, typename T2, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) bool AdjustSoftMaxRes(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return false;
    }
    return AdjustSoftMaxResImpl<T1, T2, isDataFormatNZ>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
}
}

#pragma end_pipe
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/simple_softmax_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/simple_softmax_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/simple_softmax_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/simple_softmax_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.reduceSize + tiling.reduceSize];
    const uint16_t originalSrcM = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);

    DataCopyParams copyParams = { originalSrcM, 1, 0, 1 };
    DataCopy(tmpBuffer0, inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer0[FLOAT_NUM_PER_BLK], inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer1, inSumTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inSumTensor[offset2], copyParams);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer1, MASK_PLACEHOLDER,
            1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& inSumTensor,
    const LocalTensor<half>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    Cast<float, half, false>(tmpBuffer1, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    Cast<float, half, false>(tmpBuffer1, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t originalSrcM = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);

    DataCopyParams copyParams = { originalSrcM, 1, 0, 1 };
    DataCopy(tmpBuffer1, inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inMaxTensor[offset2], copyParams);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, inSumTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inSumTensor[offset2], copyParams);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxNZImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float> workLocal,
    const SoftMaxTiling& tiling)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SimpleSoftMaxGenericNZImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2, splitCount);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SimpleSoftMaxGenericNZImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2, splitCount);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;

        Cast<float, half, false>(tmpBuffer1, maxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Cast<float, half, false>(tmpBuffer1, expSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxBasicBlock(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
# 264 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/simple_softmax_impl.h"
    const uint32_t splitBlock = tiling.srcK / FLOAT_REPEAT_SIZE;
    const uint8_t repstride = tiling.srcK / FLOAT_NUM_PER_BLK;
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcM * FLOAT_REPEAT_SIZE);
    for (uint32_t j = 0; j < splitBlock; ++j) {
        Sub<float, false>(dst[FLOAT_REPEAT_SIZE * j], src[FLOAT_REPEAT_SIZE * j], maxTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 0, repstride, repstride, 1 });
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcSize);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(dst, dst, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcM * FLOAT_REPEAT_SIZE);
    for (uint32_t j = 0; j < splitBlock; ++j) {
        Div<float, false>(dst[FLOAT_REPEAT_SIZE * j], dst[FLOAT_REPEAT_SIZE * j], expSumTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 0, repstride, repstride, 1 });
    }
    SetMaskNorm();
    ResetMask();

}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<half>& inSumTensor,
    const LocalTensor<half>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const uint32_t splitSize = curSplitM * tiling.splitK;
    const uint32_t reduceSize = curSplitM * tiling.reduceK;

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    Cast(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    Cast(tmpBuffer2, inSumTensor[offset2], RoundMode::CAST_NONE, reduceSize);
    PipeBarrier<PIPE_V>();
    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const uint32_t splitSize = curSplitM * tiling.splitK;

    GenericSubNDImpl(dst[offset1], src[offset1], inMaxTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    GenericDivNDImpl(dst[offset1], dst[offset1], inSumTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& src, const LocalTensor<float> workLocal,
    const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SimpleSoftMaxBasicBlock(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
    } else {
        if constexpr (sizeof(T) == sizeof(float)) {
            SimpleSoftMaxGenericNDImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, 0, 0, tiling.srcM);
        } else {
            uint32_t offset1 = 0;
            uint32_t offset2 = 0;
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < tiling.rangeM; i++) {
                offset1 = i * tiling.splitSize;
                offset2 = i * tiling.reduceSize;
                SimpleSoftMaxGenericNDImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2,
                    tiling.splitM);
            }
            PipeBarrier<PIPE_V>();
            if (tiling.tailM != 0) {
                offset1 = tiling.rangeM * tiling.splitSize;
                offset2 = tiling.rangeM * tiling.reduceSize;
                SimpleSoftMaxGenericNDImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2,
                    tiling.tailM);
            }
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;

        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                inMaxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                inSumTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const uint32_t splitSize = curSplitM * tiling.splitK;

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, inMaxTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, tiling.splitSize);
    PipeBarrier<PIPE_V>();
    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, inSumTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SimpleSoftMaxBasicBlock(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
    } else {
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset1 = i * tiling.splitSize;
            offset2 = i * tiling.reduceSize;
            SimpleSoftMaxGenericNDImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2,
                tiling.splitM);
        }
        PipeBarrier<PIPE_V>();
        if (tiling.tailM != 0) {
            offset1 = tiling.rangeM * tiling.splitSize;
            offset2 = tiling.rangeM * tiling.reduceSize;
            SimpleSoftMaxGenericNDImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2,
                tiling.tailM);
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/simple_softmax_base_impl.h" 2




namespace AscendC {
template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), false, isDataFormatNZ);
            SimpleSoftMaxNZImpl<T1, T2>(dst, inSumTensor, inMaxTensor, src, workLocal, newTiling);
        } else {
            SimpleSoftMaxNZImpl<T1, T2>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), isBasicBlock);
            SimpleSoftMaxNDImpl<T1, isBasicBlock>(dst, inSumTensor, inMaxTensor, src, workLocal, newTiling);
        } else {
            SimpleSoftMaxNDImpl<T1, isBasicBlock>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SimpleSoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SimpleSoftMaxImpl<T1, T2, isBasicBlock, isDataFormatNZ>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ>(dstTensor, inSumTensor, inMaxTensor, srcTensor, tiling,
        softmaxShapeInfo);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<half, float, isBasicBlock, isDataFormatNZ>(dstTensor, inSumTensor, inMaxTensor, srcTensor, tiling,
        softmaxShapeInfo);
}
# 96 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<T, T, isBasicBlock, isDataFormatNZ>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
}
# 124 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SimpleSoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<half, float, isBasicBlock, isDataFormatNZ>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
}
}

#pragma end_pipe
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv2_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv2_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_flashv2_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_flashv2_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void FlashV2NZUpdateGenericImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    DataCopy(inSumTmp, inSumTensor[offset2], { 1, copyBlockCount, 0, 0 });
    DataCopy(inMaxTmp, inMaxTensor[offset2], { 1, copyBlockCount, 0, 0 });

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            splitCount);
    }
    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);
    Max<float, false>(maxTensor[offset2], inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(expMaxTensor[offset2], inMaxTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, maxTensor[offset2], { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], maxTensor[offset2], { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();

    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
            splitCount);
    }

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);

    Mul<float, false>(inMaxTmp, expMaxTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(sumTensor[offset2], inMaxTmp, sumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void FlashV2NZUpdateGenericImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    Cast<float, half, false>(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    Cast<float, half, false>(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    Max<float, false>(tmpBuffer1, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void FlashV2NZUpdateGenericImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    DataCopy(inMaxTmp, inMaxTensor[offset2], { 1, copyBlockCount, 0, 0 });
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);
    Max<float, false>(maxTensor[offset2], inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, inMaxTmp, { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inMaxTmp, { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
        1, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, maxTensor[offset2], { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], maxTensor[offset2], { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    DataCopy(inSumTmp, inSumTensor[offset2], { 1, copyBlockCount, 0, 0 });

    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);

    Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(sumTensor[offset2], inMaxTmp, sumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZNoUpdateImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT - paddingTailCount;
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZNoUpdateImpl(const LocalTensor<half>& dstTensor,
    const LocalTensor<float>& sumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT - paddingTailCount;
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZUpdateImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT - paddingTailCount;
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NZUpdateImpl(const LocalTensor<half>& dstTensor,
    const LocalTensor<float>& sumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<half>& expMaxTensor, const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT - paddingTailCount;
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxFlashV2NZImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NZNoUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NZUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor,
            inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxFlashV2NZImpl(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NZNoUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NZUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor,
            inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SpecialBasicBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint8_t splitM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
        BlockReduceMax<float, false>(tmpbuffer, src, splitM * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpbuffer, tmpbuffer, splitM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpbuffer, splitM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
    } else {
        BlockReduceMax<float, false>(tmpbuffer, src, HALF_FACTOR * FLOAT_REPEAT_SIZE, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpbuffer, tmpbuffer, HALF_FACTOR * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Max<float, false>(dst, tmpbuffer, tmpbuffer[FLOAT_NUM_PER_BLK], 1, 1,
            { 1, HALF_FACTOR, HALF_FACTOR, DEFAULT_REPEAT_STRIDE, B16_DATA_NUM_PER_BLOCK, B16_DATA_NUM_PER_BLOCK });
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SpecialBasicBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint8_t splitM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
        BlockReduceSum<float, false>(tmpbuffer, src, splitM * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpbuffer, tmpbuffer, splitM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpbuffer, splitM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
    } else {
        BlockReduceSum<float, false>(tmpbuffer, src, HALF_FACTOR * FLOAT_REPEAT_SIZE, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpbuffer, tmpbuffer, HALF_FACTOR * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Add<float, false>(dst, tmpbuffer, tmpbuffer[FLOAT_NUM_PER_BLK], 1, 1,
            { 1, HALF_FACTOR, HALF_FACTOR, DEFAULT_REPEAT_STRIDE, B16_DATA_NUM_PER_BLOCK, B16_DATA_NUM_PER_BLOCK });
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer, const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR || splitK == SOFTMAX_SPECIAL_BASICBLOCK_LEN) {
        SpecialBasicBlockMaxImpl(dst, src, tmpBuffer, (uint8_t)splitM, splitK);
    } else {
        const uint8_t splitCeilM = (uint8_t)(DivCeil(splitM, FLOAT_NUM_PER_BLK));
        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer, src, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else if (splitK > DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
            BigBlockReduceMax(tmpBuffer, src, splitBlock, splitM, splitK);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (splitK / FLOAT_REPEAT_SIZE));
            BasicBlockMaxImpl(tmpBuffer, src, (uint8_t)splitM, offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}
[aicore] __inline__ __attribute__((always_inline)) void BasicBlockReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer, const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR || splitK == SOFTMAX_SPECIAL_BASICBLOCK_LEN) {
        SpecialBasicBlockAddImpl(dst, src, tmpBuffer, (uint8_t)splitM, splitK);
    } else {
        const uint8_t splitCeilM = (uint8_t)(DivCeil(splitM, FLOAT_NUM_PER_BLK));
        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer, src, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else if (splitK > DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
            BigBlockReduceSum(tmpBuffer, src, splitBlock, splitM, splitK);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (splitK / FLOAT_REPEAT_SIZE));
            BasicBlockAddImpl(tmpBuffer, src, (uint8_t)splitM, offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2BasicBlockImpl(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inExpSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;

    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer3, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], tmpBuffer3, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<float, half, false>(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer3, tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Cast<half, float, false>(maxTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });


        Sub<float, false>(tmpBuffer2, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer2, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expMaxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(tmpBuffer3, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], tmpBuffer3, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        Cast<float, half, false>(tmpBuffer3, inExpSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Mul<float, false>(tmpBuffer3, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expSumTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2BasicBlockImpl(const LocalTensor<float>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& src,
    const LocalTensor<float>& expMaxTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, src[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Copy<float, false>(tmpBuffer2, inMaxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Max<float, false>(maxTensor[offset2], tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }


        Sub<float, false>(tmpBuffer2, tmpBuffer2, maxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(expMaxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Exp<float, false>(dst[offset1], dst[offset1], MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceSumImpl(tmpBuffer3, dst[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Mul<float, false>(inExpSumTensor[offset2], expMaxTensor[offset2], inExpSumTensor[offset2], MASK_PLACEHOLDER,
            reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(expSumTensor[offset2], inExpSumTensor[offset2], tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2UpdateImpl(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inExpSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    Cast(tmpBuffer1, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Max<float, false>(tmpBuffer3, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Sub<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer3, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitSize);

    Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    NewReduceSumLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Exp<float, false>(tmpBuffer1, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(expMaxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<float, half, false>(tmpBuffer2, inExpSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer2, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(expSumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2UpdateImpl(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling,
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDImpl(tmpBuffer0, src[offset1], tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Max<float, false>(tmpBuffer0, inMaxTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Sub<float, false>(tmpBuffer1, inMaxTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Exp<float, false>(expMaxTensor[offset2], tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
    GenericSubNDImpl(dst[offset1], src[offset1], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    DataCopy(maxTensor[offset2], tmpBuffer0, reduceSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(tmpBuffer0, dst[offset1], tmpBuffer1, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Mul<float, false>(expSumTensor[offset2], expMaxTensor[offset2], inExpSumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(expSumTensor[offset2], expSumTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NDImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SoftmaxFlashV2BasicBlockImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
            workLocal, tiling);
    } else {
        ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.reduceM, tiling.reduceK };
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        uint32_t splitSize = tiling.splitSize;
        uint32_t reduceSize = tiling.reduceSize;
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i <= tiling.rangeM; i++) {
            SoftmaxFlashV2UpdateImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, reduceParam, tiling, offset1, offset2, splitSize, reduceSize);
            offset1 += tiling.splitSize;
            offset2 += tiling.reduceSize;
            if (i == (tiling.rangeM - 1)) {
                if (tiling.tailM == 0) {
                    break;
                }
                offset2 = tiling.rangeM * tiling.reduceSize;
                offset1 = tiling.rangeM * tiling.splitSize;
                splitSize = tiling.tailSplitSize;
                reduceSize = tiling.tailReduceSize;
                reduceParam.originalSrcM = tiling.tailM;
                reduceParam.srcM = tiling.tailM;
                reduceParam.dstM = tiling.tailM;
                PipeBarrier<PIPE_V>();
            }
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<half>& dst,
    const LocalTensor<half>& expSumTensor, const LocalTensor<half>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceMaxImpl(reduceSumBuffer, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], reduceSumBuffer, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(reduceSumBuffer, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], reduceSumBuffer, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expSumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<float>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, src[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(dst[offset1], src[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceSumImpl(tmpBuffer2, dst[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
        Brcb(expSumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceBuffer = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(reduceBuffer, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, reduceBuffer, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], reduceBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(reduceBuffer, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    Cast(expSumTensor[offset2], reduceBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdate(const LocalTensor<T1>& dst, const LocalTensor<T1>& expSumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
    } else {
        PipeBarrier<PIPE_V>();
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        uint32_t splitSize = tiling.splitSize;
        uint32_t reduceSize = tiling.reduceSize;
        ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.reduceM, tiling.reduceK };
        for (uint32_t i = 0; i <= tiling.rangeM; i++) {
            SoftmaxFlashV2NoUpdateImpl(dst, expSumTensor, maxTensor, src, workLocal, reduceParam, tiling, offset1,
                offset2, splitSize, reduceSize);
            offset1 += tiling.splitSize;
            offset2 += tiling.reduceSize;
            if (i == (tiling.rangeM - 1)) {
                if (tiling.tailM == 0) {
                    break;
                }
                offset2 = tiling.rangeM * tiling.reduceSize;
                offset1 = tiling.rangeM * tiling.splitSize;
                splitSize = tiling.tailSplitSize;
                reduceSize = tiling.tailReduceSize;
                reduceParam.originalSrcM = tiling.tailM;
                reduceParam.srcM = tiling.tailM;
                reduceParam.dstM = tiling.tailM;
                PipeBarrier<PIPE_V>();
            }
        }
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2PostProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NoUpdate<T1, T2, isBasicBlock>(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NDImpl<T1, T2, isBasicBlock>(dstTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor,
            inExpSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2BasicBlock(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];

    const LocalTensor<float>& inMaxTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    const LocalTensor<float>& inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Max<float, false>(maxTensor[offset2], inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }

        Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();


        Copy<float, false>(tmpBuffer1, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { B16_BYTE_SIZE, 1, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE });
        Copy<float, false>(tmpBuffer1[DEFAULT_REPEAT_STRIDE], inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { B16_BYTE_SIZE, 1, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer1, FLOAT2HALF_ROUND_MODE,
            MASK_PLACEHOLDER, reduceCeilValue * B16_BYTE_SIZE, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceSumImpl(inSumTmp, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1, inSumTmp, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Mul<float, false>(inSumTmp, inMaxTmp, inExpSumTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(expSumTensor[offset2], inSumTmp, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<half>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();

        Brcb(expSumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2UpdateImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling,
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    DataCopy(tmpBuffer1, inMaxTensor[offset2], reduceSize);
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    Max(maxTensor[offset2], inMaxTensor[offset2], tmpBuffer3, reduceSize);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);

    NewReduceSumLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);

    Sub(tmpBuffer1, tmpBuffer1, maxTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer1, tmpBuffer1, reduceSize);
    PipeBarrier<PIPE_V>();

    BroadCastLastImpl(tmpBuffer0, tmpBuffer1,
        { tiling.reduceM, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE, tiling.reduceM, tiling.reduceK });
    PipeBarrier<PIPE_V>();
    Cast(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer0, FLOAT2HALF_ROUND_MODE, reduceSize * B16_BYTE_SIZE);

    Mul(tmpBuffer1, tmpBuffer1, inExpSumTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();
    Add(expSumTensor[offset2], tmpBuffer1, tmpBuffer3, reduceSize);
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
}


template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NoUpdate(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
    } else {
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        uint32_t splitSize = tiling.splitSize;
        uint32_t reduceSize = tiling.reduceSize;
        ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.reduceM, tiling.reduceK };
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i <= tiling.rangeM; i++) {
            SoftmaxFlashV2NoUpdateImpl(dst, expSumTensor, maxTensor, src, workLocal, reduceParam, tiling, offset1,
                offset2, splitSize, reduceSize);
            offset1 += tiling.splitSize;
            offset2 += tiling.reduceSize;
            if (i == (tiling.rangeM - 1)) {
                if (tiling.tailM == 0) {
                    break;
                }
                offset2 = tiling.rangeM * tiling.reduceSize;
                offset1 = tiling.rangeM * tiling.splitSize;
                splitSize = tiling.tailSplitSize;
                reduceSize = tiling.tailReduceSize;
                reduceParam.originalSrcM = tiling.tailM;
                reduceParam.srcM = tiling.tailM;
                reduceParam.dstM = tiling.tailM;
                PipeBarrier<PIPE_V>();
            }
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2NDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SoftmaxFlashV2BasicBlock(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
            workLocal, tiling);
    } else {
        ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.reduceM, tiling.reduceK };
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        uint32_t splitSize = tiling.splitSize;
        uint32_t reduceSize = tiling.reduceSize;
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i <= tiling.rangeM; i++) {
            SoftmaxFlashV2UpdateImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, reduceParam, tiling, offset1, offset2, splitSize, reduceSize);
            offset1 += tiling.splitSize;
            offset2 += tiling.reduceSize;
            if (i == (tiling.rangeM - 1)) {
                if (tiling.tailM == 0) {
                    break;
                }
                offset2 = tiling.rangeM * tiling.reduceSize;
                offset1 = tiling.rangeM * tiling.splitSize;
                splitSize = tiling.tailSplitSize;
                reduceSize = tiling.tailReduceSize;
                reduceParam.originalSrcM = tiling.tailM;
                reduceParam.srcM = tiling.tailM;
                reduceParam.dstM = tiling.tailM;
                PipeBarrier<PIPE_V>();
            }
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flashv2_base_impl.h" 2




namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) constexpr SoftMaxTiling SoftMaxFlashV2TilingFuncImpl(const uint32_t srcM, const uint32_t srcK,
    const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, const uint32_t localWorkSpaceSize,
    const bool isUpdate = false, const bool isBasicBlock = false, const bool isDataFormatNZ = false)
{
    SoftMaxTiling softmaxTiling;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    softmaxTiling.srcM = srcM;
    softmaxTiling.srcK = srcK;
    softmaxTiling.srcSize = srcM * srcK;

    softmaxTiling.outMaxM = srcM;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = srcM * elementNumPerBlk;

    if (isDataFormatNZ) {
        softmaxTiling.reduceM = localWorkSpaceSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT * HALF_FACTOR + srcK);
    } else {
        if (isBasicBlock && srcK % FLOAT_REPEAT_SIZE == 0 && srcM % SOFTMAX_BASIC_TILE_NUM == 0) {
            softmaxTiling.reduceM =
                CalculateNDSplitM(localWorkSpaceSize, dataTypeSize1, elementNumPerBlk, { srcM, srcK }, isBasicBlock);
        } else {
            softmaxTiling.reduceM = (dataTypeSize1 == B16_BYTE_SIZE) ?
                localWorkSpaceSize / (SOFTMAX_COMPUTE_DIM * elementNumPerBlk + srcK + FLOAT_REPEAT_SIZE) :
                localWorkSpaceSize / (elementNumPerBlk + FLOAT_REPEAT_SIZE);
        }
    }

    if (softmaxTiling.reduceM < srcM && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < srcM ? softmaxTiling.reduceM : srcM;

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = srcK;
    softmaxTiling.splitSize = softmaxTiling.reduceM * srcK;

    softmaxTiling.rangeM = srcM / softmaxTiling.reduceM;
    softmaxTiling.tailM = srcM % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * srcK;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return softmaxTiling;
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t workLocalSize = workLocal.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if constexpr (config.isCheckTiling) {
            if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
                SoftMaxTiling newTiling = SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2),
                    workLocalSize, isUpdate, false, true);
                SoftMaxFlashV2NZImpl<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
                    expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
            } else {
                SoftMaxFlashV2NZImpl<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
                    expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
            }
        } else {
            SoftMaxFlashV2NZImpl<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
                expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
        }
    } else {
        if constexpr (config.isCheckTiling) {
            if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
                SoftMaxTiling newTiling = SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2),
                    workLocalSize, isUpdate, isBasicBlock);
                SoftmaxFlashV2PostProcess<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
                    expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
            } else {
                SoftmaxFlashV2PostProcess<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
                    expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
            }
        } else {
            SoftmaxFlashV2PostProcess<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
                expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
        }
    }
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxFlashV2Impl<T1, T2, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxFlashV2Impl<T1, T2, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, tiling, softmaxShapeInfo);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
[aicore] __inline__ __attribute__((always_inline)) constexpr SoftMaxTiling SoftMaxFlashV2TilingFunc(const SoftMaxShapeInfo& shapeInfo,
    const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, const uint32_t localWorkSpaceSize,
    const bool isUpdate = false, const bool isBasicBlock = false, const bool isDataFormatNZ = false)
{
    return SoftMaxFlashV2TilingFuncImpl(shapeInfo.srcM, shapeInfo.srcK, dataTypeSize1, dataTypeSize2,
        localWorkSpaceSize / B32_BYTE_SIZE, isUpdate, isBasicBlock, isDataFormatNZ);
}
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inExpSumTensor, const LocalTensor<T>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<T, T, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, tiling, softmaxShapeInfo);
                                       ;
}
# 112 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<half>& dstTensor, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<half, float, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, tiling, softmaxShapeInfo);
                                       ;
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inExpSumTensor, const LocalTensor<T>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<T, T, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}
# 198 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashV2(const LocalTensor<half>& dstTensor, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<half, float, isUpdate, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}
}
#pragma end_pipe
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_grad_base_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_grad_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_grad_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/v220/softmax_grad_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradFrontGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, half, false>(tmpBuffer1[splitOffset * j],
            gradTensor[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer2, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(dst[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradFrontGenericNZImpl(const LocalTensor<float>& dst,
    const LocalTensor<float>& gradTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            gradTensor[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(dst[offset2], tmpBuffer1, { (uint16_t)reduceParam.originalSrcM, 1, 1, 0 });
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontNZImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, tiling.splitK, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, tiling.splitK, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };

    const uint32_t lastBlockMaskLen = tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGradFrontGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset1, offset2,
            splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGradFrontGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset1, offset2,
            splitCount, tailReduceParam);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontNDImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape)
{
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };

    if constexpr (sizeof(T) == sizeof(half)) {
        LocalTensor<float> srcBuffer = workLocal;
        LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
        LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

        LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
        LocalTensor<float> addBuffer =
            workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];
        const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
        const uint32_t elementNumPerBlk = DEFAULT_C0_SIZE / B32_BYTE_SIZE;
        uint8_t offset = (uint8_t)(splitBlock * elementNumPerBlk);
        const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
        const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
        const uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            if constexpr (isBasicBlock) {
                Cast<float, half, false>(srcBuffer, srcTensor[i * tiling.splitSize], RoundMode::CAST_NONE,
                    MASK_PLACEHOLDER, repeatTimes, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
                Cast<float, half, false>(gradBuffer, gradTensor[i * tiling.splitSize], RoundMode::CAST_NONE,
                    MASK_PLACEHOLDER, repeatTimes, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
                Mul<float, false>(dstBuffer, srcBuffer, gradBuffer, MASK_PLACEHOLDER, repeatTimes,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                for (uint32_t j = 1; j < splitBlock; ++j) {
                    PipeBarrier<PIPE_V>();
                    Add<float, false>(dstBuffer, dstBuffer, dstBuffer[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                        (uint8_t)(tiling.splitM), { 1, 1, 1, offset, offset, offset });
                }
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(dstBuffer, dstBuffer, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                    offset);
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(reduceBuffer, dstBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();

                Brcb(dstBuffer, reduceBuffer, splitCeilM, { B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
                Brcb(dstBuffer[DEFAULT_BLK_NUM], reduceBuffer, splitCeilM,
                    { B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });

                PipeBarrier<PIPE_V>();
                Cast<half, float, false>(dstTensor[i * tiling.reduceSize], dstBuffer, FLOAT2HALF_ROUND_MODE,
                    MASK_PLACEHOLDER, reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            } else {
                Cast(srcBuffer, srcTensor[i * tiling.splitSize], RoundMode::CAST_NONE, tiling.splitSize);
                Cast(gradBuffer, gradTensor[i * tiling.splitSize], RoundMode::CAST_NONE, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstBuffer, srcBuffer, gradBuffer, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                Cast(dstTensor[i * tiling.reduceSize], addBuffer, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            }
        }
        if (tiling.tailM != 0) {
            Cast(srcBuffer, srcTensor[tiling.rangeM * tiling.splitSize], RoundMode::CAST_NONE, tiling.tailSplitSize);
            Cast(gradBuffer, gradTensor[tiling.rangeM * tiling.splitSize], RoundMode::CAST_NONE, tiling.tailSplitSize);
            PipeBarrier<PIPE_V>();
            Mul(dstBuffer, srcBuffer, gradBuffer, tiling.tailSplitSize);
            reduceSumParam.srcM = tiling.tailM;
            reduceSumParam.dstM = tiling.tailM;
            reduceSumParam.originalSrcM = tiling.tailM;
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
            PipeBarrier<PIPE_V>();
            Cast(dstTensor[tiling.rangeM * tiling.reduceSize], addBuffer, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        }
    } else {
        LocalTensor<float> srcBuffer = workLocal;
        LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize];
        uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
        const uint32_t elementNumPerBlk = DEFAULT_C0_SIZE / B32_BYTE_SIZE;
        uint8_t offset = (uint8_t)(splitBlock * elementNumPerBlk);
        const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, elementNumPerBlk));
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            if constexpr (isBasicBlock) {
                offset2 = i * tiling.reduceSize;
                offset1 = i * tiling.splitSize;
                PipeBarrier<PIPE_V>();
                Mul<float, false>(srcBuffer, srcTensor[offset1], gradTensor[offset1], MASK_PLACEHOLDER, repeatTimes,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

                for (uint32_t j = 1; j < splitBlock; ++j) {
                    PipeBarrier<PIPE_V>();
                    Add<float, false>(srcBuffer, srcBuffer, srcBuffer[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                        (uint8_t)(tiling.splitM), { 1, 1, 1, offset, offset, offset });
                }
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(srcBuffer, srcBuffer, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                    splitBlock * DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(reduceBuffer, srcBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                Brcb(dstTensor[offset2], reduceBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
            } else {
                Mul(srcBuffer, srcTensor[i * tiling.splitSize], gradTensor[i * tiling.splitSize], tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(dstTensor[i * tiling.reduceSize], srcBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
            }
        }

        if (tiling.tailM != 0) {
            Mul(srcBuffer, srcTensor[tiling.rangeM * tiling.splitSize], gradTensor[tiling.rangeM * tiling.splitSize],
                tiling.tailSplitSize);
            PipeBarrier<PIPE_V>();

            reduceSumParam.srcM = tiling.tailM;
            reduceSumParam.dstM = tiling.tailM;
            reduceSumParam.originalSrcM = tiling.tailM;
            ReduceSumLastNDImpl(dstTensor[tiling.rangeM * tiling.reduceSize], srcBuffer, reduceBuffer, reduceSumParam);
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    LocalTensor<float> tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, half, false>(tmpBuffer1[splitOffset * j],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer2[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer3, tmpBuffer2, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer1[splitOffset * j], tmpBuffer1[splitOffset * j], tmpBuffer3, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer2[splitOffset * j], tmpBuffer1[splitOffset * j], tmpBuffer0[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer2[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void SoftMaxGradGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& gradTensor,
    const LocalTensor<float>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(dst[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
            src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradNZImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, bool isFront = false)
{
    if (isFront) {
        SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling);
    } else {
        const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
        const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
        uint32_t lastBlockMaskLen = tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
            tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
            SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        uint64_t mask[2] = { 0, 0 };
        CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        uint32_t offset = 0;
        uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset = i * splitCount;
            SoftMaxGradGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset, splitCount,
                mainReduceParam);
        }
        PipeBarrier<PIPE_V>();
        if (tiling.tailM != 0) {
            offset = tiling.rangeM * splitCount;
            splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
            SoftMaxGradGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset, splitCount,
                tailReduceParam);
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradNDGenericImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const ReduceLastND& reduceSumParam, const BroadCastLastND& brcParam, const bool isFront, const uint32_t offset1,
    const uint32_t offset2, const uint32_t splitSize, const uint32_t reduceSize)
{
    LocalTensor<float> srcBuffer = workLocal;
    LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
    LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

    LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    LocalTensor<float> addBuffer =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];

    Cast(srcBuffer, srcTensor[offset1], RoundMode::CAST_NONE, splitSize);
    Cast(gradBuffer, gradTensor[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    Mul(dstBuffer, srcBuffer, gradBuffer, splitSize);
    PipeBarrier<PIPE_V>();
    ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
    PipeBarrier<PIPE_V>();
    if (isFront) {
        Cast(dstTensor[offset2], addBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
    } else {
        BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, tiling.reduceK };
        BroadCastLastImpl(dstBuffer, addBuffer, brcParam);
        PipeBarrier<PIPE_V>();
        Sub(dstBuffer, gradBuffer, dstBuffer, splitSize);
        PipeBarrier<PIPE_V>();
        Mul(dstBuffer, dstBuffer, srcBuffer, splitSize);
        PipeBarrier<PIPE_V>();
        Cast(dstTensor[offset1], dstBuffer, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradNDImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape, bool isFront = false)
{
    const ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                                          tiling.splitK, tiling.reduceM, tiling.reduceK };
    const BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, tiling.reduceK };
    LocalTensor<float> srcBuffer = workLocal;
    LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
    LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

    LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    LocalTensor<float> addBuffer =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxGradNDGenericImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, reduceSumParam, brcParam, isFront,
            offset1, offset2, tiling.splitSize, tiling.reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
    }
    if (tiling.tailM != 0) {
        const ReduceLastND tailReduceSumParam = { tiling.tailM, originalSrcShape.k, tiling.tailM,
                                                  tiling.splitK, tiling.tailM, tiling.reduceK };
        const BroadCastLastND tailBrcParam = { tiling.tailM, tiling.srcK, tiling.tailM, tiling.reduceK };
        SoftmaxGradNDGenericImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, tailReduceSumParam, tailBrcParam,
            isFront, offset1, offset2, tiling.tailSplitSize, tiling.tailReduceSize);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradPostProcess(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape, bool isFront = false)
{
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };

    if constexpr (sizeof(T) == sizeof(half)) {
        SoftmaxGradNDImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, originalSrcShape, isFront);
    } else {
        if (isFront) {
            SoftmaxGradFrontNDImpl<float>(dstTensor, srcTensor, gradTensor, workLocal, tiling, originalSrcShape);
        } else {
            LocalTensor<float> splitBuffer = workLocal;
            LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize];
            LocalTensor<float> addBuffer = workLocal[tiling.splitSize + tiling.reduceSize];

            BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, elementNumPerBlk };
            for (uint32_t i = 0; i < tiling.rangeM; i++) {
                Mul(splitBuffer, srcTensor[i * tiling.splitSize], gradTensor[i * tiling.splitSize], tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, splitBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                BroadCastLastImpl(splitBuffer, addBuffer, brcParam);
                PipeBarrier<PIPE_V>();
                Sub(splitBuffer, gradTensor[i * tiling.splitSize], splitBuffer, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstTensor[i * tiling.splitSize], srcTensor[i * tiling.splitSize], splitBuffer, tiling.splitSize);
            }
            if (tiling.tailM != 0) {
                reduceSumParam.srcM = tiling.tailM;
                reduceSumParam.dstM = tiling.tailM;
                reduceSumParam.originalSrcM = tiling.tailM;
                Mul(splitBuffer, srcTensor[tiling.rangeM * tiling.splitSize],
                    gradTensor[tiling.rangeM * tiling.splitSize], tiling.tailSplitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, splitBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                BroadCastLastImpl(splitBuffer, addBuffer, brcParam);
                PipeBarrier<PIPE_V>();
                Sub(splitBuffer, gradTensor[tiling.rangeM * tiling.splitSize], splitBuffer, tiling.tailSplitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstTensor[tiling.rangeM * tiling.splitSize], srcTensor[tiling.rangeM * tiling.splitSize],
                    splitBuffer, tiling.tailSplitSize);
            }
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_grad_base_impl.h" 2




namespace AscendC {
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, true, false, true);
            SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, newTiling);
        } else {
            SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, true, isBasicBlock);
            SoftmaxGradFrontNDImpl<T, isBasicBlock>(dstTensor, gradTensor, srcTensor, workLocal, newTiling,
                originalSrcShape);
        } else {
            SoftmaxGradFrontNDImpl<T, isBasicBlock>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
                originalSrcShape);
        }
    }
}

template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, bool isFront,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);

    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, isFront, false, true);
            SoftmaxGradNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, newTiling, isFront);
        } else {
            SoftmaxGradNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling, isFront);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, isFront, false);
            SoftmaxGradPostProcess<T>(dstTensor, gradTensor, srcTensor, workLocal, newTiling, originalSrcShape,
                isFront);
        } else {
            SoftmaxGradPostProcess<T>(dstTensor, gradTensor, srcTensor, workLocal, tiling, originalSrcShape, isFront);
        }
    }
}

template <typename T, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, bool isFront,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling, isFront, softmaxShapeInfo);
}
template <typename T, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    bool isFront, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling, isFront, softmaxShapeInfo);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGrad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, bool isFront = false,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, tiling, isFront, softmaxShapeInfo);
                                    ;
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFront(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, tiling, softmaxShapeInfo);
                                    ;
}
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGrad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    bool isFront = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradImpl<T, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, sharedTmpBuffer, tiling, isFront,
        softmaxShapeInfo);
                                    ;
}
# 129 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxgrad.h"
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxGradFront(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, sharedTmpBuffer, tiling,
        softmaxShapeInfo);
}
}
#pragma end_pipe
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_membase_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_membase_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void XorCalcSimplified(const LocalTensor<T>& dstAddr, const LocalTensor<T> &src0Addr,
    const LocalTensor<T> &src1Addr, const LocalTensor<T>& tmpTensor)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    And<T, false>(dstAddr, src0Addr, src1Addr, MASK_PLACEHOLDER, 1, binaryParams);

    Or<T, false>(tmpTensor, src0Addr, src1Addr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Not<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    And<T, false>(dstAddr, tmpTensor, dstAddr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void XorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{



    uint32_t stackSize = sharedTmpBuffer.GetSize() / sizeof(T) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    const uint32_t loopCount = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T>(0, stackSize);
    for (uint32_t i = 0; i < loopCount; i++) {
        XorCalcSimplified(dstTensor[i * stackSize],
            src0Tensor[i * stackSize],
            src1Tensor[i * stackSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    if (tail != 0) {
        SetVectorMask<T>(0, tail);
        XorCalcSimplified(dstTensor[loopCount * stackSize],
            src0Tensor[loopCount * stackSize],
            src1Tensor[loopCount * stackSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    SetMaskNorm();
    SetVectorMask<T>(FULL_MASK, FULL_MASK);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h" 2




namespace AscendC {
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/xor/xor_common_impl.h"
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, int16_t>::value || std::is_same<T, uint16_t>::value) &&
        "Xor input datatype must be int16_t or uint16_t!");

    XorImpl(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T>& dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{




    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Xor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor)
{




    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, src0Tensor.GetSize());
}
#pragma end_pipe
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void FloorProcess(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    (void)tmpTensor;
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void FloorProcess(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();



    Cast<float, half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    FloorProcess(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{


    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        FloorProcess(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T>(0, calcTail);
        FloorProcess(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FloorImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const uint32_t calCount)
{
    Cast<float, float>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, calCount);
}
[aicore] __inline__ __attribute__((always_inline)) void FloorImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    (void)sharedTmpBuffer;
    Cast<float, float>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, calCount);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/floor/floor_common_impl.h" 2
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Floor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value)
        && "Floor input datatype must be half or float!");
    FloorImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/floor.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Floor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value)
        && "Floor input datatype must be half or float!");
    FloorImpl(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2



# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/../../impl/index/arithprogression/arithprogression_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/../../impl/index/arithprogression/arithprogression_common_impl.h"
namespace AscendC {

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetBaseArithProgression(const LocalTensor<T> &dstLocal, const T firstValue, const T diffValue,
    const int32_t count)
{
    for (int i = 0; i < count; i++) {
        dstLocal.SetValue(i, static_cast<T>(firstValue) +
            static_cast<T>(diffValue) * static_cast<T>(i));
    }
}


template <>
[aicore] __inline__ __attribute__((always_inline)) void GetBaseArithProgression(const LocalTensor<half> &dstLocal, const half firstValue,
    const half diffValue, const int32_t count)
{
    for (int i = 0; i < count; i++) {
        dstLocal.SetValue(i, static_cast<float>(firstValue) +
            static_cast<float>(diffValue) * static_cast<float>(i));
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ArithProgressionImpl(const LocalTensor<T> &dstLocal, const T firstValue, const T diffValue,
    const int32_t count)
{

                                                                                                       ;

                                                                                  ;


    if (g_coreType == AIC) {
        return;
    }

    struct UnaryRepeatParams addsParamsStride1(1, 1, 1, 1);
    struct UnaryRepeatParams addsParamsStride8(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);

    constexpr int32_t BLOCK_NUM = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t REPEAT_NUM = (ONE_REPEAT_BYTE_SIZE / sizeof(T));
    if (count > BLOCK_NUM) {

        GetBaseArithProgression<T>(dstLocal, firstValue, diffValue, BLOCK_NUM);
        auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        if (count > REPEAT_NUM) {

            SetVectorMask<T>(0, (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(BLOCK_NUM)) - 1));
            PipeBarrier<PIPE_V>();
            for (int i = 0; i < DEFAULT_BLK_NUM - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * BLOCK_NUM], dstLocal[i * BLOCK_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(BLOCK_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride1);
                PipeBarrier<PIPE_V>();
            }
            int32_t repeat = count / REPEAT_NUM;
            int32_t tail = count % REPEAT_NUM;
            ResetMask();
            PipeBarrier<PIPE_V>();

            for (int i = 0; i < repeat - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * REPEAT_NUM], dstLocal[i * REPEAT_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(REPEAT_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride8);
                PipeBarrier<PIPE_V>();
            }
            if (tail > 0) {
                int32_t tail_aligned = (tail + BLOCK_NUM - 1) / BLOCK_NUM * BLOCK_NUM;
                SetVectorMask<T>(tail_aligned);
                PipeBarrier<PIPE_V>();
                Adds<T, false>(dstLocal[repeat * REPEAT_NUM], dstLocal[(repeat - 1) * REPEAT_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(REPEAT_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride8);
                PipeBarrier<PIPE_V>();
            }
        } else {

            int32_t countAligned = (count + BLOCK_NUM - 1) / BLOCK_NUM * BLOCK_NUM;
            int32_t repeat = countAligned / BLOCK_NUM;
            SetVectorMask<T>(0, (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(BLOCK_NUM)) - 1));
            PipeBarrier<PIPE_V>();
            for (int i = 0; i < repeat - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * BLOCK_NUM], dstLocal[i * BLOCK_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(BLOCK_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride1);
                PipeBarrier<PIPE_V>();
            }
        }
    } else {

        GetBaseArithProgression<T>(dstLocal, firstValue, diffValue, count);
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h" 2

namespace AscendC {
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/index/arithprogression.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V, S"))) void ArithProgression(const LocalTensor<T> &dstLocal,
    const T firstValue, const T diffValue, const int32_t count)
{
    ArithProgressionImpl(dstLocal, firstValue, diffValue, count);
}
}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgrad_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgrad_common_impl.h"
const uint32_t LAYERNORM_GRAD_B32_BYTE_SIZE = 4;
const uint32_t LAYERNORM_GRAD_B16_BYTE_SIZE = 2;

namespace AscendC {

struct LayerNormGradShapeInfo {
    DataFormat dataFormat = DataFormat::ND;
};

struct LayerNormGradParams {
    [aicore] LayerNormGradParams(LayerNormGradTiling &tiling, LocalTensor<float> stackBuffer)
        : bLength(tiling.bLength),
          sLength(tiling.sLength),
          hLength(tiling.hLength),
          loopNum(tiling.loopNum),
          tailSize(tiling.tailSize),
          nohTailSize(tiling.nohTailSize),
          oneCalSize(tiling.oneCalSize),
          nohCalSize(tiling.nohCalSize),
          x1Tensor(stackBuffer[tiling.x1TensorPos]),
          x2Tensor(stackBuffer[tiling.x2TensorPos]),
          x3Tensor(stackBuffer[tiling.x3TensorPos]),
          pdVarTensor(stackBuffer[tiling.pdVarTensorPos]),
          pdMeanTensor(stackBuffer[tiling.pdMeanTensorPos]),
          tmpTensor(stackBuffer[tiling.tmpTensorPos]),
          tmpTensor1(stackBuffer[tiling.tmpTensor1Pos]),
          tmpTensor2(stackBuffer[tiling.tmpTensor2Pos]),
          tmpTensorBSH(stackBuffer[tiling.tmpTensorBSHPos]),
          lastDimValueBack(*(reinterpret_cast<float *>(&tiling.lastDimValueBack))),
          lastDimValueBackMulTwo(*(reinterpret_cast<float *>(&tiling.lastDimValueBackMulTwo)))
    {
        x1Tensor.SetSize(tiling.x1TensorSize);
        x2Tensor.SetSize(tiling.x2TensorSize);
        x3Tensor.SetSize(tiling.x3TensorSize);
        pdVarTensor.SetSize(tiling.pdVarTensorSize);
        pdMeanTensor.SetSize(tiling.pdMeanTensorSize);
        tmpTensor.SetSize(tiling.tmpTensorSize);
        tmpTensor1.SetSize(tiling.tmpTensor1Size);
        tmpTensor2.SetSize(tiling.tmpTensor2Size);
        tmpTensorBSH.SetSize(tiling.tmpTensorBSHSize);
    }

    [aicore] LayerNormGradParams(uint32_t b, uint32_t s, uint32_t h)
    {
        bLength = b;
        sLength = s;
        hLength = h;
    }

    uint32_t bLength;
    uint32_t sLength;
    uint32_t hLength;

    uint32_t loopNum;
    uint32_t tailSize;
    uint32_t nohTailSize;
    uint32_t oneCalSize;
    uint32_t nohCalSize;

    float lastDimValueBack;
    float lastDimValueBackMulTwo;

    LocalTensor<float> x1Tensor;
    LocalTensor<float> x2Tensor;
    LocalTensor<float> x3Tensor;
    LocalTensor<float> pdVarTensor;
    LocalTensor<float> pdMeanTensor;
    LocalTensor<float> tmpTensor;
    LocalTensor<float> tmpTensor1;
    LocalTensor<float> tmpTensor2;
    LocalTensor<float> tmpTensorBSH;
};

[aicore] __inline__ __attribute__((always_inline)) void DuplicateLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    float scalarList[BRCB_BROADCAST_NUMBER] = {0};
    const uint32_t rangeM = bsLength / BRCB_BROADCAST_NUMBER;
    const uint32_t tailM = bsLength % BRCB_BROADCAST_NUMBER;

    for (uint32_t i = 0; i < rangeM; i++) {
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            scalarList[j] = src[i * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            Duplicate(dst[(i * BRCB_BROADCAST_NUMBER + j) * hLength], scalarList[j], hLength);
        }
    }
    if (tailM != 0) {
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = src[rangeM * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < tailM; j++) {
            Duplicate(dst[(rangeM * BRCB_BROADCAST_NUMBER + j) * hLength], scalarList[j], hLength);
        }
    }
}


[aicore] __inline__ __attribute__((always_inline)) void BrcbLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    const uint32_t maxRepeatHSize = BRCB_MAX_REPEAT_SIZE * hLength;

    const uint32_t lineRound = hLength / BRCB_BROADCAST_NUMBER;

    const uint32_t rowRound = bsLength / BRCB_BROADCAST_NUMBER;
    const uint32_t rowTail = bsLength % BRCB_BROADCAST_NUMBER;
    const uint32_t rowRoundLen = bsLength - rowTail;

    const uint32_t repeatTimes = rowRound / MAX_REPEAT_TIMES;
    const uint32_t tailTimes = rowRound % MAX_REPEAT_TIMES;

    BrcbRepeatParams repeatParams;

    repeatParams.dstBlkStride = lineRound;

    repeatParams.dstRepStride = hLength;

    for (uint32_t i = 0; i < lineRound; i++) {
        for (uint32_t j = 0; j < repeatTimes; j++) {
            Brcb(dst[i * BRCB_BROADCAST_NUMBER + j * maxRepeatHSize], src[j * BRCB_MAX_REPEAT_SIZE], MAX_REPEAT_TIMES,
                repeatParams);
        }

        if (tailTimes > 0) {
            Brcb(dst[i * BRCB_BROADCAST_NUMBER + repeatTimes * maxRepeatHSize], src[repeatTimes * BRCB_MAX_REPEAT_SIZE],
                tailTimes, repeatParams);
        }
        PipeBarrier<PIPE_V>();
    }

    if (rowTail != 0) {
        DuplicateLastDimImpl(dst[rowRoundLen * hLength], src[rowRoundLen], rowTail, hLength);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void BroadcastLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t dstSize, const uint32_t srcSize)
{

    BrcbLastDimImpl(dst, src, srcSize, dstSize / srcSize);



}

[aicore] __inline__ __attribute__((always_inline)) void ReduceSumImpl(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const uint32_t calSize, const uint32_t hLength)
{
                                                                                                   ;
    const uint32_t count = calSize / hLength;

    for (uint32_t i = 0; i < count; i++) {
        uint32_t totalNum = hLength;
        uint32_t iMulhLength = i * hLength;

        LocalTensor<float> srctmp = src;
        LocalTensor<float> dstTmp = dst;

        while (totalNum > 1) {
            uint32_t repeatTimes = totalNum / ONE_REPEAT_FLOAT_SIZE;
            uint32_t tailSize = totalNum % ONE_REPEAT_FLOAT_SIZE;

            uint32_t blockNum = repeatTimes / MAX_REPEAT_TIMES;
            uint32_t blockTail = repeatTimes % MAX_REPEAT_TIMES;

            for (uint32_t j = 0; j < blockNum; j++) {
                WholeReduceSum(dst[iMulhLength + j * MAX_REPEAT_TIMES], srctmp[iMulhLength + j * MAX_REPEAT_FLOAT_SIZE],
                    ONE_REPEAT_FLOAT_SIZE, MAX_REPEAT_TIMES, 1, 1, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();

            if (totalNum == ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            } else {
                dstTmp = dst[iMulhLength + blockNum * MAX_REPEAT_TIMES];
            }

            if (blockTail > 0) {
                WholeReduceSum(dstTmp, srctmp[iMulhLength + blockNum * MAX_REPEAT_FLOAT_SIZE], ONE_REPEAT_FLOAT_SIZE,
                    blockTail, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();

            if (totalNum < ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            } else {
                dstTmp = dst[iMulhLength + totalNum / ONE_REPEAT_FLOAT_SIZE];
            }

            if (tailSize > 0) {
                WholeReduceSum(dstTmp, srctmp[iMulhLength + repeatTimes * ONE_REPEAT_FLOAT_SIZE], tailSize,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();

            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
            srctmp = dst;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateTensor(const LocalTensor<T> &dst, const LocalTensor<T> &src, const uint32_t count,
    const uint32_t length)
{
    BroadcastLastDimImpl(dst, src, count * length, count);
    PipeBarrier<PIPE_V>();
    return;
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdX1(const LocalTensor<float> &inputDy, const LocalTensor<float> &inputGamma,
    LayerNormGradParams &param, const uint32_t nohSize, const uint32_t hLength)
{

    for (size_t i = 0; i < nohSize; ++i) {
        Mul(param.x1Tensor[i * hLength], inputDy[i * hLength], inputGamma, hLength);
    }
    PipeBarrier<PIPE_V>();
    return;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ComputePdX2(const LocalTensor<T> &inputX, const LocalTensor<T> &inputMean,
    const LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize, const uint32_t hLength)
{

    DuplicateTensor(param.tmpTensorBSH, inputMean, nohSize, hLength);
    PipeBarrier<PIPE_V>();

    Sub(param.x2Tensor, inputX, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    return;
}

[aicore] __inline__ __attribute__((always_inline)) void DoOneDiv(LocalTensor<float> &dstTensor, LocalTensor<float> &oneTensor,
    LocalTensor<float> &src1Tensor, const uint32_t nohSize)
{
    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_BLOCK);
    Duplicate<float, false>(oneTensor, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, nohSize);
    Div<float, false>(dstTensor, oneTensor, src1Tensor, MASK_PLACEHOLDER, 1,
        { 1, 0, 1, DEFAULT_REPEAT_STRIDE, 0, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdVar(const LocalTensor<float> &inputVariance, float epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize)
{
    const float multiplier1 = -1.5;
    const float multiplier2 = -0.5;

    Adds(param.tmpTensor, inputVariance, epsilon, nohSize);
    PipeBarrier<PIPE_V>();

    Mul(param.tmpTensorBSH, param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
    Sqrt(param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();

    DoOneDiv(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);


    DuplicateTensor(param.tmpTensorBSH, param.tmpTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Mul(param.tmpTensorBSH, param.x2Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensorBSH, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Muls(param.tmpTensorBSH, param.tmpTensorBSH, static_cast<float>(multiplier2), calSize);
    PipeBarrier<PIPE_V>();




    ReduceSumImpl(param.pdVarTensor, param.tmpTensorBSH, calSize, param.hLength);
    PipeBarrier<PIPE_V>();
    return;
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdMean(const LocalTensor<float> &inputVariance, const LocalTensor<float> &resForGamma,
    float epsilon, LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize)
{
    constexpr float exponent = -0.5;
    constexpr float multiplier = -1.0;
    constexpr float multiplier2 = -2.0;

    Adds(param.tmpTensor, inputVariance, epsilon, nohSize);
    PipeBarrier<PIPE_V>();


    Sqrt(param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();

    DoOneDiv(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);


    DuplicateTensor(param.tmpTensorBSH, param.tmpTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Mul(resForGamma, param.x2Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();


    Mul(param.x3Tensor, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Muls(param.tmpTensorBSH, param.x3Tensor, static_cast<float>(multiplier), calSize);
    PipeBarrier<PIPE_V>();


    ReduceSumImpl(param.pdMeanTensor, param.tmpTensorBSH, calSize, param.hLength);


    Muls(param.tmpTensorBSH, param.x2Tensor, static_cast<float>(multiplier2), calSize);
    PipeBarrier<PIPE_V>();

    ReduceSumImpl(param.tmpTensor, param.tmpTensorBSH, calSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Muls(param.tmpTensor, param.tmpTensor, static_cast<float>(param.lastDimValueBack), nohSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensor, param.pdVarTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();


    Add(param.pdMeanTensor, param.pdMeanTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void ComputePdX(const LocalTensor<float> &inputVariance, const LocalTensor<float> &outputPdX,
    float epsilon, const LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize)
{


    Muls(param.pdVarTensor, param.pdVarTensor, static_cast<float>(param.lastDimValueBackMulTwo), nohSize);
    PipeBarrier<PIPE_V>();

    DuplicateTensor(param.tmpTensorBSH, param.pdVarTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();

    Mul(param.x1Tensor, param.tmpTensorBSH, param.x2Tensor, calSize);
    PipeBarrier<PIPE_V>();


    Muls(param.pdMeanTensor, param.pdMeanTensor, static_cast<float>(param.lastDimValueBack), nohSize);
    PipeBarrier<PIPE_V>();
    DuplicateTensor(param.tmpTensorBSH, param.pdMeanTensor, nohSize, param.hLength);


    Add(param.x1Tensor, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();

    Add(outputPdX, param.x1Tensor, param.x3Tensor, calSize);
    PipeBarrier<PIPE_V>();
    return;
}

[aicore] __inline__ __attribute__((always_inline)) void GetTmpTensor(const LocalTensor<float> &outputPdX, const LocalTensor<float> &inputDy,
    const LocalTensor<float> &inputX, LayerNormGradParams &param, bool isReuseSource = false)
{
    param.tmpTensor = outputPdX;
    if (isReuseSource == true) {
        param.x1Tensor = inputDy;
        param.x2Tensor = inputX;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess(const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX,
    const LocalTensor<T> &inputVariance, const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma,
    const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma, T epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess<half>(const LocalTensor<half> &inputDy, const LocalTensor<half> &inputX,
    const LocalTensor<half> &inputVariance, const LocalTensor<half> &inputMean, const LocalTensor<half> &inputGamma,
    const LocalTensor<half> &outputPdX, const LocalTensor<half> &resForGamma, half epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{
    Cast(param.tmpTensor1, inputDy, RoundMode::CAST_NONE, calSize);
    Cast(param.tmpTensor2, inputGamma, RoundMode::CAST_NONE, param.hLength);
    PipeBarrier<PIPE_V>();

    ComputePdX1(param.tmpTensor1, param.tmpTensor2, param, nohSize, param.hLength);

    Cast(param.tmpTensor1, inputX, RoundMode::CAST_NONE, calSize);
    Cast(param.tmpTensor2, inputMean, RoundMode::CAST_NONE, nohSize);
    PipeBarrier<PIPE_V>();
    ComputePdX2(param.tmpTensor1, param.tmpTensor2, param, calSize, nohSize, param.hLength);

    Cast(param.tmpTensor1, inputVariance, RoundMode::CAST_NONE, nohSize);
    PipeBarrier<PIPE_V>();
    ComputePdVar(param.tmpTensor1, epsilon, param, calSize, nohSize);


    ComputePdMean(param.tmpTensor1, param.tmpTensor2, epsilon, param, calSize, nohSize);



    ComputePdX(param.tmpTensor1, param.tmpTensor, epsilon, param, calSize, nohSize);

    Cast(outputPdX, param.tmpTensor, RoundMode::CAST_NONE, calSize);
    Cast(resForGamma, param.tmpTensor2, RoundMode::CAST_NONE, calSize);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess<float>(const LocalTensor<float> &inputDy, const LocalTensor<float> &inputX,
    const LocalTensor<float> &inputVariance, const LocalTensor<float> &inputMean, const LocalTensor<float> &inputGamma,
    const LocalTensor<float> &outputPdX, const LocalTensor<float> &resForGamma, float epsilon,
    LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{
    GetTmpTensor(outputPdX, inputDy, inputX, param, isReuseSource);

    ComputePdX1(inputDy, inputGamma, param, nohSize, param.hLength);

    ComputePdX2(inputX, inputMean, param, calSize, nohSize, param.hLength);

    ComputePdVar(inputVariance, epsilon, param, calSize, nohSize);


    ComputePdMean(inputVariance, resForGamma, epsilon, param, calSize, nohSize);
    PipeBarrier<PIPE_V>();



    ComputePdX(inputVariance, outputPdX, epsilon, param, calSize, nohSize);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradComputeND(const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX,
    const LocalTensor<T> &inputVariance, const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma,
    const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma, T epsilon, LayerNormGradParams &param,
    bool isReuseSource)
{
    int offset0 = 0;
    int offset1 = 0;

    for (size_t i = 0; i < param.loopNum; ++i) {
        ComputeProcess<T>(inputDy[offset0], inputX[offset0], inputVariance[offset1], inputMean[offset1], inputGamma,
            outputPdX[offset0], resForGamma[offset0], epsilon, param, param.oneCalSize, param.nohCalSize,
            isReuseSource);
        offset0 += param.oneCalSize;
        offset1 += param.nohCalSize;
    }

    if (param.tailSize != 0) {
        ComputeProcess<T>(inputDy[offset0], inputX[offset0], inputVariance[offset1], inputMean[offset1], inputGamma,
            outputPdX[offset0], resForGamma[offset0], epsilon, param, param.tailSize, param.nohTailSize, isReuseSource);
    }
    return;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradImpl(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, LocalTensor<uint8_t> &sharedTmpBuffer, T epsilon,
    LayerNormGradTiling &tiling, const LayerNormGradShapeInfo &shapeInfo = {})
{
                                       ;


                                                                                  ;

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LayerNormGradParams param(tiling, stackBuffer);

    if (shapeInfo.dataFormat == DataFormat::ND) {
        LayerNormGradComputeND(inputDy, inputX, inputVariance, inputMean, inputGamma, outputPdX, resForGamma, epsilon,
            param, isReuseSource);
    } else {
                                                                                           ;
    }
                                      ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradImpl(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, T epsilon, LayerNormGradTiling &tiling,
    const LayerNormGradShapeInfo &shapeInfo = {})
{
    LocalTensor<uint8_t> stackBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackBuffer);
                                                                                 ;

    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        stackBuffer, epsilon, tiling);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGrad(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, LocalTensor<uint8_t> &sharedTmpBuffer, T epsilon,
    LayerNormGradTiling &tiling, const LayerNormGradShapeInfo &shapeInfo = {})
{
    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        sharedTmpBuffer, epsilon, tiling, shapeInfo);
}
# 77 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgrad.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGrad(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, T epsilon, LayerNormGradTiling &tiling,
    const LayerNormGradShapeInfo &shapeInfo = {})
{
    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        epsilon, tiling, shapeInfo);
}
#pragma end_pipe
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgradbeta_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernormgrad/layernormgradbeta_common_impl.h"
namespace AscendC {
struct LayerNormGradBetaParams {
    [aicore] LayerNormGradBetaParams(){};

    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;

    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    uint32_t hCurLength = 0;

    LocalTensor<float> gammaTempTensor;
    LocalTensor<float> betaTempTensor;
    LocalTensor<float> inputDyTmpTensor;
    LocalTensor<float> resForGammaTmpTensor;
};


template <bool isClearDst = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSumFirstN(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, hLength);
    uint32_t startIndex = 0;
    if constexpr (isClearDst) {
        const UnaryRepeatParams unaryRepeatParams;
        Adds<float, false>(dst, src, static_cast<float>(0), MASK_PLACEHOLDER, 1, unaryRepeatParams);
        startIndex = 1;
        PipeBarrier<PIPE_V>();
    }

    const BinaryRepeatParams binaryParams;
    for (; startIndex < bsLength; startIndex++) {
        Add<float, false>(dst, src[startIndex * hLength], dst, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool isClearDst = false>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess(const LocalTensor<float> &resForGamma, const LocalTensor<float> &inputDy,
    const LocalTensor<float> &outputPdGamma, const LocalTensor<float> &outputPdBeta,
    const LayerNormGradBetaParams &params)
{
    const LocalTensor<float> &resForGammaTmpTensor = params.resForGammaTmpTensor;

    SetVectorMask<float, MaskMode::COUNTER>(0, params.bshCurLength);

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(resForGammaTmpTensor, inputDy, resForGamma, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    ReduceSumFirstN<isClearDst>(outputPdGamma, resForGammaTmpTensor, params.bsCurLength, params.hCurLength);

    ReduceSumFirstN<isClearDst>(outputPdBeta, inputDy, params.bsCurLength, params.hCurLength);
}

template <bool isClearDst = false>
[aicore] __inline__ __attribute__((always_inline)) void ComputeProcess(const LocalTensor<half> &resForGamma, const LocalTensor<half> &inputDy,
    const LocalTensor<half> &outputPdGamma, const LocalTensor<half> &outputPdBeta,
    const LayerNormGradBetaParams &params)
{
    const LocalTensor<float> &inputDyTmpTensor = params.inputDyTmpTensor;
    const LocalTensor<float> &resForGammaTmpTensor = params.resForGammaTmpTensor;

    const LocalTensor<float> &gammaTempTensor = params.gammaTempTensor;
    const LocalTensor<float> &betaTempTensor = params.betaTempTensor;

    SetVectorMask<half, MaskMode::COUNTER>(0, params.bshCurLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);


    Cast<float, half, false>(inputDyTmpTensor, inputDy, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(resForGammaTmpTensor, resForGamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    ComputeProcess<isClearDst>(resForGammaTmpTensor, inputDyTmpTensor, gammaTempTensor, betaTempTensor, params);

    SetVectorMask<float, MaskMode::COUNTER>(0, params.hCurLength);

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, float, false>(outputPdGamma, gammaTempTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(outputPdBeta, betaTempTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBetaComputeND(const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy,
    const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta, const LayerNormGradBetaTiling &tiling,
    LayerNormGradBetaParams &params)
{
    ComputeProcess<true>(resForGamma, inputDy, outputPdGamma, outputPdBeta, params);

    uint32_t inputOffset = tiling.oneCalSize;

    for (uint32_t index = 1; index < tiling.loopRound; index++) {
        ComputeProcess(resForGamma[inputOffset], inputDy[inputOffset], outputPdGamma, outputPdBeta, params);
        inputOffset += tiling.oneCalSize;
    }

    if (tiling.inputTailSize > 0) {
        params.bshCurLength = tiling.inputTailSize;
        params.bsCurLength = tiling.bsTailSize;

        ComputeProcess(resForGamma[tiling.inputTailPos], inputDy[tiling.inputTailPos], outputPdGamma, outputPdBeta,
            params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormGradBetaTensorInfo(const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy,
    const LocalTensor<float> &stackBuffer, const LayerNormGradBetaTiling &tiling, LayerNormGradBetaParams &params)
{
    params.bLength = tiling.bLength;
    params.sLength = tiling.sLength;
    params.hLength = tiling.hLength;
    params.originalHLength = tiling.originalHLength;

    params.bshCurLength = tiling.bshCurLength;
    params.bsCurLength = tiling.bsCurLength;
    params.hCurLength = tiling.originalHLength;

    if constexpr (sizeof(T) == sizeof(half)) {
        params.gammaTempTensor = stackBuffer[tiling.gammaTempTensorPos];
        params.betaTempTensor = stackBuffer[tiling.betaTempTensorPos];
        params.inputDyTmpTensor = stackBuffer[tiling.inputDyTmpTensorPos];
        params.resForGammaTmpTensor = stackBuffer[tiling.resForGammaTmpTensorPos];




          ;
    }

    if constexpr (sizeof(T) == sizeof(float)) {
        if constexpr (isReuseSource) {
            params.resForGammaTmpTensor = resForGamma;
        } else {
            params.resForGammaTmpTensor = stackBuffer[tiling.resForGammaTmpTensorPos];





              ;
        }
    }




      ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBetaImpl(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const LayerNormGradBetaTiling &tiling)
{
                                           ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
                                              ;
        return;
    }

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LayerNormGradBetaParams params;
    GetLayerNormGradBetaTensorInfo<T, isReuseSource>(resForGamma, inputDy, stackBuffer, tiling, params);

    SetMaskCount();
    LayerNormGradBetaComputeND(resForGamma, inputDy, outputPdGamma, outputPdBeta, tiling, params);

    SetMaskNorm();
    ResetMask();
                                          ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBetaImpl(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, LayerNormGradBetaTiling &tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, sharedTmpBuffer, tiling);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBeta(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const LayerNormGradBetaTiling &tiling)
{
    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, sharedTmpBuffer, tiling);
}
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernormgradbeta.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormGradBeta(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, LayerNormGradBetaTiling &tiling)
{
    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, tiling);
}
#pragma end_pipe
}
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h" 2
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DuplicateLastDimImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t srcSize, const uint32_t brcbSize)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    T scalarList[BRCB_BROADCAST_NUMBER] = {0};
    const uint32_t rangeM = srcSize / BRCB_BROADCAST_NUMBER;
    const uint32_t tailM = srcSize % BRCB_BROADCAST_NUMBER;

    for (uint32_t i = 0; i < rangeM; i++) {
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            scalarList[j] = srcTensor[i * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            Duplicate(dstTensor[(i * BRCB_BROADCAST_NUMBER + j) * brcbSize], scalarList[j], brcbSize);
        }
    }
    if (tailM != 0) {
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcTensor[rangeM * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < tailM; j++) {
            Duplicate(dstTensor[(rangeM * BRCB_BROADCAST_NUMBER + j) * brcbSize], scalarList[j], brcbSize);
        }
    }

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AlignedPad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    PadParams& padParams, PadTiling& tiling)
{
    uint16_t leftPad = padParams.leftPad;
    uint16_t rightPad = padParams.rightPad;
    int32_t padValue = padParams.padValue;

    uint32_t height = tiling.srcHeight;
    uint32_t width = tiling.srcWidth;
    uint32_t oriWidth = tiling.srcOriWidth;




      ;


                                                                                                                ;


                                                                                                                       ;


    uint32_t elementsPerBlock = ONE_BLK_SIZE / sizeof(T);

    DataCopy(dstTensor, srcTensor, height * width);
    PipeBarrier<PIPE_V>();

    uint64_t mask[2];
    mask[0] = ((1 << rightPad) - 1) << (elementsPerBlock - (width - oriWidth));
    mask[1] = 0;

    uint32_t widthWithoutLastBlock = tiling.widthWithoutLastBlock;
    uint32_t blocksPerRow = tiling.blocksPerRow;

    uint32_t heightTiling = tiling.heightTiling;
    uint32_t heightFractal = tiling.heightFractal;
    uint32_t heightFractalTail = tiling.heightFractalTail;
    for (uint32_t i = 0; i < heightFractal; i++) {
        Duplicate<T, true>(dstTensor[i * tiling.mainLoopOffset + widthWithoutLastBlock], static_cast<T>(padValue), mask,
            heightTiling, 1, blocksPerRow);
    }
    if (heightFractalTail) {
        Duplicate<T, true>(dstTensor[tiling.tailBlockOffset], static_cast<T>(padValue), mask, heightFractalTail, 1,
            blocksPerRow);
    }
}
# 126 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnAlignedPad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    PadParams& padParams, const LocalTensor<T>& tmpBuffer, PadTiling& tiling)
{
    uint16_t leftPad = padParams.leftPad;
    uint16_t rightPad = padParams.rightPad;
    int32_t padValue = padParams.padValue;

    uint32_t height = tiling.srcHeight;
    uint32_t width = tiling.srcWidth;
    uint32_t oriWidth = tiling.srcOriWidth;






      ;

    uint32_t tmp1BlockNum = tiling.tmpBuffer1BlockNum;

    LocalTensor<T> tmp1 = tmpBuffer;
    LocalTensor<T> tmp2 = tmpBuffer[tiling.tmpBuffer2Offset];

    uint32_t widthTiling = tiling.widthTiling;
    uint32_t widthFractal = tiling.widthFractal;
    uint32_t widthFractalTail = tiling.widthFractalTail;

    uint32_t widthFractalTailAlingned = tiling.widthFractalTailAlingned;

    uint32_t brcbTiling = tiling.brcbTiling;
    uint32_t brcbFractal = tiling.brcbFractal;
    uint32_t brcbFractalTail = tiling.brcbFractalTail;
    uint32_t brcbFractalCount = 0;

    uint32_t maxRepeatTimes = tiling.maxRepeatTimes;
    uint32_t brcbTilingRepeatTimes = tiling.brcbTilingRepeatTimes;
    uint32_t brcbTilingRepeatTimesTail = tiling.brcbTilingRepeatTimesTail;
    uint32_t brcbFractalTailRepeatTimes = tiling.brcbFractalTailRepeatTimes;
    uint32_t brcbFractalTailRepeatTimesTail = tiling.brcbFractalTailRepeatTimesTail;

    uint32_t tmp1RowFull = tiling.tmpBuffer1RowNum;
    uint32_t tmp1RowCount = tiling.tmpBuffer1RowNum;
    uint32_t tmp1RemainRow = 0;

    uint32_t tmp2RowFull = tiling.tmpBuffer1RowNum;
    uint32_t tmp2RowCount = 0;
    uint32_t tmp2NeedRow = tiling.tmpBuffer1RowNum;

    uint32_t tmpWidth = ONE_BLK_SIZE / sizeof(T);


    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = tmp1BlockNum;
    if (transDataParams.repeatTimes > 1) {
        transDataParams.dstRepStride = 16;
        transDataParams.srcRepStride = 16;
    }

    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE];


    for (uint16_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (uint64_t)(tmp2[i * tmpWidth].GetPhyAddr());
        srcList[i] = (uint64_t)(tmp2[i * tmpWidth].GetPhyAddr());
    }




    for (uint32_t j = 0; j < height; j++) {
        for (uint32_t i = 0; i < (widthFractal + 1); i++) {
            tmp2RowCount = 0;

            if (i == 0 && leftPad != 0) {
                Duplicate<T, true>(tmp2, static_cast<T>(padValue), tmpWidth, leftPad, 1, 1);
                tmp2RowCount += leftPad;
            }


            if (i == widthFractal) {
                if (rightPad != 0) {
                    Duplicate<T, true>(tmp2[(widthFractalTailAlingned - rightPad) * tmpWidth], static_cast<T>(padValue),
                        tmpWidth, rightPad, 1, 1);
                }
                tmp2NeedRow = widthFractalTailAlingned - tmp2RowCount - rightPad;
            } else {
                tmp2NeedRow = tmp2RowFull - tmp2RowCount;
            }

            while (tmp2NeedRow != 0) {
                PipeBarrier<PIPE_V>();
                tmp1RemainRow = tmp1RowFull - tmp1RowCount;
                if (tmp2NeedRow > tmp1RemainRow) {
                    if (tmp1RemainRow != 0) {
                        DataCopy(tmp2[tmp2RowCount * tmpWidth], tmp1[tmp1RowCount * tmpWidth],
                            { 1, static_cast<uint16_t>(tmp1RemainRow), 0, 0 });
                        tmp1RowCount += tmp1RemainRow;
                        tmp2RowCount += tmp1RemainRow;
                        tmp2NeedRow -= tmp1RemainRow;
                        PipeBarrier<PIPE_V>();
                    }


                    if (brcbFractalCount == brcbFractal) {
                        for (uint32_t i = 0; i < brcbFractalTailRepeatTimes; i++) {
                            Brcb(tmp1[i * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + i * maxRepeatTimes * 8], maxRepeatTimes,
                                { 1, 8 });
                        }
                        if (brcbFractalTailRepeatTimesTail) {
                            Brcb(tmp1[brcbFractalTailRepeatTimes * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling +
                                brcbFractalTailRepeatTimes * maxRepeatTimes * 8],
                                brcbFractalTailRepeatTimesTail, { 1, 8 });
                        }
                        tmp1RowFull = brcbFractalTail;
                    } else {
                        for (uint32_t i = 0; i < brcbTilingRepeatTimes; i++) {
                            Brcb(tmp1[i * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + i * maxRepeatTimes * 8], maxRepeatTimes,
                                { 1, 8 });
                        }
                        if (brcbTilingRepeatTimesTail) {
                            Brcb(tmp1[brcbTilingRepeatTimes * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + brcbTilingRepeatTimes * maxRepeatTimes * 8],
                                brcbTilingRepeatTimesTail, { 1, 8 });
                        }
                    }
# 266 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_base_impl.h"
                    brcbFractalCount += 1;
                    tmp1RowCount = 0;
                } else {
                    DataCopy(tmp2[tmp2RowCount * tmpWidth], tmp1[tmp1RowCount * tmpWidth],
                        { 1, static_cast<uint16_t>(tmp2NeedRow), 0, 0 });
                    tmp1RowCount += tmp2NeedRow;
                    tmp2RowCount += tmp2NeedRow;
                    tmp2NeedRow = 0;
                }
            }

            PipeBarrier<PIPE_V>();


            TransDataTo5HD<T>(dstList, srcList, transDataParams);
            PipeBarrier<PIPE_V>();
            if (i == widthFractal) {

                if (sizeof(T) == sizeof(half)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(widthFractalTailAlingned / 16), 1, 15, 0 });
                } else if (sizeof(T) == sizeof(float)) {
                    if (widthFractalTailAlingned / 16 != 0) {
                        DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum], tmp2,
                            { static_cast<uint16_t>(widthFractalTailAlingned / 16), 2, 14, 0 });
                    }
                    if (widthFractalTailAlingned % 16) {
                        DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum +
                            widthFractalTailAlingned / 16 * 16],
                            tmp2[widthFractalTailAlingned / 16 * 16 * 8], { 1, 1, 15, 0 });
                    }
                }
            } else {

                if (sizeof(T) == sizeof(half)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + i * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(tmp1BlockNum), 1, 15, 0 });
                } else if (sizeof(T) == sizeof(float)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + i * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(tmp1BlockNum), 2, 14, 0 });
                }
            }
        }
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void PadCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    PadParams &padParams, const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
    uint32_t width = tiling.srcWidth;


    if (width * sizeof(T) % ONE_BLK_SIZE == 0) {
        AlignedPad(dstTensor, srcTensor, padParams, tiling);
    } else {
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        UnAlignedPad(dstTensor, srcTensor, padParams, tmpBuffer, tiling);
    }
}
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_v220_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPadCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    UnPadParams &unPadParams, LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
    uint16_t rightPad = unPadParams.rightPad;
    uint16_t height = tiling.srcHeight;
    uint16_t width = tiling.srcWidth;

    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = height;
    reducev2Params.src0RepeatStride = static_cast<uint16_t>(width * sizeof(T) / ONE_BLK_SIZE);
    uint64_t rsvdCnt = 0;
    GatherMask(dstTensor, srcTensor, REDUCEV2_MODE_SEVEN, true, (width - rightPad), reducev2Params, rsvdCnt);
    ResetMask();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/pad/pad_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void PadImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
    PadCompute<T>(dstTensor, srcTensor, padParams, sharedTmpBuffer, tiling);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPadImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    UnPadParams &unPadParams, LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{

                                                                                                             ;






      ;
    UnPadCompute<T>(dstTensor, srcTensor, unPadParams, sharedTmpBuffer, tiling);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h" 2

namespace AscendC {
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
#pragma begin_pipe(V)
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Pad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
                             ;
    PadImpl<T>(dstTensor, srcTensor, padParams, sharedTmpBuffer, tiling);
                            ;
}
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Pad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    PadTiling &tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    PadImpl<T>(dstTensor, srcTensor, padParams, tmpBuffer, tiling);
}
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, UnPadParams &unPadParams,
     LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
    UnPadImpl<T>(dstTensor, srcTensor, unPadParams, sharedTmpBuffer, tiling);
}
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/pad.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void UnPad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, UnPadParams &unPadParams,
    UnPadTiling &tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    UnPadImpl<T>(dstTensor, srcTensor, unPadParams, tmpBuffer, tiling);
}
#pragma end_pipe
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl_common.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl_common.h"
namespace AscendC {
constexpr uint32_t HCCL_GROUP_ID_0 = 0;
using HcclHandle = int8_t;

enum class HcclCMDType {
    HCCL_CMD_INVALID = 0,
    HCCL_CMD_BROADCAST = 1,
    HCCL_CMD_ALLREDUCE,
    HCCL_CMD_REDUCE,
    HCCL_CMD_SEND,
    HCCL_CMD_RECEIVE,
    HCCL_CMD_ALLGATHER,
    HCCL_CMD_REDUCE_SCATTER,
    HCCL_CMD_ALLTOALLV,
    HCCL_CMD_ALLTOALLVC,
    HCCL_CMD_ALLTOALL,
    HCCL_CMD_GATHER,
    HCCL_CMD_SCATTER,
    HCCL_CMD_BATCH_SEND_RECV,

    HCCL_CMD_FINALIZE = 100,
    HCCL_CMD_INTER_GROUP_SYNC,
    HCCL_CMD_MAX
};

enum HcclReduceOp {
    HCCL_REDUCE_SUM = 0,
    HCCL_REDUCE_PROD = 1,
    HCCL_REDUCE_MAX = 2,
    HCCL_REDUCE_MIN = 3,
    HCCL_REDUCE_RESERVED
};

enum class MC2_BUFFER_LOCATION {
    MC2_BUFFER_TYPE_DEFAULT = 0,
    MC2_BUFFER_TYPE_OUTPUT,
    MC2_BUFFER_TYPE_WINDOW_IN,
    MC2_BUFFER_TYPE_WINDOW_OUT,
    MC2_BUFFER_TYPE_WORKSPACE,
    MC2_BUFFER_TYPE_INPUT,
    MC2_BUFFER_TYPE_COMMOUT,
    MC2_BUFFER_TYPE_END
};

enum HcclServerType {
    HCCL_SERVER_TYPE_AICPU = 0,
    HCCL_SERVER_TYPE_END
};




enum HcclDataType {
    HCCL_DATA_TYPE_INT8 = 0,
    HCCL_DATA_TYPE_INT16 = 1,
    HCCL_DATA_TYPE_INT32 = 2,
    HCCL_DATA_TYPE_FP16 = 3,
    HCCL_DATA_TYPE_FP32 = 4,
    HCCL_DATA_TYPE_INT64 = 5,
    HCCL_DATA_TYPE_UINT64 = 6,
    HCCL_DATA_TYPE_UINT8 = 7,
    HCCL_DATA_TYPE_UINT16 = 8,
    HCCL_DATA_TYPE_UINT32 = 9,
    HCCL_DATA_TYPE_FP64 = 10,
    HCCL_DATA_TYPE_BFP16 = 11,
    HCCL_DATA_TYPE_RESERVED
};
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_msg.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_msg.h"
namespace AscendC {
constexpr int32_t HCCL_FAILED = -1;
constexpr int32_t HCCL_SUCCESS = 0;
constexpr int32_t HCCL_MAX_HANDLE_ID = 32;
constexpr int8_t INVALID_HANDLE_ID = -1;
constexpr int8_t INVALID_MSG_POSITION = -1;

constexpr uint32_t HCCL_MAX_RANK_NUM = 8;
constexpr uint32_t HCCL_MAX_RANK_NUM_V2 = 256;
constexpr uint32_t HCCL_MSG_CNT = 64;
constexpr uint32_t HCCL_MSG_VALID_MASK = 0x5CDF123A;



struct HcclMsg {
    HcclCMDType commType;
    HcclReduceOp opType;
    uint64_t sendBuffer;
    uint64_t recvBuffer;
    uint64_t dataCnt;


    uint64_t strideCount;
    HcclDataType hcclDataType;
    uint32_t p2pSrcDestRankId;

    uint32_t valid;
    uint8_t repeatCnt;
    uint8_t everyTurnRsp;
    uint8_t everyTurnWait;

    int8_t commDepGroupID;

    HcclHandle commDepHandleID;
    HcclHandle selfHandleID;
    uint8_t reserved[2];
    uint32_t xorCheck;
};


struct DataBlock {
    uint32_t data[16];
};

constexpr uint32_t U64_CNT_PER_CACHELINE = 8U;
constexpr uint8_t HCCL_MSG_EXT_RESERVED_CNT = 6U;



struct HcclMsgExt {

    uint64_t sendCounts[HCCL_MAX_RANK_NUM_V2];

    uint64_t sendOffset[HCCL_MAX_RANK_NUM_V2];

    uint64_t recvCounts[HCCL_MAX_RANK_NUM_V2];

    uint64_t recvOffset[HCCL_MAX_RANK_NUM_V2];
    uint64_t reserved[HCCL_MSG_EXT_RESERVED_CNT];
    uint64_t valid;
    uint64_t xorCheck;
};

constexpr uint64_t COMMIT_VALID_MASK = 987654321U;
constexpr uint64_t FINALIZE_FINISH_CNT = 1234567899999999999UL;


struct TurnCnt {
    uint64_t valid;
    uint64_t cnt;
    uint64_t reserved[6];
};

constexpr uint32_t BYTE_PER_KB = 1024U;
constexpr uint32_t BYTE_PER_MB = BYTE_PER_KB * BYTE_PER_KB;




struct HcclMsgArea {
    HcclMsg sendMsgList[HCCL_MSG_CNT];
    HcclMsg recvMsgLsit[HCCL_MSG_CNT];
    uint8_t reserved0[8 * BYTE_PER_KB];





    TurnCnt commitTurnCnt[HCCL_MSG_CNT];
    TurnCnt finishedTurnCnt[HCCL_MSG_CNT];
    uint8_t reserved1[BYTE_PER_MB];
    HcclMsgExt paramExtMsgList[HCCL_MSG_CNT];
};

struct HcclCombineOpParam {
    uint64_t workSpace;
    uint64_t workSpaceSize;
    uint32_t rankId;
    uint32_t rankNum;
    uint64_t winSize;


    uint64_t windowsIn[HCCL_MAX_RANK_NUM];


    uint64_t windowsOut[HCCL_MAX_RANK_NUM];

};
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl_def.h" 2

namespace AscendC {
template <HcclServerType serverType = HcclServerType::HCCL_SERVER_TYPE_AICPU>
class HcclImpl {
public:
    template<bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                           HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1);

    template<bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                           HcclDataType dataType, uint64_t strideCount, uint8_t repeat = 1);

    template<bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
                                               HcclDataType dataType, HcclReduceOp op, uint64_t strideCount,
                                               uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                          HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                           __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                                           uint8_t repeat = 1);
public:
    [aicore] __inline__ __attribute__((always_inline)) void Init(__attribute__((cce_global)) uint8_t* context);

    [aicore] __inline__ __attribute__((always_inline)) void Commit(HcclHandle handleId);

    [aicore] __inline__ __attribute__((always_inline)) int32_t Wait(HcclHandle handleId);

    [aicore] __inline__ __attribute__((always_inline)) int32_t Query(HcclHandle handleId);

    [aicore] __inline__ __attribute__((always_inline)) void InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID);

    [aicore] __inline__ __attribute__((always_inline)) void Finalize();

public:
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId);

    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId);

    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankId();

    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankDim();

private:
    struct AlltoAllVParamExt {
        uint64_t *sendCounts;
        uint64_t *sdispls;
        uint64_t *recvCounts;
        uint64_t *rdispls;
    };
    struct CommonPrepareParam {
        HcclCMDType commType;
        __attribute__((cce_global)) uint8_t* sendBuf;
        __attribute__((cce_global)) uint8_t* recvBuf;
        uint64_t count;
        HcclDataType dataType;
        HcclReduceOp op;
        uint64_t strideCount;
        uint8_t repeat;
        AlltoAllVParamExt paramExt;
    };



    template<bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle CommonPrepareImpl(const CommonPrepareParam &commonPrepareParam);


    [aicore] __inline__ __attribute__((always_inline)) void
    AssembleHcclSendMsg(const CommonPrepareParam &commonPrepareParam, __attribute__((cce_global)) HcclMsg *hcclSendMsg);

    [aicore] __inline__ __attribute__((always_inline)) void AssembleFinalizeMsg(__attribute__((cce_global)) HcclMsg *hcclSendMsg);

    [aicore] __inline__ __attribute__((always_inline)) bool CheckCommonPrepareParamValid(const CommonPrepareParam &commonPrepareParam);


    [aicore] __inline__ __attribute__((always_inline)) void UpdateMsgPosAndFlag();




    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) HcclMsg *WaitForAvailableHcclSendMsg();

    [aicore] __inline__ __attribute__((always_inline)) bool IsDataCountsAllZero(const uint64_t *dataCounts);

    [aicore] __inline__ __attribute__((always_inline)) bool CheckAlltoAllVParamExtValid(const AlltoAllVParamExt &paramExt);

    [aicore] __inline__ __attribute__((always_inline)) void AssembleHcclMsgExt(const AlltoAllVParamExt &paramExt, __attribute__((cce_global)) HcclMsgExt *hcclMsgExt);

    [aicore] __inline__ __attribute__((always_inline)) uint64_t GenXorForHcclMsgExt(const AlltoAllVParamExt &paramExt);

    [aicore] __inline__ __attribute__((always_inline)) uint32_t GenXorForHcclMsg(HcclMsg *msg);

    [aicore] __inline__ __attribute__((always_inline)) void CopyHcclMsg(__attribute__((cce_global)) HcclMsg *dst, HcclMsg *src);


    [aicore] __inline__ __attribute__((always_inline)) void ResetFinishedTurnCnt();


    [aicore] __inline__ __attribute__((always_inline)) void FlushDataCache(GlobalTensor<int64_t> &globalHcclMsgArea, __attribute__((cce_global)) void *gmAddr);


    [aicore] __inline__ __attribute__((always_inline)) void FlushDataCache(__attribute__((cce_global)) void *gmAddr);

private:
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext_;
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea_;






    int32_t workBlockIdx_;



    uint8_t curMsgPosition_;

    bool isHcclMsgCntReached_;


    HcclHandle curHandleId_;
    int8_t handleIdMsgPosition_[HCCL_MAX_HANDLE_ID];
    uint8_t handleIdCommitTurnCnt_[HCCL_MAX_HANDLE_ID];
    uint8_t handleIdRepeat_[HCCL_MAX_HANDLE_ID];
    uint8_t handleIdWaitCallNum_[HCCL_MAX_HANDLE_ID];
    bool isInited_ = false;
};
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 2

namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
template <HcclServerType serverType = HcclServerType::HCCL_SERVER_TYPE_AICPU>
class Hccl {
public:
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllReduce(
        __attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count, HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1);
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                           HcclDataType dataType, uint64_t strideCount, uint8_t repeat = 1);
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
        HcclDataType dataType, HcclReduceOp op, uint64_t strideCount, uint8_t repeat = 1);
# 148 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    template <bool commit = false>
    [aicore] __inline__ __attribute__((always_inline)) HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                          HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1);
public:






    [aicore] __inline__ __attribute__((always_inline)) void Init(__attribute__((cce_global)) uint8_t* context);
# 169 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) void Commit(HcclHandle handleId);
# 181 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) int32_t Wait(HcclHandle handleId);
# 191 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) int32_t Query(HcclHandle handleId);
# 201 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) void Finalize();

public:
# 213 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId);
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId);
# 233 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankId();
# 242 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h"
    [aicore] __inline__ __attribute__((always_inline)) uint32_t GetRankDim();

private:
    HcclImpl<serverType> impl_;
};
}

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_v220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_v220_impl.h"
namespace AscendC {
template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void
HcclImpl<serverType>::FlushDataCache(GlobalTensor<int64_t> &globalHcclMsgArea, __attribute__((cce_global)) void *gmAddr)
{
    AscendC::Barrier();
    globalHcclMsgArea.SetGlobalBuffer((__attribute__((cce_global)) int64_t *)gmAddr);
    __asm__("NOP");
    DataCacheCleanAndInvalid<int64_t, CacheLine::SINGLE_CACHE_LINE, DcciDst::CACHELINE_OUT>(globalHcclMsgArea);
    dsb(DSB_ALL);
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::FlushDataCache(__attribute__((cce_global)) void *gmAddr)
{
    GlobalTensor<int64_t> globalHcclMsgArea;
    FlushDataCache(globalHcclMsgArea, gmAddr);
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::CopyHcclMsg(__attribute__((cce_global)) HcclMsg *dst, HcclMsg *src)
{
    if (dst == nullptr || src == nullptr) {
        return;
    }
    dst->commType = src->commType;
    dst->opType = src->opType;
    dst->sendBuffer = src->sendBuffer;
    dst->recvBuffer = src->recvBuffer;
    dst->dataCnt = src->dataCnt;
    dst->strideCount = src->strideCount;
    dst->hcclDataType = src->hcclDataType;
    dst->p2pSrcDestRankId = src->p2pSrcDestRankId;
    dst->repeatCnt = src->repeatCnt;
    dst->everyTurnRsp = src->everyTurnRsp;
    dst->everyTurnWait = src->everyTurnWait;
    dst->commDepGroupID = src->commDepGroupID;
    dst->commDepHandleID = src->commDepHandleID;
    dst->selfHandleID = src->selfHandleID;
    dst->reserved[0] = src->reserved[0];
    dst->reserved[1] = src->reserved[1];
    dst->xorCheck = src->xorCheck;
    dst->valid = src->valid;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void
HcclImpl<serverType>::AssembleHcclSendMsg(const CommonPrepareParam &commonPrepareParam, __attribute__((cce_global)) HcclMsg *hcclSendMsg)
{
    HcclMsg msg;
    msg.commType = commonPrepareParam.commType;
    msg.opType = commonPrepareParam.op;
    msg.sendBuffer = uint64_t(commonPrepareParam.sendBuf);
    msg.recvBuffer = uint64_t(commonPrepareParam.recvBuf);
    msg.dataCnt = commonPrepareParam.count;
    msg.strideCount = commonPrepareParam.strideCount;
    msg.hcclDataType = commonPrepareParam.dataType;
    msg.p2pSrcDestRankId = 0U;
    msg.repeatCnt = commonPrepareParam.repeat;
    msg.everyTurnRsp = 1;
    msg.everyTurnWait = 1;
    msg.commDepGroupID = -1;
    msg.commDepHandleID = -1;
    msg.selfHandleID = curHandleId_;
    msg.reserved[0] = 0;
    msg.reserved[1] = 0;
    msg.valid = HCCL_MSG_VALID_MASK;
    auto xorVal = GenXorForHcclMsg(&msg);
    msg.xorCheck = xorVal;
    CopyHcclMsg(hcclSendMsg, &msg);
    FlushDataCache(hcclSendMsg);
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::AssembleFinalizeMsg(__attribute__((cce_global)) HcclMsg *hcclSendMsg)
{
    HcclMsg msg;
    msg.commType = HcclCMDType::HCCL_CMD_FINALIZE;
    msg.opType = HcclReduceOp::HCCL_REDUCE_RESERVED;
    msg.sendBuffer = 0UL;
    msg.recvBuffer = 0UL;
    msg.dataCnt = 0UL;
    msg.strideCount = 0UL;
    msg.hcclDataType = HcclDataType::HCCL_DATA_TYPE_RESERVED;
    msg.p2pSrcDestRankId = 0U;
    msg.repeatCnt = 0;
    msg.everyTurnRsp = 0;
    msg.everyTurnWait = 0;
    msg.commDepGroupID = -1;
    msg.commDepHandleID = -1;
    msg.selfHandleID = INVALID_HANDLE_ID;
    msg.reserved[0] = 0;
    msg.reserved[1] = 0;
    msg.valid = HCCL_MSG_VALID_MASK;
    auto xorVal = GenXorForHcclMsg(&msg);
    msg.xorCheck = xorVal;
    CopyHcclMsg(hcclSendMsg, &msg);
    FlushDataCache(hcclSendMsg);
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) bool HcclImpl<serverType>::CheckCommonPrepareParamValid(const CommonPrepareParam &commonPrepareParam)
{
    if ((commonPrepareParam.sendBuf == nullptr) || (commonPrepareParam.recvBuf == nullptr)) {


                                                                     ;
        return false;
    }
    if ((commonPrepareParam.count == 0) && (commonPrepareParam.commType != HcclCMDType::HCCL_CMD_ALLTOALLV)) {


                                                                                                                     ;
        return false;
    }
    if ((commonPrepareParam.commType == HcclCMDType::HCCL_CMD_ALLTOALLV) &&
        (!CheckAlltoAllVParamExtValid(commonPrepareParam.paramExt))) {
        return false;
    }
    if ((commonPrepareParam.dataType < HCCL_DATA_TYPE_INT8) ||
        (commonPrepareParam.dataType >= HCCL_DATA_TYPE_RESERVED)) {


                                                                     ;
        return false;
    }
    if (commonPrepareParam.repeat == 0) {


                                                                     ;
        return false;
    }
    return true;
}

template<HcclServerType serverType>
template<bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType>::CommonPrepareImpl(const CommonPrepareParam &commonPrepareParam)
{
    if (!CheckCommonPrepareParamValid(commonPrepareParam)) {
        return INVALID_HANDLE_ID;
    }

    HcclHandle handleId = curHandleId_;
    if (handleId >= HCCL_MAX_HANDLE_ID) {




                                      ;
        return INVALID_HANDLE_ID;
    }

    if (GetBlockIdx() == workBlockIdx_) {

                                                                                                     ;
        __attribute__((cce_global)) HcclMsg *hcclSendMsg = WaitForAvailableHcclSendMsg();
        if (commonPrepareParam.commType == HcclCMDType::HCCL_CMD_ALLTOALLV) {
            auto hcclMsgExt = &(hcclMsgArea_->paramExtMsgList[curMsgPosition_]);
            AssembleHcclMsgExt(commonPrepareParam.paramExt, hcclMsgExt);
        }
        AssembleHcclSendMsg(commonPrepareParam, hcclSendMsg);
    }

    handleIdMsgPosition_[handleId] = curMsgPosition_;
    handleIdRepeat_[handleId] = commonPrepareParam.repeat;
    if constexpr (commit) {
        handleIdCommitTurnCnt_[handleId] = commonPrepareParam.repeat;
        if (GetBlockIdx() == workBlockIdx_) {
            __attribute__((cce_global)) TurnCnt *commitGM = hcclMsgArea_->commitTurnCnt + curMsgPosition_;
            do {
                FlushDataCache(commitGM);
            } while (commitGM->cnt != 0);


                                                                                                         ;
            commitGM->cnt = commonPrepareParam.repeat;
            commitGM->valid = COMMIT_VALID_MASK;
            FlushDataCache(commitGM);
        }
    }
    UpdateMsgPosAndFlag();
    curHandleId_++;
    return handleId;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::UpdateMsgPosAndFlag()
{
    curMsgPosition_++;
    if (curMsgPosition_ == HCCL_MSG_CNT) {
        curMsgPosition_ = 0;
        isHcclMsgCntReached_ = true;
    }
}

template<HcclServerType serverType>
template<bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType>::AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                HcclDataType dataType, HcclReduceOp op, uint8_t repeat)
{
    if (!isInited_) {

                                                                                                                ;
        return INVALID_HANDLE_ID;
    }
    if ((op < HCCL_REDUCE_SUM) || (op >= HCCL_REDUCE_RESERVED)) {

                                                                                                         ;
        return INVALID_HANDLE_ID;
    }
    return CommonPrepareImpl<commit>({
        HcclCMDType::HCCL_CMD_ALLREDUCE,
        sendBuf,
        recvBuf,
        count,
        dataType,
        op,
        0,
        repeat
    });
}

template<HcclServerType serverType>
template<bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType>::AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{
    if (!isInited_) {

                                                                                                                ;
        return INVALID_HANDLE_ID;
    }
    return CommonPrepareImpl<commit>({
        HcclCMDType::HCCL_CMD_ALLGATHER,
        sendBuf,
        recvBuf,
        sendCount,
        dataType,
        HcclReduceOp::HCCL_REDUCE_RESERVED,
        strideCount,
        repeat
    });
}

template<HcclServerType serverType>
template<bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType>::ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
                                    HcclDataType dataType, HcclReduceOp op, uint64_t strideCount, uint8_t repeat)
{
    if (!isInited_) {

                                                                                                                    ;
        return INVALID_HANDLE_ID;
    }
    if ((op < HCCL_REDUCE_SUM) || (op >= HCCL_REDUCE_RESERVED)) {

                                                                                                             ;
        return INVALID_HANDLE_ID;
    }
    return CommonPrepareImpl<commit>({
        HcclCMDType::HCCL_CMD_REDUCE_SCATTER,
        sendBuf,
        recvBuf,
        recvCount,
        dataType,
        op,
        strideCount,
        repeat
    });
}

template<HcclServerType serverType>
template<bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle HcclImpl<serverType>::AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                                            HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{
    if (!isInited_) {

                                                                                                               ;
        return INVALID_HANDLE_ID;
    }
    return CommonPrepareImpl<commit>({
        HcclCMDType::HCCL_CMD_ALLTOALL,
        sendBuf,
        recvBuf,
        dataCount,
        dataType,
        HcclReduceOp::HCCL_REDUCE_RESERVED,
        strideCount,
        repeat
    });
}

template<HcclServerType serverType>
template<bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
HcclImpl<serverType>::AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType, uint8_t repeat)
{
    if (!isInited_) {

                                                                                                                ;
        return INVALID_HANDLE_ID;
    }
    if (sendType != recvType) {

                                                                                  ;
        return INVALID_HANDLE_ID;
    }
    AlltoAllVParamExt paramExt = {
        static_cast<uint64_t *>(sendCounts),
        static_cast<uint64_t *>(sdispls),
        static_cast<uint64_t *>(recvCounts),
        static_cast<uint64_t *>(rdispls)
    };
    CommonPrepareParam commonPrepareParam = {
        HcclCMDType::HCCL_CMD_ALLTOALLV,
        sendBuf,
        recvBuf,
        0,
        sendType,
        HCCL_REDUCE_RESERVED,
        0,
        repeat,
        paramExt
    };
    return CommonPrepareImpl<commit>(commonPrepareParam);
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::Init(__attribute__((cce_global)) uint8_t* context)
{
    if (context == nullptr) {
                                                                             ;
        isInited_ = false;
        return;
    }
    hcclContext_ = (__attribute__((cce_global)) HcclCombineOpParam*) context;

    uint64_t msgAddr = hcclContext_->workSpace;
    if (msgAddr & 0x1ff) {
        msgAddr = (msgAddr & (~((uint64_t)0x1ff))) + 0x200;
    }
    hcclMsgArea_ = (__attribute__((cce_global)) HcclMsgArea*) msgAddr;
    workBlockIdx_ = 0;
    curMsgPosition_ = 0;
    isHcclMsgCntReached_ = false;
    curHandleId_ = 0;

    for (uint32_t i = 0U; i < HCCL_MAX_HANDLE_ID; ++i) {
        handleIdMsgPosition_[i] = INVALID_MSG_POSITION;
        handleIdCommitTurnCnt_[i] = 0;
        handleIdRepeat_[i] = 0;
        handleIdWaitCallNum_[i] = 0;
    }
    if (hcclMsgArea_ == nullptr) {
                                                                               ;
        isInited_ = false;
        return;
    }
    isInited_ = true;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) int32_t HcclImpl<serverType>::Wait(HcclHandle handleId)
{
    if (!isInited_) {

                                                                                                           ;
        return HCCL_FAILED;
    }
    if ((handleId <= INVALID_HANDLE_ID) || (handleId >= HCCL_MAX_HANDLE_ID)) {


                                                ;
        return HCCL_FAILED;
    }
    if ((handleIdWaitCallNum_[handleId] >= handleIdCommitTurnCnt_[handleId]) ||
        (handleIdWaitCallNum_[handleId] >= handleIdRepeat_[handleId])) {





                                             ;
        return HCCL_FAILED;
    }
    handleIdWaitCallNum_[handleId]++;
    int8_t curMsgPos = handleIdMsgPosition_[handleId];
    if (curMsgPos == INVALID_MSG_POSITION) {
                                                                                                              ;
        return HCCL_FAILED;
    }
    GlobalTensor<int64_t> globalHcclMsgArea;
    while (true) {
        __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->finishedTurnCnt + curMsgPos;
        FlushDataCache(globalHcclMsgArea, finishGM);
        if (finishGM->cnt >= handleIdWaitCallNum_[handleId]) {



                                                 ;
            break;
        }
    }
    return HCCL_SUCCESS;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) int32_t HcclImpl<serverType>::Query(HcclHandle handleId)
{
    if (!isInited_) {

                                                                                                            ;
        return HCCL_FAILED;
    }
    if ((handleId <= INVALID_HANDLE_ID) || handleId >= HCCL_MAX_HANDLE_ID) {



                                      ;
        return HCCL_FAILED;
    }
    int8_t curMsgPos = handleIdMsgPosition_[handleId];
    if (curMsgPos == INVALID_MSG_POSITION) {
                                                                                                           ;
        return HCCL_FAILED;
    }
    __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->finishedTurnCnt + curMsgPos;
    FlushDataCache(finishGM);
    uint64_t curFinishCount = finishGM->cnt;

                                                                              ;
    if (curFinishCount >= handleIdRepeat_[handleId]) {
        return handleIdRepeat_[handleId];
    } else {
        return curFinishCount;
    }
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::Commit(HcclHandle handleId)
{
    if (!isInited_) {

                                                                                                             ;
        return;
    }
    if ((handleId <= INVALID_HANDLE_ID) || handleId >= HCCL_MAX_HANDLE_ID) {



                                      ;
        return;
    }
    if (handleIdCommitTurnCnt_[handleId] >= handleIdRepeat_[handleId]) {


                                                                                      ;
        return;
    }
    handleIdCommitTurnCnt_[handleId]++;
    int8_t curMsgPos = handleIdMsgPosition_[handleId];
    if (curMsgPos == INVALID_MSG_POSITION) {
                                                                                                            ;
        return;
    }
    if (GetBlockIdx() == workBlockIdx_) {
        __attribute__((cce_global)) TurnCnt *commitGM = hcclMsgArea_->commitTurnCnt + curMsgPos;
        do {
            FlushDataCache(commitGM);
        } while (commitGM->cnt >= handleIdCommitTurnCnt_[handleId]);


                                                      ;
        commitGM->cnt = handleIdCommitTurnCnt_[handleId];
        commitGM->valid = COMMIT_VALID_MASK;
        FlushDataCache(commitGM);
    }
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID)
{
    if (!isInited_) {

                                                                                                       ;
        return;
    }

    if (GetBlockIdx() == workBlockIdx_) {

                                                  ;
        __attribute__((cce_global)) HcclMsg *hcclSendMsg = WaitForAvailableHcclSendMsg();
        HcclMsg msg;
        msg.commType = HcclCMDType::HCCL_CMD_INTER_GROUP_SYNC;
        msg.opType = HcclReduceOp::HCCL_REDUCE_RESERVED;
        msg.sendBuffer = 0UL;
        msg.recvBuffer = 0UL;
        msg.dataCnt = 0UL;
        msg.strideCount = 0UL;
        msg.hcclDataType = HcclDataType::HCCL_DATA_TYPE_RESERVED;
        msg.p2pSrcDestRankId = 0U;
        msg.repeatCnt = 0;
        msg.everyTurnRsp = 0;
        msg.everyTurnWait = 0;
        msg.commDepGroupID = srcGroupID;
        msg.commDepHandleID = srcHandleID;
        msg.selfHandleID = INVALID_HANDLE_ID;
        msg.reserved[0] = 0;
        msg.reserved[1] = 0;
        msg.valid = HCCL_MSG_VALID_MASK;
        auto xorVal = GenXorForHcclMsg(&msg);
        msg.xorCheck = xorVal;
        CopyHcclMsg(hcclSendMsg, &msg);
        FlushDataCache(hcclSendMsg);
    }
    UpdateMsgPosAndFlag();
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::Finalize()
{
    if (!isInited_) {

                                                                                                               ;
        return;
    }

    if (GetBlockIdx() == workBlockIdx_) {


        if (curHandleId_ > 0) {
            auto lastHandleId = curHandleId_ - 1;
                                                                                                                 ;
            while (Query(lastHandleId) < handleIdRepeat_[lastHandleId]) {}
        }


                                                  ;
        __attribute__((cce_global)) HcclMsg *hcclSendMsg = WaitForAvailableHcclSendMsg();
        AssembleFinalizeMsg(hcclSendMsg);

                                                                                                            ;
        do {
            FlushDataCache(hcclSendMsg);
        } while (hcclSendMsg->valid == HCCL_MSG_VALID_MASK);

        __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->finishedTurnCnt + curMsgPosition_;
                                                                                                        ;
        do {
            FlushDataCache(finishGM);
        } while (finishGM->cnt != FINALIZE_FINISH_CNT);

                                                                                                ;
        ResetFinishedTurnCnt();
    }
    UpdateMsgPosAndFlag();
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* HcclImpl<serverType>::GetWindowsInAddr(uint32_t rankId)
{
    if (rankId >= GetRankDim()) {


                                        ;
        return nullptr;
    }
    return (__attribute__((cce_global)) uint8_t*) hcclContext_->windowsIn[rankId];
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* HcclImpl<serverType>::GetWindowsOutAddr(uint32_t rankId)
{
    if (rankId >= GetRankDim()) {


                                        ;
        return nullptr;
    }
    return (__attribute__((cce_global)) uint8_t*) hcclContext_->windowsOut[rankId];
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) uint32_t HcclImpl<serverType>::GetRankId()
{
    return hcclContext_->rankId;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) uint32_t HcclImpl<serverType>::GetRankDim()
{
    return hcclContext_->rankNum;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::ResetFinishedTurnCnt()
{
    __attribute__((cce_global)) TurnCnt *finishArea = hcclMsgArea_->finishedTurnCnt;
    const uint32_t kResetCnt = isHcclMsgCntReached_ ? HCCL_MSG_CNT : curMsgPosition_ + 1;
    GlobalTensor<int64_t> globalHcclMsgArea;
    for (uint32_t i = 0U; i < kResetCnt; ++i) {
        __attribute__((cce_global)) TurnCnt *finishGM = finishArea + i;
        finishGM->cnt = 0;
        FlushDataCache(globalHcclMsgArea, finishGM);
    }
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) HcclMsg *HcclImpl<serverType>::WaitForAvailableHcclSendMsg()
{
    __attribute__((cce_global)) HcclMsg *hcclSendMsg = hcclMsgArea_->sendMsgList + curMsgPosition_;
 if (!isHcclMsgCntReached_) {
        do {
            FlushDataCache(hcclSendMsg);
        } while (hcclSendMsg->valid == HCCL_MSG_VALID_MASK);
                                                                                 ;
  return hcclSendMsg;
 }
 GlobalTensor<int64_t> globalHcclMsgArea;
 while (true) {
  FlushDataCache(globalHcclMsgArea, hcclSendMsg);
  if (hcclSendMsg->valid == ~HCCL_MSG_VALID_MASK) {
                                                                                  ;
   return hcclSendMsg;
  }
 }
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) bool HcclImpl<serverType>::IsDataCountsAllZero(const uint64_t *dataCounts)
{
    const uint32_t kRankNum = hcclContext_->rankNum;
    for (uint32_t i = 0; i < kRankNum; ++i) {
        if (dataCounts[i] != 0) {
            return false;
        }
    }
    return true;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) bool HcclImpl<serverType>::CheckAlltoAllVParamExtValid(const AlltoAllVParamExt &paramExt)
{
    if ((paramExt.sendCounts == nullptr) || (paramExt.sdispls == nullptr) || (paramExt.recvCounts == nullptr) ||
        (paramExt.rdispls == nullptr)) {

                                                                                                             ;
        return false;
    }
    if (IsDataCountsAllZero(paramExt.sendCounts) || IsDataCountsAllZero(paramExt.recvCounts)) {
                                                                                                            ;
        return false;
    }
    return true;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void HcclImpl<serverType>::AssembleHcclMsgExt(const AlltoAllVParamExt &paramExt,
                                                                __attribute__((cce_global)) HcclMsgExt *hcclMsgExt)
{

    const uint32_t kRankNum = hcclContext_->rankNum;
    for (uint32_t i = 0U; i < kRankNum; ++i) {
        hcclMsgExt->sendCounts[i] = paramExt.sendCounts[i];
        hcclMsgExt->sendOffset[i] = paramExt.sdispls[i];
        hcclMsgExt->recvCounts[i] = paramExt.recvCounts[i];
        hcclMsgExt->recvOffset[i] = paramExt.rdispls[i];
    }
    for (uint8_t i = 0U; i < HCCL_MSG_EXT_RESERVED_CNT; ++i) {
        hcclMsgExt->reserved[i] = 0UL;
    }
    hcclMsgExt->xorCheck = GenXorForHcclMsgExt(paramExt);
    hcclMsgExt->valid = HCCL_MSG_VALID_MASK;

    GlobalTensor<int64_t> globalHcclMsgArea;
    for (uint32_t i = 0U; i < kRankNum; i += U64_CNT_PER_CACHELINE) {
        FlushDataCache(globalHcclMsgArea, (hcclMsgExt->sendCounts + i));
        FlushDataCache(globalHcclMsgArea, (hcclMsgExt->sendOffset + i));
        FlushDataCache(globalHcclMsgArea, (hcclMsgExt->recvCounts + i));
        FlushDataCache(globalHcclMsgArea, (hcclMsgExt->recvOffset + i));
    }
    FlushDataCache(globalHcclMsgArea, hcclMsgExt->reserved);
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) uint32_t HcclImpl<serverType>::GenXorForHcclMsg(HcclMsg *msg)
{
    DataBlock *block = reinterpret_cast<DataBlock *>(msg);
    constexpr uint32_t kBlockCntForXor = 15U;
    uint32_t xorVal = 0U;
    for (uint32_t i = 0; i < kBlockCntForXor; ++i) {
        xorVal ^= block->data[i];
    }
    return xorVal;
}

template<HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) uint64_t HcclImpl<serverType>::GenXorForHcclMsgExt(const AlltoAllVParamExt &paramExt)
{
    const uint32_t kRankNum = hcclContext_->rankNum;
    uint64_t xorVal = 0U;
    for (uint32_t i = 0U; i < kRankNum; ++i) {
        xorVal ^= paramExt.sendCounts[i];
        xorVal ^= paramExt.sdispls[i];
        xorVal ^= paramExt.recvCounts[i];
        xorVal ^= paramExt.rdispls[i];
    }
    xorVal ^= HCCL_MSG_VALID_MASK;
    return xorVal;
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/../../impl/hccl/hccl_impl.h" 2

namespace AscendC {
template <HcclServerType serverType>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType>::AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                                         HcclDataType dataType, HcclReduceOp op, uint8_t repeat)
{
    return impl_.template AllReduce<commit>(sendBuf, recvBuf, count, dataType, op, repeat);
}

template <HcclServerType serverType>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType>::AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                                         HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{
    return impl_.template AllGather<commit>(sendBuf, recvBuf, sendCount, dataType, strideCount, repeat);
}

template <HcclServerType serverType>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle Hccl<serverType>::AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                                        HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{
    return impl_.template AlltoAll<commit>(sendBuf, recvBuf, dataCount, dataType, strideCount, repeat);
}

template <HcclServerType serverType>
template <bool commit>
[aicore] __inline__ __attribute__((always_inline)) HcclHandle
Hccl<serverType>::ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount, HcclDataType dataType,
                                HcclReduceOp op, uint64_t strideCount, uint8_t repeat)
{
    return impl_.template ReduceScatter<commit>(sendBuf, recvBuf, recvCount, dataType, op, strideCount, repeat);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType>::Init(__attribute__((cce_global)) uint8_t* context)
{
    impl_.Init(context);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType>::Commit(HcclHandle handleId)
{
    impl_.Commit(handleId);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) int32_t Hccl<serverType>::Wait(HcclHandle handleId)
{
    return impl_.Wait(handleId);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) int32_t Hccl<serverType>::Query(HcclHandle handleId)
{
    return impl_.Query(handleId);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) void Hccl<serverType>::Finalize()
{
    impl_.Finalize();
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* Hccl<serverType>::GetWindowsInAddr(uint32_t rankId)
{
    return impl_.GetWindowsInAddr(rankId);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((cce_global)) uint8_t* Hccl<serverType>::GetWindowsOutAddr(uint32_t rankId)
{
    return impl_.GetWindowsOutAddr(rankId);
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) uint32_t Hccl<serverType>::GetRankId()
{
    return impl_.GetRankId();
}

template <HcclServerType serverType>
[aicore] __inline__ __attribute__((always_inline)) uint32_t Hccl<serverType>::GetRankDim()
{
    return impl_.GetRankDim();
}
}
# 250 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/hccl/hccl.h" 2
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_common_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TruncCastForFrac(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor)
{
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/frac/frac_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FracCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{

    TruncCastForFrac(dstTensor, srcTensor, dstTensor);

    Sub<T, false>(dstTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void FracCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    LocalTensor<float> srcTmpTensor = tmpTensor;
    LocalTensor<float> dstTmpTensor = tmpTensor[splitSize];


    Cast<float, half, false>(srcTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();

    TruncCastForFrac(dstTmpTensor, srcTmpTensor, dstTmpTensor);

    Sub<float, false>(dstTmpTensor, srcTmpTensor, dstTmpTensor, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dstTensor, dstTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FracImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                        ;



                                                                             ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);
                                                                                                 ;
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;
    uint32_t round = 1;
    uint32_t tail = 0;
    constexpr uint8_t FRAC_HALF_CALC_PROCEDURE = 2;
    if constexpr (sizeof(T) == sizeof(half)) {
        stackSize = tmpBufferSize / FRAC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                             ;
        round = calCount / stackSize;
        tail = calCount % stackSize;
        SetMaskCount();
        SetVectorMask<half, MaskMode::COUNTER>(0, stackSize);
    } else {
        SetMaskCount();
        SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    }

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        FracCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize);
        offset = offset + stackSize;
    }
    if (tail != 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        FracCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FracImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                        ;

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FracImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FracImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Frac<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Frac<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    FracImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_v220_impl.h"
namespace AscendC {

constexpr uint32_t TENSOR_TENSOR_FLOAT = 4;
constexpr uint32_t TENSOR_TENSOR_INT = 6;
constexpr uint32_t TENSOR_TENSOR_HALF = 7;
constexpr uint32_t TENSOR_SCALAR_FLOAT = 5;
constexpr uint32_t TENSOR_SCALAR_INT = 7;
constexpr uint32_t TENSOR_SCALAR_HALF = 7;


struct AscPowerFParams {
    [aicore] AscPowerFParams() {};
    LocalTensor<float> tmpTensor1;
    LocalTensor<float> tmpTensor2;
    LocalTensor<float> tmpTensor3;

    LocalTensor<uint8_t> tmpMask1;
    LocalTensor<uint8_t> tmpMask2;
    LocalTensor<uint8_t> tmpMask3;
    LocalTensor<uint8_t> finiteIntegerYMask;
};


struct AscPowerIParams {
    [aicore] AscPowerIParams() {};
    float expIterateSum;

    LocalTensor<int32_t> expUBIterate;
    LocalTensor<int32_t> oriAbsExp;
    LocalTensor<int32_t> recordExpNode;
    LocalTensor<int32_t> tmpTensor1;
    LocalTensor<int32_t> tmpTensor2;

    LocalTensor<uint8_t> negMask;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> tmpScalar;
};


[aicore] __inline__ __attribute__((always_inline)) void PowerIParamsCalc(const LocalTensor<uint8_t>& tmpTensor, AscPowerIParams& param,
    uint32_t splitSize)
{
    param.expUBIterate = tmpTensor.ReinterpretCast<int32_t>();
    param.oriAbsExp = param.expUBIterate[splitSize];
    param.recordExpNode = param.oriAbsExp[splitSize];
    param.tmpTensor1 = param.recordExpNode[splitSize];
    param.tmpTensor2 = param.tmpTensor1[splitSize];
    param.negMask = param.tmpTensor2[splitSize].ReinterpretCast<uint8_t>();
    param.mask = param.negMask[splitSize];
    param.tmpScalar = param.mask[splitSize];
    param.expUBIterate.SetSize(splitSize);
    param.oriAbsExp.SetSize(splitSize);
    param.recordExpNode.SetSize(splitSize);
    param.tmpTensor1.SetSize(splitSize);
    param.tmpTensor2.SetSize(splitSize);
    param.negMask.SetSize(splitSize);
    param.mask.SetSize(splitSize);
    param.tmpScalar.SetSize(ONE_BLK_SIZE);
}


[aicore] __inline__ __attribute__((always_inline)) void PowerFParamsCalc(const LocalTensor<float>& tmpTensor,
    AscPowerFParams& param, uint32_t splitSize)
{
    param.tmpTensor1 = tmpTensor;
    param.tmpTensor2 = tmpTensor[splitSize];
    param.tmpTensor3 = param.tmpTensor2[splitSize];
    param.tmpMask1 = param.tmpTensor3[splitSize].ReinterpretCast<uint8_t>();
    param.tmpMask2 = param.tmpMask1[splitSize];
    param.tmpMask3 = param.tmpMask2[splitSize];
    param.finiteIntegerYMask = param.tmpMask3[splitSize];
    param.tmpTensor1.SetSize(splitSize);
    param.tmpTensor2.SetSize(splitSize);
    param.tmpTensor3.SetSize(splitSize);
    param.tmpMask1.SetSize(splitSize);
    param.tmpMask2.SetSize(splitSize);
    param.tmpMask3.SetSize(splitSize);
    param.finiteIntegerYMask.SetSize(splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void VselPowerTensorScalar(const LocalTensor<float>& dst, const LocalTensor<uint8_t>& sel,
    const LocalTensor<float>& src0, const LocalTensor<float>& tmpScalar,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams &binaryParam, const uint32_t calCount)
{
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();

    Select<float, uint8_t>(dst, sel, src0, repeat, binaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void VselPowerTensorTensor(const LocalTensor<float>& dst, const LocalTensor<uint8_t>& sel,
    const LocalTensor<float>& src0, const LocalTensor<float>& src1, const LocalTensor<float>& tmpScalar,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{




    uint32_t selAddr = static_cast<uint32_t>(
        reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel.GetPhyAddr())));
    SetVectorMask<uint32_t>(0, 1);
    Duplicate<uint32_t, false>(tmpScalar.ReinterpretCast<uint32_t>(), selAddr, MASK_PLACEHOLDER, 1,
        DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, calCount);
    SetCmpMask<int64_t>(tmpScalar.ReinterpretCast<int64_t>());
    PipeBarrier<PIPE_V>();
    Select<float, SELMODE::VSEL_TENSOR_TENSOR_MODE>(dst, src0, src1, repeat, binaryParam);
}






[aicore] __inline__ __attribute__((always_inline)) void InitDst(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam)
{
    Abs<float, false>(dst, src0, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dst, src1, dst, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void DetermineSign(const LocalTensor<float>& src1, const AscPowerFParams& param,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam)
{
    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Cast<float, float, false>(
        param.tmpTensor1, param.tmpTensor1, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor2, param.tmpTensor1, 0.5f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Cast<float, float, false>(
        param.tmpTensor2, param.tmpTensor2, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor2, param.tmpTensor2, 2.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(param.tmpTensor1, param.tmpTensor1, param.tmpTensor2, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor1, param.tmpTensor1, -2.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(param.tmpTensor1, param.tmpTensor1, 1.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Cast<float, float, false>(
        param.tmpTensor1, param.tmpTensor1, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParam);
}


[aicore] __inline__ __attribute__((always_inline)) void GenMaskForSign(const LocalTensor<float>& src0, const LocalTensor<float>& src1,
    const AscPowerFParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    constexpr float intThreshold = 0.00000001f;
    constexpr uint32_t signBit = 31;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    ShiftRight<uint32_t, false>(param.tmpTensor2.ReinterpretCast<uint32_t>(), src0.ReinterpretCast<uint32_t>(), signBit,
        MASK_PLACEHOLDER, 1, unaryParam, false);
    PipeBarrier<PIPE_V>();
    CompareScalar<int32_t, uint8_t, false>(param.tmpMask1, param.tmpTensor2.ReinterpretCast<int32_t>(),
        static_cast<int32_t>(1), CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(param.tmpTensor2, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Cast<float, float, false>(
        param.tmpTensor3, param.tmpTensor2, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(param.tmpTensor3, param.tmpTensor2, param.tmpTensor3, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(param.tmpTensor3, param.tmpTensor3, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.finiteIntegerYMask, param.tmpTensor3, static_cast<float>(intThreshold),
        CMPMODE::LT, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(),
        param.finiteIntegerYMask.ReinterpretCast<uint16_t>(), param.tmpMask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}


[aicore] __inline__ __attribute__((always_inline)) void GenMaskForOne(const LocalTensor<float>& src0, const LocalTensor<float>& src1,
    const AscPowerFParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    NotNumUnion notNum;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask1, param.tmpTensor1, static_cast<float>(0), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(1), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(-1), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    notNum.i = F32_INF;
    CompareScalar<float, uint8_t, false>(param.tmpMask3, param.tmpTensor1, notNum.f, CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask2.ReinterpretCast<uint16_t>(), param.tmpMask3.ReinterpretCast<uint16_t>(),
        param.tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}

[aicore] __inline__ __attribute__((always_inline)) void GenMaskForNan(const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const AscPowerFParams& param,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    NotNumUnion notNum;
    notNum.i = F32_INF;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask1, param.tmpTensor1, notNum.f, CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(),
        param.finiteIntegerYMask.ReinterpretCast<uint16_t>(), param.tmpMask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(0), CMPMODE::GE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    notNum.i = F32_NEG_INF;
    CompareScalar<float, uint8_t, false>(param.tmpMask3, src0, notNum.f, CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask2.ReinterpretCast<uint16_t>(), param.tmpMask3.ReinterpretCast<uint16_t>(),
        param.tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Or<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}

[aicore] __inline__ __attribute__((always_inline)) void CommonPowerF(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1, const LocalTensor<float>& tmpScalar,
    const AscPowerFParams& powerParam, const uint32_t calCount)
{
    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;

    PipeBarrier<PIPE_V>();
    InitDst(dstTensor, srcTensor0, srcTensor1, unaryParam, binaryParam);
    PipeBarrier<PIPE_V>();
    DetermineSign(srcTensor1, powerParam, unaryParam, binaryParam);
    PipeBarrier<PIPE_V>();
    GenMaskForSign(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(powerParam.tmpTensor2, powerParam.tmpMask1, powerParam.tmpTensor1, tmpScalar,
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, powerParam.tmpTensor2, dstTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    GenMaskForOne(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(dstTensor, powerParam.tmpMask1, dstTensor, tmpScalar,
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    GenMaskForNan(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(dstTensor, powerParam.tmpMask1, dstTensor, tmpScalar[ONE_BLK_SIZE / sizeof(float)],
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
}





[aicore] __inline__ __attribute__((always_inline)) void InitBulkPowerI(AscPowerIParams& param, const LocalTensor<int32_t>& src0,
    const LocalTensor<int32_t>& src1, const LocalTensor<int32_t>& dst, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(
        param.tmpTensor1.ReinterpretCast<float>(), src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.negMask, param.tmpTensor1.ReinterpretCast<float>(),
        static_cast<float>(0), CMPMODE::LT, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<int32_t, false>(param.tmpTensor1, src1, -1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(param.expUBIterate.ReinterpretCast<float>(), param.negMask,
        param.tmpTensor1.ReinterpretCast<float>(), src1.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Muls<int32_t, false>(param.oriAbsExp, param.expUBIterate, 1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<int32_t, uint8_t, false>(param.mask, param.oriAbsExp, static_cast<int32_t>(0),
        CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
    Duplicate<int32_t, false>(param.recordExpNode, 0, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(param.tmpTensor2, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
        param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, param.tmpTensor2.ReinterpretCast<float>(),
        src0.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
    ShiftRight<int32_t, false>(param.expUBIterate, param.expUBIterate, 1, MASK_PLACEHOLDER, 1, unaryParam, false);
    PipeBarrier<PIPE_V>();
    CompareScalar<int32_t, uint8_t, false>(param.mask, param.expUBIterate, static_cast<int32_t>(0),
        CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
}

[aicore] __inline__ __attribute__((always_inline)) void BulkProcessPowerI(const LocalTensor<int32_t>& dst, AscPowerIParams& param,
    const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(param.tmpTensor1.ReinterpretCast<float>(), param.expUBIterate,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    ReduceSum<float, false>(param.tmpScalar.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), calCount);
    param.expIterateSum = param.tmpScalar.ReinterpretCast<float>().GetValue(0);
    PipeBarrier<PIPE_V>();
    if (param.expIterateSum != 0) {
        Mul<int32_t, false>(param.tmpTensor1, dst, dst, MASK_PLACEHOLDER, 1, binaryParam);
        int32_t scalarValue = 2;
        Muls<int32_t, false>(param.tmpTensor2, param.recordExpNode, scalarValue, MASK_PLACEHOLDER, 1, unaryParam);
        PipeBarrier<PIPE_V>();
        VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, dst.ReinterpretCast<float>(),
            param.tmpTensor1.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(),
            SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
            param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
            param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        ShiftRight<int32_t, false>(param.expUBIterate, param.expUBIterate, 1, MASK_PLACEHOLDER, 1, unaryParam, false);
        PipeBarrier<PIPE_V>();
        CompareScalar<int32_t, uint8_t, false>(param.mask, param.expUBIterate, static_cast<int32_t>(0),
            CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void InitFinePowerI(AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Sub<int32_t, false>(param.recordExpNode, param.oriAbsExp, param.recordExpNode, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<int32_t, uint8_t, false>(param.mask, param.recordExpNode, static_cast<int32_t>(0),
        CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
}

[aicore] __inline__ __attribute__((always_inline)) void FineProcessPowerI(const LocalTensor<int32_t>& dst, const LocalTensor<int32_t>& src0,
    AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(param.tmpTensor1.ReinterpretCast<float>(), param.recordExpNode,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    ReduceSum<float, false>(param.tmpScalar.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), calCount);
    param.expIterateSum = param.tmpScalar.ReinterpretCast<float>().GetValue(0);
    PipeBarrier<PIPE_V>();
    if (param.expIterateSum != 0) {
        Mul<int32_t, false>(param.tmpTensor1, dst, src0, MASK_PLACEHOLDER, 1, binaryParam);
        Adds<int32_t, false>(param.tmpTensor2, param.recordExpNode, -1, MASK_PLACEHOLDER, 1, unaryParam);
        PipeBarrier<PIPE_V>();
        VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, dst.ReinterpretCast<float>(),
            param.tmpTensor1.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(),
            SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
            param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
            param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        PipeBarrier<PIPE_V>();
        CompareScalar<int32_t, uint8_t, false>(param.mask, param.recordExpNode, static_cast<int32_t>(0),
            CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void HandleNegativeExpPowerI(const LocalTensor<int32_t>& dst, const LocalTensor<int32_t>& src0,
    AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    CompareScalar<int32_t, uint8_t, false>(param.mask, dst, static_cast<int32_t>(0),
        CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
    LocalTensor<float> resF32 = param.oriAbsExp.ReinterpretCast<float>();
    LocalTensor<float> oneTensor = param.expUBIterate.ReinterpretCast<float>();
    Cast<float, int32_t, false>(resF32, dst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    Duplicate<float, false>(oneTensor, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(resF32, oneTensor, resF32, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int32_t, float, false>(param.tmpTensor1, resF32, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.negMask, param.tmpTensor1.ReinterpretCast<float>(),
        dst.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
    Duplicate<int32_t, false>(param.tmpTensor2, 0, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, param.tmpTensor2.ReinterpretCast<float>(),
        dst.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
}

[aicore] __inline__ __attribute__((always_inline)) void CommonPowerI(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const LocalTensor<int32_t>& srcTensor1, AscPowerIParams& param, const uint32_t calCount)
{
    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;
    const uint8_t repeat = DivCeil(calCount * sizeof(int32_t), ONE_REPEAT_BYTE_SIZE);

    PipeBarrier<PIPE_V>();
    InitBulkPowerI(param, srcTensor0, srcTensor1, dstTensor, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
    param.expIterateSum = 1;
    do {
        BulkProcessPowerI(dstTensor, param, unaryParam, binaryParam, repeat, calCount);
        PipeBarrier<PIPE_V>();
    } while (param.expIterateSum != 0);
    InitFinePowerI(param, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
    do {
        FineProcessPowerI(dstTensor, srcTensor0, param, unaryParam, binaryParam, repeat, calCount);
        PipeBarrier<PIPE_V>();
    } while (param.expIterateSum != 0);
    HandleNegativeExpPowerI(dstTensor, srcTensor0, param, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/power/power_common_impl.h" 2




namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void InitTmpScalar(const LocalTensor<float>& tmpScalar)
{
    NotNumUnion notNum;
    notNum.i = F32_NAN;
    SetVectorMask<float>(0, ONE_BLK_SIZE / sizeof(float));
    Duplicate<float, false>(tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    Duplicate<float, false>(
        tmpScalar[ONE_BLK_SIZE / sizeof(float)], notNum.f, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor0,
    const LocalTensor<half>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;


      ;
    uint32_t splitSize = (stackTensor.GetSize() - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_TENSOR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<half>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(
            stackSrc0, srcTensor0[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        Cast<float, half, false>(
            stackSrc1, srcTensor1[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        Cast<float, half, false>(
            stackSrc0, srcTensor0[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        Cast<float, half, false>(
            stackSrc1, srcTensor1[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{


      ;
    uint32_t splitSize = (stackTensor.GetSize() - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_TENSOR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>(), powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        PipeBarrier<PIPE_V>();
        CommonPowerF(dstTensor[i * splitSize], srcTensor0[i * splitSize],
            srcTensor1[i * splitSize], tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            srcTensor1[loopCount * splitSize], tmpScalar, powerParam, calcTail);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const LocalTensor<int32_t>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t splitSize = stackTensor.GetSize() / sizeof(int32_t) /
        TENSOR_TENSOR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor, powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        PipeBarrier<PIPE_V>();
        CommonPowerI(dstTensor[i * splitSize], srcTensor0[i * splitSize], srcTensor1[i * splitSize],
            powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        PipeBarrier<PIPE_V>();
        CommonPowerI(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            srcTensor1[loopCount * splitSize], powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor0,
    const half& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;


      ;
    uint32_t splitSize = (stackTensor.GetSize() - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<half>(0, splitSize);
    Duplicate<float, false>(stackSrc1, static_cast<float>(scalarValue), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(
            stackSrc0, srcTensor0[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        Cast<float, half, false>(
            stackSrc0, srcTensor0[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const float& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{


      ;
    uint32_t splitSize = (stackTensor.GetSize() - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc1 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    stackSrc1.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE + splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc1, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerF(dstTensor[i * splitSize], srcTensor0[i * splitSize],
            stackSrc1, tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            stackSrc1, tmpScalar, powerParam, calcTail);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const int32_t& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t splitSize = stackTensor.GetSize() / sizeof(int32_t) /
        TENSOR_SCALAR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<int32_t> stackSrc1 = stackTensor.ReinterpretCast<int32_t>();
    stackSrc1.SetSize(splitSize);
    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor[splitSize * sizeof(int32_t)], powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(stackSrc1, scalarValue, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerI(dstTensor[i * splitSize], srcTensor0[i * splitSize], stackSrc1,
            powerParam, splitSize);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        CommonPowerI(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize], stackSrc1,
            powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<half>& dstTensor, const half& scalarValue,
    const LocalTensor<half>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;


      ;
    uint32_t splitSize = (stackTensor.GetSize() - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<half>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc0, static_cast<float>(scalarValue), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(
            stackSrc1, srcTensor1[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        Cast<float, half, false>(
            stackSrc1, srcTensor1[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<float>& dstTensor, const float& scalarValue,
    const LocalTensor<float>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{


      ;
    uint32_t splitSize = (stackTensor.GetSize() - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    stackSrc0.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE + splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc0, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerF(dstTensor[i * splitSize], stackSrc0, srcTensor1[i * splitSize],
            tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], stackSrc0, srcTensor1[loopCount * splitSize],
            tmpScalar, powerParam, calcTail);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void PowerImpl(const LocalTensor<int32_t>& dstTensor, const int32_t& scalarValue,
    const LocalTensor<int32_t>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t splitSize = stackTensor.GetSize() / sizeof(int32_t) /
        TENSOR_SCALAR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;

    LocalTensor<int32_t> stackSrc0 = stackTensor.ReinterpretCast<int32_t>();
    stackSrc0.SetSize(splitSize);
    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor[splitSize * sizeof(int32_t)], powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(stackSrc0, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerI(dstTensor[i * splitSize], stackSrc0, srcTensor1[i * splitSize],
            powerParam, splitSize);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        CommonPowerI(dstTensor[loopCount * splitSize], stackSrc0, srcTensor1[loopCount * splitSize],
            powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}





template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const T& scalarValue,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }



      ;
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value || std::is_same<T, int32_t>::value),
        "Power only support half/float/int32_t data type on current device!");

    SetMaskCount();
    PowerImpl(dstTensor, scalarValue, srcTensor1, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}





template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const T& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }



      ;
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value || std::is_same<T, int32_t>::value),
        "Power only support half/float/int32_t data type on current device!");

    SetMaskCount();
    PowerImpl(dstTensor, srcTensor0, scalarValue, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}





template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }



      ;
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value || std::is_same<T, int32_t>::value),
        "Power only support half/float/int32_t data type on current device!");

    SetMaskCount();
    PowerImpl(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, stackTensor, calCount);
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const T& src0Scalar,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, stackTensor, calCount);
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, stackTensor, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, calCount);
}
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, src0Tensor.GetSize());
}
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor, const T& src1Scalar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, sharedTmpBuffer, calCount);
}
# 134 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, calCount);
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, sharedTmpBuffer, src0Tensor.GetSize());
}
# 171 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, src0Tensor.GetSize());
}
# 194 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, sharedTmpBuffer, calCount);
}
# 211 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, calCount);
}
# 232 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, sharedTmpBuffer, src1Tensor.GetSize());
}
# 248 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/power.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, src1Tensor.GetSize());
}
#pragma end_pipe
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/log/log_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/log/log_common_impl.h"
namespace AscendC {
template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void Log2Compute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{

    const T Ln2Reciprocal = 1.4426950408889634;
    const UnaryRepeatParams unaryParams;
    Ln<float, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(dstTensor, dstTensor, Ln2Reciprocal, MASK_PLACEHOLDER, 1, unaryParams);
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void Log2Compute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{

    const float Ln2Reciprocal = 1.4426950408889634;
    const UnaryRepeatParams unaryParams;


    Cast<float, T, false>(tmpTensor.ReinterpretCast<float>(), srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpTensor.ReinterpretCast<float>(),
        tmpTensor.ReinterpretCast<float>(),
        MASK_PLACEHOLDER,
        1,
        unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpTensor.ReinterpretCast<float>(),
        tmpTensor.ReinterpretCast<float>(),
        static_cast<float>(Ln2Reciprocal),
        MASK_PLACEHOLDER,
        1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<T, float, false>(dstTensor, tmpTensor.ReinterpretCast<float>(),
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LogImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{




      ;

    const UnaryRepeatParams unaryParams;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
    Ln<T, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2Impl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{



      ;

    SetMaskCount();
    if constexpr (sizeof(T) == sizeof(float)) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
        Log2Compute(dstTensor, srcTensor);
    } else {
        uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        uint32_t loopCount = calCount / splitSize;
        uint32_t calcTail = calCount % splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            Log2Compute(dstTensor[i * splitSize], srcTensor[i * splitSize], sharedTmpBuffer);
        }
        if (calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            Log2Compute(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize], sharedTmpBuffer);
        }
    }
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}


template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log10Impl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{

    const T Ln10Reciprocal = 0.43429448190325176;
    const UnaryRepeatParams unaryParams;



      ;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
    Ln<T, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dstTensor, dstTensor, Ln10Reciprocal, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h" 2



namespace AscendC {

#pragma begin_pipe(V)







template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LogImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    Log2Impl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Log2<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;

    if constexpr (sizeof(T) == sizeof(half)) {
        bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                       ;
    }

    Log2<T, isReuseSource>(dstTensor, srcTensor, stackTensor, calCount);
}
# 123 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log2<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 136 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/log.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log10(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    Log10Impl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Log10(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log10<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void SinCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h" 2




namespace AscendC {
const uint8_t SIN_HALF_CALC_PROCEDURE = 4;
const uint8_t SIN_FLOAT_NOREUSE_CALC_PROCEDURE = 3;
const uint8_t SIN_FLOAT_REUSE_CALC_PROCEDURE = 2;


constexpr float SIN_PI_FOR_X_TODIV = 0.3183098733425140380859375;

constexpr float SIN_PI_V2 = 3.140625;
constexpr float SIN_KPI_FIRS_PI_MULS = 0.0009670257568359375;
constexpr float SIN_KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float SIN_KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;

constexpr float SIN_RES_MULIT_SCA = 2.604926501e-6;
constexpr float SIN_RES_ADDICT_UP = -0.0001980894471;
constexpr float SIN_2ADDS = 0.008333049340;
constexpr float SIN_3ADDS = -0.1666665792;
constexpr float SIN_POINT_FIVE = 0.5;
constexpr float SIN_M4_SCA = 4.0;
constexpr float SIN_K2_SCA = -2.0;

[aicore] __inline__ __attribute__((always_inline)) void SinSignCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(kpi, inputX, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, SIN_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SinCast(dstTensor, dstTensor, RoundMode::CAST_FLOOR);

    Muls<float, false>(dstTensor, dstTensor, SIN_M4_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(roundTensor, roundTensor, SIN_K2_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void SinPolynomialApproximation(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{





    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SinSignCompute(dstTensor, inputX, roundTensor, kpi);


    Muls<float, false>(roundTensor, kpi, SIN_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, roundTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void SinKpi(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Muls<float, false>(kpi, roundTensor, SIN_PI_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, srcTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void SinRound(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
                                const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
# 157 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h"
    const UnaryRepeatParams unaryParams;
    Muls<float, false>(roundTensor, srcTensor, SIN_PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SinCast(roundTensor, roundTensor, RoundMode::CAST_ROUND);
    SinKpi(inputX, srcTensor, roundTensor, kpi);
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void SinCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize, bool isReuseSource)
{
    const BinaryRepeatParams binParams;
    LocalTensor<T> roundTensor = tmpTensor;
    LocalTensor<T> kpi = tmpTensor[splitSize];
    LocalTensor<T> inputX = srcTensor;
    if (!isReuseSource) {
        inputX = tmpTensor[splitSize * 2];
    }
    SinRound(inputX, srcTensor, roundTensor, kpi);
    SinPolynomialApproximation(dstTensor, inputX, roundTensor, kpi);
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void SinCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize, bool isReuseSource)
{
    (void)isReuseSource;
    const BinaryRepeatParams binParams;
    const LocalTensor<float>& tmpBuffer = tmpTensor;
    const LocalTensor<float>& roundTensor = tmpBuffer[splitSize];
    const LocalTensor<float>& kpi = roundTensor[splitSize];
    const LocalTensor<float>& inputX = kpi[splitSize];

    Cast<float, half, false>(tmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    SinRound(inputX, tmpBuffer, roundTensor, kpi);
    SinPolynomialApproximation(tmpBuffer, inputX, roundTensor, kpi);

    Cast<half, float, false>(dstTensor, tmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}


template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
# 221 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sin/sin_common_impl.h"
    const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);


      ;
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {

                                                                                                            ;
        stackSize = tmpBufferSize / SIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        if constexpr (isReuseSource) {
            stackSize = tmpBufferSize / SIN_FLOAT_REUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            stackSize = tmpBufferSize / SIN_FLOAT_NOREUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    }
                                                                                         ;

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);
    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        SinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
        offset = offset + stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        SinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    SinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sin<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SinImpl(dstTensor, srcTensor, calCount);
}
# 116 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sin.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sin<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h" 1
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CosCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h" 2




namespace AscendC {
const uint8_t COS_HALF_CALC_PROCEDURE = 4;
const uint8_t COS_FLOAT_NOREUSE_CALC_PROCEDURE = 3;
const uint8_t COS_FLOAT_REUSE_CALC_PROCEDURE = 2;


constexpr float COS_PI_FOR_X_TODIV = 0.3183098733425140380859375;

constexpr float PI_0 = 3.140625;
constexpr float COS_KPI_FIRS_PI_MULS = 0.0009670257568359375;
constexpr float COS_KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float COS_KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;
constexpr float COS_KPI_FOR_PI_MULS = -1.0290623200529979163359041220560e-13;

constexpr float COS_PI_DOWN = 1.57079637050628662109375;

constexpr float COS_PI_RESDOWN_ADDS_NEG = -0.00000004371139000189375;

constexpr float COS_RES_MULIT_SCA = 2.604926501e-6;
constexpr float COS_RES_ADDICT_UP = -0.0001980894471;
constexpr float COS_2ADDS = 0.008333049340;
constexpr float COS_3ADDS = -0.1666665792;
constexpr float COS_POINT_FIVE = 0.5;
constexpr float COS_M4_SCA = 4.0;
constexpr float COS_K2_SCA = -2.0;

[aicore] __inline__ __attribute__((always_inline)) void KPI(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Muls<float, false>(kpi, roundTensor, PI_0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, srcTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(inputX, inputX, COS_PI_DOWN, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_FOR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(inputX, inputX, COS_PI_RESDOWN_ADDS_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void CosRound(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
# 112 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h"
    const UnaryRepeatParams unaryParams;
    Muls<float, false>(roundTensor, srcTensor, COS_PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    CosCast(roundTensor, roundTensor, RoundMode::CAST_RINT);
    KPI(inputX, srcTensor, roundTensor, kpi);
}

[aicore] __inline__ __attribute__((always_inline)) void SignCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(kpi, inputX, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, COS_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    CosCast(dstTensor, dstTensor, RoundMode::CAST_FLOOR);


    Muls<float, false>(dstTensor, dstTensor, COS_M4_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(roundTensor, roundTensor, COS_K2_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void CosPolynomialApproximation(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{





    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SignCompute(dstTensor, inputX, roundTensor, kpi);


    Muls<float, false>(roundTensor, kpi, COS_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, roundTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mins<float, false>(dstTensor, dstTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(dstTensor, dstTensor, -1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void CosCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpBuffer, const uint32_t calSize, bool isReuseSource)
{
    const LocalTensor<T>& roundTensor = tmpBuffer;
    const LocalTensor<T>& kpi = roundTensor[calSize];
    LocalTensor<T> inputX = srcTensor;
    if (!isReuseSource) {
        inputX = roundTensor[calSize * 2];
    }

    CosRound(inputX, srcTensor, roundTensor, kpi);
    CosPolynomialApproximation(dstTensor, inputX, roundTensor, kpi);
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void CosCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpBuffer, const uint32_t calSize, bool isReuseSource)
{
    (void)isReuseSource;
    const LocalTensor<float>& tempTensorConv = tmpBuffer;
    const LocalTensor<float>& roundTensor = tempTensorConv[calSize];
    const LocalTensor<float>& kpi = roundTensor[calSize];
    const LocalTensor<float>& inputX = kpi[calSize];

    Cast<float, half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    CosRound(inputX, tempTensorConv, roundTensor, kpi);
    CosPolynomialApproximation(tempTensorConv, inputX, roundTensor, kpi);

    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 252 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cos/cos_common_impl.h"
    const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);


      ;

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {

                                                                                                            ;
        calSize = tmpBufferSize / COS_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        if constexpr (isReuseSource) {
            calSize = tmpBufferSize / COS_FLOAT_REUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            calSize = tmpBufferSize / COS_FLOAT_NOREUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    }


      ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        CosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize, isReuseSource);
        offset = offset + calSize;
    }

    if (tail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        CosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize, isReuseSource);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Cos<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 102 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Cos<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cos.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h" 1
# 13 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/../math_constant_util.h" 1
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/../math_constant_util.h"
namespace AscendC {

constexpr float NUM_ONE = 1.0;
constexpr float NEG_ONE = -1.0;
constexpr float HALF_PI = 1.5707963267948966192313216916398;
constexpr float BOUNDARY = 0.70710678118654752440084436210485;

}
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h" 2

namespace AscendC {
constexpr uint8_t ASIN_HALF_CALC_PROCEDURE = 6;
constexpr uint8_t ASIN_FLOAT_CALC_PROCEDURE = 4;
constexpr uint32_t ASIN_TAYLOR_EXPAND_COUNT = 7;

constexpr float kCOEF[] = {
    1.0,
    0.16666666666666666666666666666667,
    0.075,
    0.04464285714285714285714285714286,
    0.03038194444444444444444444444444,
    0.02237215909090909090909090909091,
    0.01735276442307692307692307692308,
    0.01396484375,
};





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetSign(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float FP16_MAX = 32768;
    constexpr float FP16_MIN = 3.0517578125e-05;
    constexpr float FP32_MAX = 4611686018427387904;
    constexpr float FP32_MIN = 2.168404344971009e-19;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? FP32_MAX : FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? FP32_MIN : FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinTaylorCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& localTemp)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Mul<T, false>(dst, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    Mul<T, false>(localTemp, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(kCOEF[ASIN_TAYLOR_EXPAND_COUNT]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = ASIN_TAYLOR_EXPAND_COUNT - 1; i > 0; i--) {

        Adds<T, false>(dst, dst, static_cast<T>(kCOEF[i]), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<T, false>(dst, dst, localTemp, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<T, false>(dst, dst, static_cast<T>(kCOEF[0]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinTaylorComputeBySquareValue(const LocalTensor<T>& dst, const LocalTensor<T>& src)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Muls<T, false>(dst, src, static_cast<T>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(kCOEF[ASIN_TAYLOR_EXPAND_COUNT]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = ASIN_TAYLOR_EXPAND_COUNT - 1; i > 0; i--) {

        Adds<T, false>(dst, dst, static_cast<T>(kCOEF[i]), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<T, false>(dst, dst, static_cast<T>(kCOEF[0]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<T, false>(src, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void AsinFp16Compute(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& stackBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;


    const LocalTensor<float>& tmpFloatBuffer1 = stackBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[calSize];
    const uint32_t tmpHalfBuffer1Offset = calSize * 4;
    const uint32_t tmpHalfBuffer2Offset = calSize * 5;
    const LocalTensor<half>& tmpHalfBuffer1 = stackBuffer[tmpHalfBuffer1Offset];
    const LocalTensor<half>& tmpHalfBuffer2 = stackBuffer[tmpHalfBuffer2Offset];



    Cast<float, half, false>(tmpFloatBuffer2, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorComputeBySquareValue(tmpFloatBuffer1, tmpFloatBuffer2);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Abs<half, false>(tmpHalfBuffer2, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorCompute(dst, tmpHalfBuffer2, tmpHalfBuffer1);
    PipeBarrier<PIPE_V>();
# 162 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
    Mins<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(BOUNDARY), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(-BOUNDARY), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const LocalTensor<int8_t>& tmpS8Buffer = tmpHalfBuffer1.ReinterpretCast<int8_t>();
    Cast<int8_t, half, false>(tmpS8Buffer, tmpHalfBuffer2, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, int8_t, false>(tmpHalfBuffer2, tmpS8Buffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<half, false>(dst, dst, tmpHalfBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer2, tmpHalfBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer2, dst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    GetSign(tmpHalfBuffer1, src, tmpHalfBuffer2);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(tmpFloatBuffer2, tmpHalfBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(dst, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<T>& tmpBuffer2 = tmpBuffer[calSize];
    const LocalTensor<T>& dupBuffer = tmpBuffer[calSize * 2];

    Mul<T, false>(tmpBuffer2, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer2, tmpBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer2, tmpBuffer2, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sqrt<T, false>(dst, tmpBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorCompute(tmpBuffer2, dst, tmpBuffer);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer2, tmpBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpBuffer2, tmpBuffer2, HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(tmpBuffer, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorComputeBySquareValue(dst, tmpBuffer);
    PipeBarrier<PIPE_V>();
# 262 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
    Mins<T, false>(tmpBuffer, tmpBuffer, BOUNDARY, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer, tmpBuffer, -BOUNDARY, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LocalTensor<int32_t> tmpS32Buffer = tmpBuffer.template ReinterpretCast<int32_t>();
    Cast<int32_t, T, false>(tmpS32Buffer, tmpBuffer, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<T, int32_t, false>(tmpBuffer, tmpS32Buffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer, tmpBuffer, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dst, dst, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tmpBuffer, tmpBuffer, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer, tmpBuffer, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tmpBuffer2, tmpBuffer2, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(dst, dst, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();



    GetSign(tmpBuffer2, src, tmpBuffer);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AsinCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{


    AsinFp16Compute(dst, src, tmpBuffer, calSize);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 329 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asin/asin_common_impl.h"
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                                      ;
    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();



    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ASIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ASIN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                          ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AsinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AsinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AsinImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Asin<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AsinImpl(dstTensor, srcTensor, calCount);
}
# 101 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Asin<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/acos_common_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/acos_common_impl.h"
namespace AscendC {


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcosCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    AsinCompute(dst, src, tmpBuffer, calSize);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(dst, dst, static_cast<T>(-HALF_PI), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AcosCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;

    AsinFp16Compute(dst, src, tmpBuffer, calSize);
    PipeBarrier<PIPE_V>();



    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, -HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(dst, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acos/acos_common_impl.h"
    const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                                      ;
    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();



    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ASIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ASIN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                          ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AcosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AcosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcosImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AcosImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AcosImpl(dstTensor, srcTensor, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Acos<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 102 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Acos<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asinh/asinh_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/asinh/asinh_common_impl.h"
namespace AscendC {
constexpr uint32_t ASINH_HALF_CALC_PROC = 3;
constexpr uint32_t ASINH_FLOAT_CALC_PROC = 3;
constexpr float ASINH_ONE = 1;
constexpr float ASINH_FP16_MAX = 32768;
constexpr float ASINH_FP16_MIN = 3.0517578125e-05;
constexpr float ASINH_FP32_MAX = 4611686018427387904;
constexpr float ASINH_FP32_MIN = 2.168404344971009e-19;
constexpr uint32_t ASINH_STRIDE_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AsinhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AsinhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AsinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(float);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / ASINH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;;
    } else {
        splitCount = splitCount / ASINH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;;
    }
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;
    SetMaskCount();
    SetVectorMask<T>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AsinhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AsinhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void AsinhGetSign(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? ASINH_FP32_MAX : ASINH_FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? ASINH_FP32_MIN : ASINH_FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void AsinhCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;





    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer3 = tmpFloatBuffer1[calCount];
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount * 2];


    Abs<T, false>(tmpFloatBuffer1, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<T>(ASINH_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    AsinhGetSign(tmpFloatBuffer1, src, tmpFloatBuffer3);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dst, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void AsinhCompute(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer3 = tmpFloatBuffer1[calCount];
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount * 2];





    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Abs<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<half>(ASINH_ONE), MASK_PLACEHOLDER,
        1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    AsinhGetSign(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer3);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / ASINH_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Asinh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Asinh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor);
}
#pragma end_pipe
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acosh/acosh_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/acosh/acosh_common_impl.h"
namespace AscendC {
constexpr uint32_t ACOSH_HALF_CALC_PROC = 2;
constexpr uint32_t ACOSH_FLOAT_CALC_PROC = 1;
constexpr float ACOSH_NEG_ONE = -1;
constexpr uint32_t ACOSH_STRIDE_DIGITS = 2;

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void AcoshCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;





    Mul<T, false>(tmpFloatBuffer1, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<T>(ACOSH_NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(tmpFloatBuffer1, src, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Div<T, false>(dst, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<T, false>(dst, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void AcoshCompute(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount];






    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / ACOSH_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<half>(ACOSH_NEG_ONE),
        MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE / ACOSH_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(float);

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / ACOSH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ACOSH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));

    SetMaskCount();
    SetVectorMask<T>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AcoshCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AcoshCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T> &dstTensor,
 const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AcoshImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}






template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor);
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/acosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Acosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
#pragma end_pipe
}
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atan/atan_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atan/atan_common_impl.h"
namespace AscendC {
constexpr uint8_t ATAN_HALF_CALC_PROCEDURE = 6;
constexpr uint8_t ATAN_FLOAT_CALC_PROCEDURE = 5;
constexpr float ATAN_FP16_MAX = 32768;
constexpr float ATAN_FP16_MIN = 3.0517578125e-05;
constexpr float ATAN_FP32_MAX = 4611686018427387904;
constexpr float ATAN_FP32_MIN = 2.168404344971009e-19;
constexpr uint8_t TAYLOR_COUNT_FOUR = 4;
constexpr uint8_t TAYLOR_COUNT_SIX = 6;
constexpr float MIN_INPUT_VALUE = -10000;
constexpr float MAX_INPUT_VALUE = 10000;





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? ATAN_FP32_MAX : ATAN_FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? ATAN_FP32_MIN : ATAN_FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void TaylorExpand(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& squareTensor, int32_t expandLevel)
{


    constexpr float factorList[7] = {1, -0.3333333333333333, 0.2, -0.14285714285714285,
        0.1111111111111111, - 0.09090909090909091, 0.07692307692307693};
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(squareTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
    Mul<float, false>(dstTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(dstTensor, dstTensor, factorList[expandLevel], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = expandLevel - 1; i > 0; --i) {

        Adds<float, false>(dstTensor, dstTensor, factorList[i], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<float, false>(dstTensor, dstTensor, squareTensor, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<float, false>(dstTensor, dstTensor, factorList[0], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, dstTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
}

[aicore] __inline__ __attribute__((always_inline)) void AtanTransform(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor, const float transFactor)
{

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;
    const float transFactorNeg = 0 - transFactor;


    Muls<float, false>(dstTensor, srcTensor, transFactor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);

    Adds<float, false>(tmpTensor, srcTensor, transFactorNeg, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, tmpTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}





[aicore] __inline__ __attribute__((always_inline)) void AtanFormulaImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;
    const float piByFour = 0.78539816339744830961566084581988;
    const float piByEight = 0.39269908169872415480783042290994;
    const float tanPiByEight = 0.4142135623730950;
    LocalTensor<float> clipTensor = tmpTensor[splitSize];
    LocalTensor<float> absTensor = clipTensor[splitSize];
    LocalTensor<float> tmpTensor2 = absTensor[splitSize];
    LocalTensor<float> squareTensor = tmpTensor2[splitSize];




    Mins<float, false>(clipTensor, srcTensor, MAX_INPUT_VALUE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(clipTensor, clipTensor, MIN_INPUT_VALUE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<float, false>(absTensor, clipTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    TaylorExpand(dstTensor, absTensor, squareTensor, TAYLOR_COUNT_FOUR);



    AtanTransform(tmpTensor, absTensor, tmpTensor2, tanPiByEight);
    TaylorExpand(tmpTensor2, tmpTensor, squareTensor, TAYLOR_COUNT_FOUR);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpTensor2, tmpTensor2, piByEight, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);




    Adds<float, false>(tmpTensor2, absTensor, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    Adds<float, false>(tmpTensor, absTensor, -static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmpTensor, tmpTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(tmpTensor, tmpTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    TaylorExpand(tmpTensor2, tmpTensor, squareTensor, TAYLOR_COUNT_FOUR);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpTensor2, tmpTensor2, piByFour, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);


    AtanTransform(tmpTensor2, tmpTensor, squareTensor, tanPiByEight);
    TaylorExpand(tmpTensor, tmpTensor2, squareTensor, TAYLOR_COUNT_SIX);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpTensor, tmpTensor, piByEight, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpTensor, tmpTensor, piByFour, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sign(tmpTensor, clipTensor, tmpTensor2);


    Mul<float, false>(dstTensor, dstTensor, tmpTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AtanCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    AtanFormulaImpl(dstTensor, srcTensor, tmpTensor, splitSize);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void AtanCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const LocalTensor<float>& tempTensorConv = tmpTensor[splitSize * 5];
    Cast<float, half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    AtanFormulaImpl(tempTensorConv, tempTensorConv, tmpTensor, splitSize);
    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atan/atan_common_impl.h"
    const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);

                                                                                                                      ;

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ATAN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ATAN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }


                                                                                                          ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AtanCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AtanCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AtanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Atan<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AtanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 86 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Atan<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 103 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AtanImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CoshCast(const LocalTensor<half>& dst, const LocalTensor<float>& src)
{
    Cast<half, float, false>(dst, src, RoundMode::CAST_ROUND, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h" 2




namespace AscendC {
constexpr float SCALAR_LN2 = -0.69314718055994530941723212145818;
constexpr float SCALAR_BROAD_CAST = 0.25;
const uint8_t COSH_HALF_CALC_PROCEDURE = 6;
const uint8_t COSH_FLOAT_CALC_PROCEDURE = 2;



template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CoshCompute(
    const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<T> tmpBuffer2 = tmpBuffer[calSize];

    Adds<T, false>(tmpBuffer2, src, static_cast<T>(SCALAR_LN2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<T, false>(tmpBuffer, tmpBuffer2, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Duplicate<T, false>(dst, static_cast<T>(SCALAR_BROAD_CAST), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();
    Div<T, false>(tmpBuffer2, dst, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Add<T, false>(dst, tmpBuffer, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void CoshCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<float>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[calSize];
    const LocalTensor<float>& tmpFloatBuffer3 = tmpFloatBuffer2[calSize];

    Cast<float, half, false>(tmpFloatBuffer3, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer3, SCALAR_LN2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(tmpFloatBuffer3, SCALAR_BROAD_CAST, MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmpFloatBuffer2, tmpFloatBuffer3, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer3, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    CoshCast(dst, tmpFloatBuffer3);
}


template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 112 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cosh/cosh_common_impl.h"
    const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                                      ;
    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / COSH_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / COSH_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                          ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        CoshCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        CoshCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CoshImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Cosh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CoshImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Cosh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    CoshImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erf/erf_common_impl.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erf/erf_common_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) constexpr RoundMode GetErfCastType()
{

    return RoundMode::CAST_ROUND;



}


[aicore] __inline__ __attribute__((always_inline)) void ErfClip(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer)
{
    constexpr float ERF_BOUNDARY_MAX = 3.92;
    UnaryRepeatParams unaryParams;

    Mins<float, false>(tmpBuffer, src, static_cast<float>(ERF_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(dst, tmpBuffer, static_cast<float>(-ERF_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void ErfComputeP(const LocalTensor<float>& tmpBuffer, const LocalTensor<float>& src,
    const uint32_t calSize)
{
    constexpr float SCALAR_P0 = 0.29639384698e5;
    constexpr float SCALAR_P1 = 0.50637915060e4;
    constexpr float SCALAR_P2 = 0.13938061484e4;
    constexpr float SCALAR_P3 = 0.10162808918e3;
    constexpr float SCALAR_P4 = 0.75517016694e1;
    constexpr float SCALAR_P5 = 0.053443748819;

    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer;
    LocalTensor<float> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<float> tmpBuffer3 = tmpBuffer2[calSize];

    Mul<float, false>(tmpBuffer1, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpBuffer2, tmpBuffer1, static_cast<float>(SCALAR_P5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, src, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfComputeQ(const LocalTensor<float>& tmpBuffer, const LocalTensor<float>& src,
    const uint32_t calSize)
{
    constexpr float SCALAR_Q0 = 0.26267224157e5;
    constexpr float SCALAR_Q1 = 0.13243365831e5;
    constexpr float SCALAR_Q2 = 0.30231248150e4;
    constexpr float SCALAR_Q3 = 0.39856963806e3;
    constexpr float SCALAR_Q4 = 0.31212858877e2;

    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer;
    LocalTensor<float> tmpBuffer3 = tmpBuffer1[calSize * 2];

    Adds<float, false>(tmpBuffer3, tmpBuffer1, static_cast<float>(SCALAR_Q4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& tmpBuffer,
    const uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<T> tmpBuffer1 = tmpBuffer;
    LocalTensor<T> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<T> tmpBuffer3 = tmpBuffer2[calSize];


    ErfClip(dst, src, tmpBuffer1);

    ErfComputeP(tmpBuffer1, dst, calSize);
    ErfComputeQ(tmpBuffer1, dst, calSize);

    Div<T, false>(dst, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ErfCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, const uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<float> tmpBuffer3 = tmpBuffer2[calSize];
    LocalTensor<float> tmpBuffer4 = tmpBuffer3[calSize];


    Cast<float, half, false>(tmpBuffer4, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    ErfClip(tmpBuffer4, tmpBuffer4, tmpBuffer1);

    ErfComputeP(tmpBuffer1, tmpBuffer4, calSize);
    ErfComputeQ(tmpBuffer1, tmpBuffer4, calSize);

    Div<float, false>(tmpBuffer1, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    constexpr RoundMode castType = GetErfCastType();

    Cast<half, float, false>(dst, tmpBuffer1, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 212 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erf/erf_common_impl.h"
    constexpr uint8_t ERF_HALF_CALC_PROCEDURE = 8;
    constexpr uint8_t ERF_FLOAT_CALC_PROCEDURE = 3;

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                                      ;

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ERF_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ERF_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                          ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        ErfCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        ErfCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ErfImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    ErfImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    Erf<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{
    ErfImpl(dstTensor, srcTensor, calCount);
}
# 99 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    Erf<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h" 1
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erfc/erfc_common_impl.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erfc/erfc_common_impl.h"
namespace AscendC {
constexpr float ERFC_BOUNDARY_MAX = 10;
constexpr uint8_t TMPBUF_IDX_3 = 2;
constexpr uint8_t TMPBUF_IDX_5 = 4;
constexpr uint8_t TMPBUF_IDX_6 = 5;

[aicore] __inline__ __attribute__((always_inline)) constexpr RoundMode GetErfcCastType()
{

    return RoundMode::CAST_ROUND;



}


[aicore] __inline__ __attribute__((always_inline)) void ErfcPreCompute(const LocalTensor<float>& dstBuf1, const LocalTensor<float>& srcBuf1,
    const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Abs<float, false>(tmpCompBuf1, srcBuf1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    constexpr float SCALAR_ERFC_FP32_MIN = 2.168404344971009e-19;
    Adds<float, false>(tmpCompBuf1, tmpCompBuf1, static_cast<float>(SCALAR_ERFC_FP32_MIN), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstBuf1, srcBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void ErfcComputeR(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf3 = tmpCompBuf1[TMPBUF_IDX_3 * calSize];
    LocalTensor<float> tmpCompBuf4 = tmpCompBuf3[calSize];


    constexpr float R0 = 0.1735313680e-7;
    constexpr float R1 = -0.9856738394e-6;
    constexpr float R2 = 0.2517003236e-4;
    constexpr float R3 = -0.3848015171e-3;
    constexpr float R4 = 0.5681528564e0;
    constexpr float R5 = 0.5245623129e1;
    constexpr float R6 = 0.2107740710e2;
    constexpr float R7 = 0.4212761755e2;
    constexpr float R8 = 0.4380524149e2;


    Muls<float, false>(tmpCompBuf3, tmpCompBuf1, static_cast<float>(R0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R6), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R7), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R8), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void ErfcComputeS(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf3 = tmpCompBuf1[TMPBUF_IDX_3 * calSize];
    LocalTensor<float> tmpCompBuf5 = tmpCompBuf1[TMPBUF_IDX_5 * calSize];


    constexpr float S1 = 0.9349684299e1;
    constexpr float S2 = 0.3756930664e2;
    constexpr float S3 = 0.8058268949e2;
    constexpr float S4 = 0.9155653738e2;
    constexpr float S5 = 0.4380524152e2;


    Adds<float, false>(tmpCompBuf3, tmpCompBuf1, static_cast<float>(S1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfcPostCompute(const LocalTensor<float>& dstBuf1, const LocalTensor<float>& srcBuf1,
    const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf2 = tmpCompBuf1[calSize];
    LocalTensor<float> tmpCompBuf3 = tmpCompBuf2[calSize];

    Muls<float, false>(tmpCompBuf2, srcBuf1, static_cast<float>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf2, tmpCompBuf2, static_cast<float>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf3, srcBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstBuf1, tmpCompBuf3, tmpCompBuf2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfcPublicSteps(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf2 = tmpCompBuf1[calSize];
    LocalTensor<float> tmpCompBuf3 = tmpCompBuf2[calSize];
    LocalTensor<float> tmpCompBuf4 = tmpCompBuf3[calSize];
    LocalTensor<float> tmpCompBuf5 = tmpCompBuf4[calSize];

    Mins<float, false>(tmpCompBuf2, tmpCompBuf1, static_cast<float>(ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
    ErfcComputeR(tmpCompBuf1, calSize);
    ErfcComputeS(tmpCompBuf1, calSize);


    Div<float, false>(tmpCompBuf3, tmpCompBuf4, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf1, tmpCompBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpCompBuf1, tmpCompBuf1, static_cast<float>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpCompBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void ErfcClip(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer)
{
    UnaryRepeatParams unaryParams;

    Mins<float, false>(dst, src, static_cast<float>(ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(tmpBuffer, dst, static_cast<float>(-ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ErfcCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{

    LocalTensor<T> tmpCompBuf1 = tmpBuffer;


    ErfcClip(dst, src, dst);
    ErfcPreCompute(dst, dst, tmpCompBuf1, calSize);


    ErfcPublicSteps(tmpCompBuf1, calSize);

    ErfcPostCompute(dst, dst, tmpCompBuf1, calSize);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void ErfcCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{

    LocalTensor<float> tmpCompBuf1 = tmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpCompBuf6 = tmpCompBuf1[TMPBUF_IDX_6 * calSize];


    Cast<float, half, false>(tmpCompBuf6, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    ErfcClip(tmpCompBuf1, tmpCompBuf6, tmpCompBuf6);

    ErfcPreCompute(tmpCompBuf6, tmpCompBuf6, tmpCompBuf1, calSize);

    ErfcPublicSteps(tmpCompBuf1, calSize);

    ErfcPostCompute(tmpCompBuf1, tmpCompBuf6, tmpCompBuf1, calSize);

    constexpr RoundMode castType = GetErfcCastType();

    Cast<half, float, false>(dst, tmpCompBuf1, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ErfcImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 302 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/erfc/erfc_common_impl.h"
    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                                      ;

    constexpr uint8_t ERFC_HALF_CALC_PROCEDURE = 12;
    constexpr uint8_t ERFC_FLOAT_CALC_PROCEDURE = 5;
    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ERFC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ERFC_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                          ;

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;

    for (uint32_t i = 0; i < round; i++) {
        ErfcCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        ErfcCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ErfcImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ErfcImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    ErfcImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Erfc<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    ErfcImpl(dstTensor, srcTensor, calCount);
}
# 112 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Erfc<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/clamp/clamp_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/clamp/clamp_common_impl.h"
namespace AscendC {
constexpr uint32_t CLAMP_FLOAT_MASK = 64;
constexpr uint32_t CLAMP_HALF_MASK = 128;
constexpr uint32_t CLAMP_BYTE_PER_REPEAT = 512;

struct ClampParams {
    [aicore] ClampParams(){};
    uint32_t vcmpvsRepeat = 0;
    uint64_t ClampMask = 0;
    uint64_t selectTailElement = 0;
    uint32_t selectTailOffset = 0;
    uint32_t clampSplitCount = 0;
    uint32_t selectTailRepeatLoop = 0;
    uint32_t selectTailRepeatTail = 0;
    uint32_t selectTailPreRepeatOffset = 0;
    uint32_t selectTailMainRepeatOffset = 0;
    uint32_t selectTailTailRepeatOffset = 0;
    uint32_t loopCount = 0;
    uint32_t calcTail = 0;
    uint32_t vcmpvsRepeatLoop = 0;
    uint32_t vcmpvsRepeatTail = 0;
    uint32_t vcmpvsPreRepeatOffset = 0;
    uint32_t vcmpvsMainRepeatOffset = 0;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ClampComputeCount(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t repeat, const uint64_t mask,
    CLAMPMODE selMode, const BinaryRepeatParams& repeatParams)
{
    if (selMode == CLAMPMODE::CLAMP_MAX) {
        CompareScalar(sharedTmpBuffer, srcTensor, static_cast<T>(scalar), CMPMODE::LT, mask, (uint8_t)repeat,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else if (selMode == CLAMPMODE::CLAMP_MIN) {
        CompareScalar(sharedTmpBuffer, srcTensor, static_cast<T>(scalar), CMPMODE::GT, mask, (uint8_t)repeat,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else {
                                                                                        ;
    }
    PipeBarrier<PIPE_V>();

    Select(dstTensor, sharedTmpBuffer, srcTensor, static_cast<T>(scalar), SELMODE::VSEL_TENSOR_SCALAR_MODE, mask,
        (uint8_t)repeat, repeatParams);
    PipeBarrier<PIPE_V>();
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GetMainParams(const uint32_t calCount, ClampParams& params)
{
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        params.clampSplitCount = params.clampSplitCount / CLAMP_HALF_MASK * CLAMP_HALF_MASK;
        params.vcmpvsRepeat = params.clampSplitCount / CLAMP_HALF_MASK;
        params.ClampMask = CLAMP_HALF_MASK;
    } else {
        params.clampSplitCount = params.clampSplitCount / CLAMP_FLOAT_MASK * CLAMP_FLOAT_MASK;
        params.vcmpvsRepeat = params.clampSplitCount / CLAMP_FLOAT_MASK;
        params.ClampMask = CLAMP_FLOAT_MASK;
    }
                                                                                                            ;
    params.loopCount = calCount / params.clampSplitCount;
    params.calcTail = calCount % params.clampSplitCount;
    params.vcmpvsRepeatLoop = params.vcmpvsRepeat / MAX_REPEAT_TIMES;
    params.vcmpvsRepeatTail = params.vcmpvsRepeat % MAX_REPEAT_TIMES;
    params.vcmpvsPreRepeatOffset = MAX_REPEAT_TIMES * params.ClampMask;
    params.vcmpvsMainRepeatOffset = params.vcmpvsRepeatLoop * MAX_REPEAT_TIMES * params.ClampMask;
}

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void GetTailParams(const uint32_t calcTail, ClampParams& params)
{
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        params.vcmpvsRepeat = calcTail / CLAMP_HALF_MASK;
        params.ClampMask = (calcTail < CLAMP_HALF_MASK) ? calcTail : CLAMP_HALF_MASK;
        params.selectTailElement = calcTail % CLAMP_HALF_MASK;
        params.selectTailOffset = params.vcmpvsRepeat * CLAMP_HALF_MASK;
    } else {
        params.vcmpvsRepeat = calcTail / CLAMP_FLOAT_MASK;
        params.ClampMask = (calcTail < CLAMP_FLOAT_MASK) ? calcTail : CLAMP_FLOAT_MASK;
        params.selectTailElement = calcTail % CLAMP_FLOAT_MASK;
        params.selectTailOffset = params.vcmpvsRepeat * CLAMP_FLOAT_MASK;
    }
    params.selectTailRepeatLoop = params.vcmpvsRepeat / MAX_REPEAT_TIMES;
    params.selectTailRepeatTail = params.vcmpvsRepeat % MAX_REPEAT_TIMES;
    params.selectTailPreRepeatOffset = MAX_REPEAT_TIMES * params.ClampMask;
    params.selectTailMainRepeatOffset = params.selectTailRepeatLoop * MAX_REPEAT_TIMES * params.ClampMask;
    params.selectTailTailRepeatOffset = params.selectTailRepeatLoop * MAX_REPEAT_TIMES * params.ClampMask +
        params.selectTailRepeatTail * params.ClampMask;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ClampCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount, CLAMPMODE selMode,
    ClampParams& params)
{
    uint32_t sharedTmpBufferSize = sharedTmpBuffer.GetSize();
    params.clampSplitCount = sharedTmpBufferSize * sizeof(uint8_t) / sizeof(uint8_t);

    GetMainParams<T>(calCount, params);
    BinaryRepeatParams vselRepeatParams;

    for (uint32_t i = 0; i < params.loopCount; i++) {
        for (uint32_t j = 0; j < params.vcmpvsRepeatLoop; j++) {
            ClampComputeCount<T>(dstTensor[i * params.clampSplitCount + j * params.vcmpvsPreRepeatOffset],
                srcTensor[i * params.clampSplitCount + j * params.vcmpvsPreRepeatOffset], sharedTmpBuffer, scalar,
                MAX_REPEAT_TIMES, params.ClampMask, selMode, vselRepeatParams);
        }
        if (params.vcmpvsRepeatTail) {
            ClampComputeCount<T>(dstTensor[i * params.clampSplitCount + params.vcmpvsMainRepeatOffset],
                srcTensor[i * params.clampSplitCount + params.vcmpvsMainRepeatOffset], sharedTmpBuffer, scalar,
                params.vcmpvsRepeatTail, params.ClampMask, selMode, vselRepeatParams);
        }
    }





    uint32_t mainCount = params.loopCount * params.clampSplitCount;
    if (params.calcTail > 0) {
        GetTailParams<T>(params.calcTail, params);
        for (uint32_t j = 0; j < params.selectTailRepeatLoop; j++) {
            ClampComputeCount<T>(dstTensor[mainCount + j * params.selectTailPreRepeatOffset],
                srcTensor[mainCount + j * params.selectTailPreRepeatOffset], sharedTmpBuffer, scalar, MAX_REPEAT_TIMES,
                params.ClampMask, selMode, vselRepeatParams);
        }
        if (params.selectTailRepeatTail) {
            ClampComputeCount<T>(dstTensor[mainCount + params.selectTailMainRepeatOffset],
                srcTensor[mainCount + params.selectTailMainRepeatOffset], sharedTmpBuffer, scalar,
                params.selectTailRepeatTail, params.ClampMask, selMode, vselRepeatParams);
        }
        if (params.selectTailElement) {
            ClampComputeCount<T>(dstTensor[mainCount + params.selectTailTailRepeatOffset],
                srcTensor[mainCount + params.selectTailTailRepeatOffset], sharedTmpBuffer, scalar, 1,
                params.selectTailElement, selMode, vselRepeatParams);
        }
    }
}



#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMaxImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{
    ClampParams params;
    ClampCompute<T>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount, CLAMPMODE::CLAMP_MAX, params);
}





template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{
    ClampParams params;
    ClampCompute<T>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount, CLAMPMODE::CLAMP_MIN, params);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h" 2



namespace AscendC {
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h"
#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ClampMaxImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const T scalar,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ClampMaxImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/clamp.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ClampMinImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ClampMin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const T scalar,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ClampMinImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}
#pragma end_pipe
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_v220_impl.h" 2

namespace AscendC {
namespace RmsNormAPI {


[aicore] __inline__ __attribute__((always_inline)) void RmsNormBasicBlockBrc(const LocalTensor<float>& dst, const LocalTensor<float>& inputAddr,
    const LocalTensor<float>& reduceAddr, const uint32_t hLength, const uint32_t bsLength)
{
    constexpr uint32_t BASIC_BLK_HLENGTH = 64;
    constexpr uint32_t BASIC_BLK_BSLENGTH = 8;
    constexpr uint32_t FLOAT_PER_BLOCK = 8;
    const uint16_t dstBlkStride = hLength / FLOAT_PER_BLOCK;
    const uint16_t dstRepStride = dstBlkStride * BASIC_BLK_BSLENGTH;
    const uint32_t repTime = bsLength / BASIC_BLK_BSLENGTH;

    SetMaskNorm();
    ResetMask();
    BrcbRepeatParams brcParams(dstBlkStride, dstRepStride);
    Brcb(dst, reduceAddr, repTime, brcParams);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
    const uint8_t repStride = hLength / FLOAT_PER_BLOCK;
    const int32_t loop = hLength / BASIC_BLK_HLENGTH;
    UnaryRepeatParams unaryParams(1, 0, repStride, repStride);
    for (int32_t i = 0; i < loop; i++) {
        const uint32_t offset = i * BASIC_BLK_HLENGTH;
        Adds<float, false>(dst[offset], dst, 0, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/rmsnorm/rmsnorm_common_impl.h" 2




namespace AscendC {
namespace RmsNormAPI {
constexpr uint32_t BASIC_BLK_HLENGTH = 64;
constexpr uint32_t BASIC_BLK_BSLENGTH = 8;
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr float RSQRT_EXPONENT = -0.5;

struct RmsNormParams {
    [aicore] RmsNormParams() {};
    uint32_t curBsLength = 0;
    uint32_t curBshLength = 0;
    LocalTensor<float> tmpAddr;
    LocalTensor<float> reducedAddr;
    LocalTensor<float> srcFp32Addr;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetRmsNormInfo(
    const LocalTensor<float>& tmpLocal, const RmsNormTiling& tiling, RmsNormParams& params)
{
    params.reducedAddr = tmpLocal;
    params.tmpAddr = tmpLocal[tiling.mainBsLengthAlign];
    if constexpr (sizeof(T) == sizeof(half)) {
        params.srcFp32Addr = tmpLocal[tiling.mainBshLength + tiling.mainBsLengthAlign];
    }
    params.curBsLength = tiling.mainBsLength;
    params.curBshLength = tiling.mainBshLength;
}


[aicore] __inline__ __attribute__((always_inline)) void RmsNormGenericReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    for (uint32_t i = 0; i < bsLength; i++) {
        uint32_t totalNum = originalHLength;
        LocalTensor<float> srcTmp = src[i * hLength];
        LocalTensor<float> dstTmp = srcTmp;

        while (totalNum > 1) {
            if (totalNum <= ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            }
            SetVectorMask<float>(0, totalNum);
            RepeatReduceSum<float, false>(dstTmp, srcTmp, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();
            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
        }
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    if constexpr (isBasicBlock) {







        SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
        const uint32_t basicBlockNum = hLength / BASIC_BLK_HLENGTH;
        const uint8_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, repStride);
        for (uint32_t i = 1; i < basicBlockNum; i++) {
            const uint32_t offset = i * BASIC_BLK_HLENGTH;
            Add<float, false>(src, src, src[offset], MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }


        RepeatReduceSum<float, false>(dst, src, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, repStride);
        PipeBarrier<PIPE_V>();
    } else {
        RmsNormGenericReduceSum(dst, src, bsLength, hLength, originalHLength);
    }
}



[aicore] __inline__ __attribute__((always_inline)) void RmsNormGeneralFirstAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bshLength, const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<float>(0, hLength);
    UnaryRepeatParams unaryParams;
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    for (uint32_t i = 0; i < bsLength; i++) {
        const uint32_t offset = i * hLength;
        Muls<float, false>(dst[offset], src0[offset], src1.GetValue(i), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, bshLength);
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormFirstAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& inputAddr,
    const LocalTensor<float>& reduceAddr, const uint32_t bshLength, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr (isBasicBlock) {
        if (bsLength > BASIC_BLK_BSLENGTH && bsLength > hLength / BASIC_BLK_HLENGTH) {
            RmsNormBasicBlockBrc(dst, inputAddr, reduceAddr, hLength, bsLength);
            SetVectorMask<float>(0, bshLength);
            BinaryRepeatParams binaryParams;
            Mul<float, false>(dst, dst, inputAddr, MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        } else {
            RmsNormGeneralFirstAxisBrcMul(dst, inputAddr, reduceAddr, bshLength, bsLength, hLength);
        }
    } else {
        RmsNormGeneralFirstAxisBrcMul(dst, inputAddr, reduceAddr, bshLength, bsLength, hLength);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void RmsNormLastAxisBrcMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const uint32_t loop = hLength / BASIC_BLK_HLENGTH;
    if (loop >= bsLength) {

        BinaryRepeatParams binaryParams;
        SetVectorMask<float>(0, hLength);
        for (uint32_t i = 0; i < bsLength; ++i) {
            uint32_t offset = i * hLength;
            Mul<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
        }
    } else {

        SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
        const uint16_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);
        for (uint32_t i = 0; i < loop; ++i) {
            uint32_t offset = i * BASIC_BLK_HLENGTH;
            Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
        }
        if (hLength % BASIC_BLK_HLENGTH != 0) {
            uint32_t offset = loop * BASIC_BLK_HLENGTH;
            uint32_t tail = hLength - offset;
            SetMaskNorm();
            SetVectorMask<float>(0, (1ull << tail) - 1);

            Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, bsLength, binaryParams);
            SetMaskCount();
        }
    }
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormLastAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr (isBasicBlock) {
        RmsNormLastAxisBrcMulImpl(dst, src0, src1, bsLength, hLength);
    } else {
        if (hLength == BASIC_BLK_HLENGTH) {
            BinaryRepeatParams binaryParams;
            binaryParams.src1RepStride = 0;
            SetVectorMask<float>(0, bsLength * hLength);
            Mul<float, false>(dst, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);
        } else if (hLength < BASIC_BLK_HLENGTH) {
            SetMaskNorm();
            SetVectorMask<float>(0, (1ull << hLength) - 1);
            uint32_t repStride = hLength / FLOAT_PER_BLOCK;
            BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);
            Mul<float, false>(dst, src0, src1, MASK_PLACEHOLDER, bsLength, binaryParams);
            SetMaskCount();
        } else {
            RmsNormLastAxisBrcMulImpl(dst, src0, src1, bsLength, hLength);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& gamma, const T epsilon, const RmsNormTiling& tiling, RmsNormParams& params)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<T>(0, params.curBshLength);
    if constexpr (sizeof(T) == sizeof(half)) {
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
        Cast<float, half, false>(params.srcFp32Addr, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    } else {
        params.srcFp32Addr = src;
    }

    BinaryRepeatParams binaryParams;
    Mul<float, false>(params.tmpAddr, params.srcFp32Addr, params.srcFp32Addr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    RmsNormReduceSum<isBasicBlock>(
        params.reducedAddr, params.tmpAddr, params.curBsLength, tiling.hLength, tiling.originalHLength);

    SetVectorMask<T>(0, params.curBsLength);
    Muls<float, false>(
        params.reducedAddr, params.reducedAddr, tiling.reciprocalOfHLength, MASK_PLACEHOLDER, 1, unaryParams);

    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.reducedAddr, params.reducedAddr, epsilon, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.reducedAddr, params.reducedAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.reducedAddr, params.reducedAddr, RSQRT_EXPONENT, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(params.reducedAddr, params.reducedAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    RmsNormFirstAxisBrcMul<isBasicBlock>(params.tmpAddr, params.srcFp32Addr, params.reducedAddr, params.curBshLength,
        params.curBsLength, tiling.hLength);
    PipeBarrier<PIPE_V>();
    if constexpr (sizeof(T) == sizeof(half)) {
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
        SetVectorMask<T>(0, tiling.hLength);
        Cast<float, half, false>(
            params.srcFp32Addr, gamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);

        PipeBarrier<PIPE_V>();
        RmsNormLastAxisBrcMul<isBasicBlock>(
            params.tmpAddr, params.tmpAddr, params.srcFp32Addr, params.curBsLength, tiling.hLength);
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
        unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
        SetVectorMask<T>(0, params.curBshLength);
        Cast<half, float, false>(dst, params.tmpAddr, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    } else {

        RmsNormLastAxisBrcMul<isBasicBlock>(dst, params.tmpAddr, gamma, params.curBsLength, tiling.hLength);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon,
    const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                    ;

    LocalTensor<float> tmpLocal = sharedTmpBuffer.ReinterpretCast<float>();
    RmsNormParams params;
    GetRmsNormInfo<T>(tmpLocal, tiling, params);
    SetMaskCount();
    for (uint32_t i = 0; i < tiling.loopRound; ++i) {
        uint32_t offset = i * tiling.mainBshLength;
        RmsNormCompute<T, isBasicBlock>(
            dstLocal[offset], srcLocal[offset], gammaLocal, epsilon, tiling, params);
    }
    if (tiling.tailBsLength != 0) {
        params.curBshLength = tiling.tailBshLength;
        params.curBsLength = tiling.tailBsLength;
        RmsNormCompute<T, isBasicBlock>(
            dstLocal[tiling.inputTailPos], srcLocal[tiling.inputTailPos], gammaLocal, epsilon, tiling, params);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h"
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon,
    const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                    ;
    RmsNormAPI::RmsNormImpl<T, isBasicBlock>(dstLocal, srcLocal, gammaLocal, sharedTmpBuffer, epsilon, tiling);
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/rmsnorm.h"
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void RmsNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const T epsilon, const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> stackBufer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(stackBufer);
                                                                                                   ;
    RmsNorm<T, isBasicBlock>(dstLocal, srcLocal, gammaLocal, stackBufer, epsilon, tiling);
}
#pragma end_pipe
}
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 2






# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_v220_impl.h"
namespace AscendC {
constexpr uint32_t FLOAT_BLOCK_NUM_V220 = 8;
constexpr uint32_t BRC_ADDS_LOOP = 7;
constexpr uint32_t BASIC_BLOCK_LEN_V220 = 64;

[aicore] __inline__ __attribute__((always_inline)) void BrcFirstBlockByAdds(const LocalTensor<float>& dst, const uint32_t repeat,
    const uint32_t firstOffset, UnaryRepeatParams& addsUnaryParams, const BatchNormParams<float>& params)
{
    for (uint32_t m = 0; m < repeat; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Adds<float, false>(dst[firstOffset + m * firstOffset], dst, 0, MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                addsUnaryParams);
        }
        if (params.oriBTail) {
            Adds<float, false>(dst[firstOffset + m * firstOffset], dst, 0, MASK_PLACEHOLDER, (uint8_t)params.oriBTail,
                addsUnaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
    ResetMask();
    addsUnaryParams.srcBlkStride = DEFAULT_BLK_STRIDE;
    for (uint32_t m = 0; m < (params.basicLoop - 1); m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Adds<float, false>(dst[BASIC_BLOCK_LEN_V220 + m * BASIC_BLOCK_LEN_V220 + i * params.oriBTmpLoopOffset],
                dst[i * params.oriBTmpLoopOffset], 0, MASK_PLACEHOLDER, MAX_REPEAT_TIMES, addsUnaryParams);
        }
        if (params.oriBTail) {
            Adds<float, false>(dst[BASIC_BLOCK_LEN_V220 + m * BASIC_BLOCK_LEN_V220 + params.oriBTmpTailOffset],
                dst[params.oriBTmpTailOffset], 0, MASK_PLACEHOLDER, (uint8_t)params.oriBTail, addsUnaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BrcFirstDimByBrcb(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    BrcbRepeatParams repeatParams;
    repeatParams.dstBlkStride = (uint16_t)tiling.shCurLengthBlockNum;
    repeatParams.dstRepStride = tiling.shCurLength * FLOAT_BLOCK_NUM_V220 / FLOAT_BLOCK_NUM_V220;


    Brcb(dst, src, (uint8_t)params.brcRepeatTimes, repeatParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::NORMAL>(FLOAT_BLOCK_NUM_V220);
    UnaryRepeatParams addsUnaryParams;
    addsUnaryParams.dstBlkStride = DEFAULT_BLK_STRIDE;
    addsUnaryParams.srcBlkStride = 0;
    addsUnaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    addsUnaryParams.srcRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    BrcFirstBlockByAdds(dst, BRC_ADDS_LOOP, FLOAT_BLOCK_NUM_V220, addsUnaryParams, params);
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/batchnorm/batchnorm_common_impl.h" 2


namespace AscendC {
constexpr uint32_t FLOAT_BLOCK_NUMBER = 8;
constexpr uint32_t BASIC_BLOCK_LEN = 64;


[aicore] __inline__ __attribute__((always_inline)) void StackBufferChecker(const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling)
{



      ;
}

template <bool needCast = false> [aicore] __inline__ __attribute__((always_inline)) void GetSrcOffset(uint32_t& srcOffset, const BatchNormTiling& tiling)
{
    if constexpr (!needCast) {
        srcOffset = tiling.meanVarSize;
    } else {
        srcOffset = tiling.shCurLength;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetUpdataParams(const BatchNormTiling& tiling, BatchNormParams<float>& params)
{
    params.srcRepeatStride = params.srcOffset / FLOAT_BLOCK_NUMBER;
    params.brcRepeatTimes = tiling.originalBLength / FLOAT_BLOCK_NUMBER;
    params.oriBloop = tiling.originalBLength / MAX_REPEAT_TIMES;
    params.oriBTail = tiling.originalBLength % MAX_REPEAT_TIMES;
    params.oriBTmpLoopOffset = tiling.shCurLength * MAX_REPEAT_TIMES;
    params.oriBTmpTailOffset = params.oriBloop * params.oriBTmpLoopOffset;
    params.oriBOutLoopOffset = tiling.meanVarSize * MAX_REPEAT_TIMES;
    params.oriBOutTailOffset = params.oriBloop * params.oriBOutLoopOffset;
    params.reduceAddLoop = (tiling.originalBLength - 1) / MAX_REPEAT_TIMES;
    params.reduceAddTail = (tiling.originalBLength - 1) % MAX_REPEAT_TIMES;
    params.reduceAddTailOffset = BASIC_BLOCK_LEN + params.reduceAddLoop * params.oriBTmpLoopOffset;
    params.basicLoop = tiling.shCurLength / BASIC_BLOCK_LEN;
}

template <typename T, bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetMainTailOffset(uint64_t& inputMainOffset, uint64_t& inputTailOffset,
    const BatchNormParams<float>& params)
{
    inputMainOffset = params.oriBTmpLoopOffset;
    inputTailOffset = params.oriBTmpTailOffset;
    if constexpr (!needCast) {
        inputMainOffset = params.oriBOutLoopOffset;
        inputTailOffset = params.oriBOutTailOffset;
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void CastGammBeta(const LocalTensor<float>& dst, const LocalTensor<half>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams castUnaryParams;
    castUnaryParams.srcRepStride = (uint8_t)tiling.castHalfRepStride;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.originalBLength);
    Cast<float, half, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, castUnaryParams);

    if constexpr (!isBasicBlock) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
    } else {
        PipeBarrier<PIPE_V>();
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void CastOutput(const LocalTensor<half>& output, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfOutRepStride;
        unaryParams.srcRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Cast<half, float, false>(output[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN],
                    src[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Cast<half, float, false>(output[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN],
                    src[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Cast<half, float, false>(output[i * tiling.meanVarSize], src[i * tiling.shCurLength], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, unaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void CastInput(const LocalTensor<float>& dst, const LocalTensor<half>& input,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        unaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        unaryParams.srcRepStride = (uint8_t)tiling.castHalfOutRepStride;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Cast<float, half, false>(dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    input[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Cast<float, half, false>(dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                    input[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        unaryParams.srcRepStride = (uint8_t)tiling.castHalfRepStride;
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Cast<float, half, false>(dst[i * tiling.shCurLength], input[i * tiling.meanVarSize], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, unaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceAddResult(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    SetMaskNorm();
    ResetMask();
    DataCopyParams datacopyParams;
    datacopyParams.blockCount = 1;
    datacopyParams.blockLen = (uint16_t)tiling.shCurLengthBlockNum;
    DataCopy<float>(dst, src, datacopyParams);
    PipeBarrier<PIPE_V>();
    BinaryRepeatParams binaryParams;
    if constexpr (isBasicBlock) {
        binaryParams.dstRepStride = 0;
        binaryParams.src0RepStride = (uint8_t)tiling.shCurLengthBlockNum;
        binaryParams.src1RepStride = 0;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.reduceAddLoop; i++) {
                Add<float, false>(dst[m * BASIC_BLOCK_LEN],
                    src[tiling.shCurLength + i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    dst[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES, binaryParams);
            }
            if (params.reduceAddTail) {
                Add<float, false>(dst[m * BASIC_BLOCK_LEN], src[tiling.shCurLength + m * BASIC_BLOCK_LEN],
                    dst[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, (uint8_t)params.reduceAddTail, binaryParams);
            }
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        for (uint32_t i = 1; i < tiling.originalBLength; i++) {
            Add<float, false>(dst, dst, src[i * tiling.shCurLength], MASK_PLACEHOLDER, (uint8_t)1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void BrcFirstDimByDup(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t shLength, const uint32_t bLength)
{
    for (uint32_t i = 0; i < bLength; i++) {
        Duplicate<float, false>(dst[i * shLength], float(src.GetValue(i)), MASK_PLACEHOLDER, (uint8_t)1, (uint16_t)1,
            (uint8_t)DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BrcFirstDim(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    SetMaskNorm();
    ResetMask();

    BrcFirstDimByBrcb<isBasicBlock>(dst, src, tiling, params);



}

template <bool isBasicBlock = false, bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputMean(const LocalTensor<float>& outputMean, const LocalTensor<float>& inputX,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        SetMaskNorm();
        ResetMask();
        unaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        unaryParams.srcRepStride = params.srcRepeatStride;
        uint64_t inputMainOffset = 0;
        uint64_t inputTailOffset = 0;
        GetMainTailOffset<float, needCast>(inputMainOffset, inputTailOffset, params);
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Muls<float, false>(params.tempTensorC[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    inputX[i * inputMainOffset + m * BASIC_BLOCK_LEN], params.firstDimValueBack, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Muls<float, false>(params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                    inputX[inputTailOffset + m * BASIC_BLOCK_LEN], params.firstDimValueBack, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
        GetReduceAddResult<isBasicBlock>(outputMean, params.tempTensorC, tiling, params);
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Muls<float, false>(params.tempTensorC[i * tiling.shCurLength], inputX[i * params.srcOffset],
                params.firstDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
        }
        PipeBarrier<PIPE_V>();

        GetReduceAddResult<isBasicBlock>(outputMean, params.tempTensorC, tiling, params);
    }
}

template <bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputVarianceBasicBlock(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    BinaryRepeatParams subBinaryParams;
    const BinaryRepeatParams mulBinaryParams;
    const UnaryRepeatParams mulsUnaryParams;
    SetMaskNorm();
    ResetMask();
    subBinaryParams.src0RepStride = params.srcRepeatStride;
    subBinaryParams.src1RepStride = 0;
    subBinaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;

    uint64_t inputMainOffset = 0;
    uint64_t inputTailOffset = 0;
    GetMainTailOffset<float, needCast>(inputMainOffset, inputTailOffset, params);
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Sub<float, false>(params.tempTensorC[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                inputX[i * inputMainOffset + m * BASIC_BLOCK_LEN], outputMean[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                MAX_REPEAT_TIMES, subBinaryParams);
            PipeBarrier<PIPE_V>();
        }
        if (params.oriBTail) {
            Sub<float, false>(params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                inputX[inputTailOffset + m * BASIC_BLOCK_LEN], outputMean[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                (uint8_t)params.oriBTail, subBinaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorB, params.tempTensorC, params.tempTensorC, MASK_PLACEHOLDER, 1, mulBinaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tempTensorA, params.tempTensorB, params.firstDimValueBack, MASK_PLACEHOLDER, 1,
        mulsUnaryParams);
    PipeBarrier<PIPE_V>();
    GetReduceAddResult<true>(outputVariance, params.tempTensorA, tiling, params);
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputVarianceNorm(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    const UnaryRepeatParams mulsUnaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);

    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Sub<float, false>(params.tempTensorC[i * tiling.shCurLength], inputX[i * params.srcOffset], outputMean,
            MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorB, params.tempTensorC, params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(params.tempTensorA, params.tempTensorB, params.firstDimValueBack, MASK_PLACEHOLDER, 1,
        mulsUnaryParams);
    PipeBarrier<PIPE_V>();

    GetReduceAddResult<false>(outputVariance, params.tempTensorA, tiling, params);
}

template <bool isBasicBlock = false, bool needCast = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputVariance(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputVarianceBasicBlock<needCast>(outputVariance, inputX, outputMean, tiling, params);
    } else {
        GetBatchNormOutputVarianceNorm(outputVariance, inputX, outputMean, tiling, params);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPreBasicBlock(const LocalTensor<float>& addSrc,
    const LocalTensor<float>& addDst, const LocalTensor<float>& tmpDst, const LocalTensor<float>& dst,
    const float epsilon, const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float exponent = -0.5;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    Adds<float, false>(addDst, addSrc, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(addDst, tmpDst, exponent, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();
    binaryParams.src0RepStride = 0;
    binaryParams.src1RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    binaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Mul<float, false>(dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], tmpDst[m * BASIC_BLOCK_LEN],
                dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                binaryParams);
        }
        if (params.oriBTail) {
            Mul<float, false>(dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], tmpDst[m * BASIC_BLOCK_LEN],
                dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, (uint8_t)params.oriBTail,
                binaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPreNorm(const LocalTensor<float>& addSrc, const LocalTensor<float>& addDst,
    const LocalTensor<float>& tmpDst, const LocalTensor<float>& dst, const float epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    constexpr float exponent = -0.5;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    Adds<float, false>(addDst, addSrc, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(addDst, tmpDst, exponent, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Mul<float, false>(dst[i * tiling.shCurLength], dst[i * tiling.shCurLength], tmpDst, MASK_PLACEHOLDER, 1,
            binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputPre(const LocalTensor<float>& src, const LocalTensor<float>& dst,
    const float epsilon, const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputPreBasicBlock(src, params.tempTensorA, params.tempTensorB, dst, epsilon, tiling, params);
    } else {
        GetBatchNormOutputPreNorm(src, params.tempTensorA, params.tempTensorB, dst, epsilon, tiling, params);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputBasicBlock(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    BrcFirstDim(params.tempTensorB, gamm, tiling, params);
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorC, params.tempTensorB, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    BrcFirstDim(params.tempTensorB, beta, tiling, params);
    BinaryRepeatParams addBinaryParams;
    addBinaryParams.dstRepStride = tiling.meanVarSize / FLOAT_BLOCK_NUMBER;
    addBinaryParams.src0RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    addBinaryParams.src1RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Add<float, false>(output[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorB[params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorC[params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                addBinaryParams);
        }
        if (params.oriBTail) {
            Add<float, false>(output[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorB[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                (uint8_t)params.oriBTail, addBinaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutputNorm(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    BrcFirstDimByDup(params.tempTensorB, gamm, tiling.shCurLength, tiling.originalBLength);
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    BrcFirstDimByDup(params.tempTensorB, beta, tiling.shCurLength, tiling.originalBLength);
    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Add<float, false>(output[i * tiling.meanVarSize], params.tempTensorB[i * tiling.shCurLength],
            params.tempTensorC[i * tiling.shCurLength], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutput(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputBasicBlock(src, output, gamm, beta, tiling, params);
    } else {
        GetBatchNormOutputNorm(src, output, gamm, beta, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormOutput(const LocalTensor<float>& src, const LocalTensor<half>& output,
    const LocalTensor<half>& gamm, const LocalTensor<half>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    BinaryRepeatParams binaryParams;
    if constexpr (isBasicBlock) {
        CastGammBeta<isBasicBlock>(params.tempTensorA, gamm, tiling, params);
        BrcFirstDim(params.tempTensorB, params.tempTensorA, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Mul<float, false>(params.tempTensorC, params.tempTensorB, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        CastGammBeta<isBasicBlock>(params.tempTensorA, beta, tiling, params);
        BrcFirstDim(params.tempTensorB, params.tempTensorA, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Add<float, false>(params.tempTensorB, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1,
            binaryParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    } else {
        CastGammBeta<isBasicBlock>(params.tempTensorA, gamm, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        BrcFirstDimByDup(params.tempTensorB, params.tempTensorA, tiling.shCurLength, tiling.originalBLength);
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1,
            binaryParams);
        PipeBarrier<PIPE_V>();
        CastGammBeta<isBasicBlock>(params.tempTensorA, beta, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        BrcFirstDimByDup(params.tempTensorB, params.tempTensorA, tiling.shCurLength, tiling.originalBLength);
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Add<float, false>(params.tempTensorB[i * tiling.shCurLength], params.tempTensorB[i * tiling.shCurLength],
                params.tempTensorC[i * tiling.shCurLength], MASK_PLACEHOLDER, 1, binaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetOutputMeanVariance(const LocalTensor<half>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        Cast<half, float, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        Cast<half, float, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormInfo(const LocalTensor<half>& inputX, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



      ;
    StackBufferChecker(stackBuffer, tiling);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetBatchNormInfo(const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



      ;
    StackBufferChecker(stackBuffer, tiling);
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormExeImpl(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& tmpOutputMean,
    const LocalTensor<float>& tmpOutputVariance, const float epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    constexpr bool needCast = false;
    if constexpr (isBasicBlock) {
        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, inputX, tiling, params);
        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, inputX, tmpOutputMean, tiling, params);
        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);
        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
    } else {

        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, inputX, tiling, params);

        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, inputX, tmpOutputMean, tiling, params);

        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);

        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormExeImpl(const LocalTensor<half>& inputX, const LocalTensor<half>& gamm,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LocalTensor<float>& tmpOutputMean,
    const LocalTensor<float>& tmpOutputVariance, const half epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    constexpr bool needCast = true;
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        SetMaskNorm();
        ResetMask();
        CastInput<isBasicBlock>(params.tempTensorA, inputX, tiling, params);

        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, params.tempTensorA, tiling, params);

        GetOutputMeanVariance<isBasicBlock>(outputMean, tmpOutputMean, tiling, params);

        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, params.tempTensorA, tmpOutputMean, tiling,
            params);

        GetOutputMeanVariance<isBasicBlock>(outputVariance, tmpOutputVariance, tiling, params);

        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);

        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);

        CastOutput<isBasicBlock>(output, params.tempTensorB, tiling, params);
    } else {
        CastInput<isBasicBlock>(params.tempTensorA, inputX, tiling, params);
        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, params.tempTensorA, tiling, params);
        GetOutputMeanVariance(outputMean, tmpOutputMean, tiling, params);
        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, params.tempTensorA, tmpOutputMean, tiling,
            params);
        GetOutputMeanVariance(outputVariance, tmpOutputVariance, tiling, params);
        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);
        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
        CastOutput<isBasicBlock>(output, params.tempTensorB, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormCompute(const LocalTensor<half>& inputX, const LocalTensor<half>& gamm,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const half epsilon, BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    constexpr bool needCast = true;
    uint32_t mvOffset = 0;

    GetSrcOffset<needCast>(params.srcOffset, tiling);
    GetUpdataParams(tiling, params);

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        BatchNormExeImpl<isBasicBlock>(inputX[mvOffset], gamm, beta, output[mvOffset], outputMean[mvOffset],
            outputVariance[mvOffset], params.meanTmpTensor[mvOffset], params.varianceTmpTensor[mvOffset], epsilon,
            tiling, params);

        mvOffset += tiling.shCurLength;
    }
    if (tiling.inputTailSize > 0) {

        tiling.bshCurLength = tiling.inputTailSize;
        tiling.shCurLength = tiling.meanVarTailSize;
        tiling.shCurLengthBlockNum = tiling.shCurLength / FLOAT_BLOCK_NUMBER;
        GetSrcOffset<needCast>(params.srcOffset, tiling);
        GetUpdataParams(tiling, params);

        BatchNormExeImpl<isBasicBlock>(inputX[tiling.inputTailPos], gamm, beta, output[tiling.inputTailPos],
            outputMean[tiling.meanVarTailPos], outputVariance[tiling.meanVarTailPos],
            params.meanTmpTensor[tiling.meanVarTailPos], params.varianceTmpTensor[tiling.meanVarTailPos], epsilon,
            tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormCompute(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    constexpr bool needCast = false;
    uint32_t mvOffset = 0;

    GetSrcOffset<needCast>(params.srcOffset, tiling);
    GetUpdataParams(tiling, params);

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        BatchNormExeImpl<isBasicBlock>(inputX[mvOffset], gamm, beta, output[mvOffset], outputMean[mvOffset],
            outputVariance[mvOffset], params.meanTmpTensor[mvOffset], params.varianceTmpTensor[mvOffset], epsilon,
            tiling, params);

        mvOffset += tiling.shCurLength;
    }
    if (tiling.inputTailSize > 0) {

        tiling.bshCurLength = tiling.inputTailSize;
        tiling.shCurLength = tiling.meanVarTailSize;
        tiling.shCurLengthBlockNum = tiling.shCurLength / FLOAT_BLOCK_NUMBER;
        GetSrcOffset<needCast>(params.srcOffset, tiling);
        GetUpdataParams(tiling, params);

        BatchNormExeImpl<isBasicBlock>(inputX[tiling.inputTailPos], gamm, beta, output[tiling.inputTailPos],
            outputMean[tiling.meanVarTailPos], outputVariance[tiling.meanVarTailPos],
            params.meanTmpTensor[tiling.meanVarTailPos], params.varianceTmpTensor[tiling.meanVarTailPos], epsilon,
            tiling, params);
    }
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, BatchNormTiling& tiling)
{

                                                                                                 ;

                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    BatchNormParams<float> params;
    GetBatchNormInfo<isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);
    params.firstDimValueBack = tiling.firstDimValueBack;

    SetMaskCount();
    BatchNormCompute<isBasicBlock>(inputX, gamm, beta, output, outputMean, outputVariance, epsilon, tiling, params);

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const T epsilon, BatchNormTiling& tiling)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        sharedTmpBuffer, epsilon, tiling);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/batchnorm.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, BatchNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        sharedTmpBuffer, epsilon, tiling);
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void BatchNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const T epsilon, BatchNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta, epsilon,
        tiling);
}
#pragma end_pipe
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tanh/tanh_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tanh/tanh_common_impl.h"
namespace AscendC {
constexpr float FP32_MIN_V2 = -8.8;
constexpr float FP32_MAX_V2 = 8.8;
constexpr float DOUBLE_X = 2;
const uint8_t TANH_HALF_CALC_PROCEDURE = 2;
const uint8_t TANH_FLOAT_CALC_PROCEDURE = 1;



[aicore] __inline__ __attribute__((always_inline)) void TanhFormulaImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const TanhParams<float>& params)
{
    const LocalTensor<float>& tmpClip = params.tmpClip;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Mins<float, false>(tmpClip, srcTensor, FP32_MAX_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(tmpClip, tmpClip, FP32_MIN_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpClip, tmpClip, DOUBLE_X, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(tmpClip, tmpClip, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(dstTensor, tmpClip, -1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpClip, tmpClip, 1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, dstTensor, tmpClip, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void TanhCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const TanhParams<float>& params)
{
    TanhFormulaImpl(dstTensor, srcTensor, params);
}

template<>
[aicore] __inline__ __attribute__((always_inline)) void TanhCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const TanhParams<float>& params)
{
    const LocalTensor<float>& tempTensorConv = params.tempTensorConv;
    Cast<float, half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    TanhFormulaImpl(tempTensorConv, tempTensorConv, params);

    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TanhFormulasTmpCalc(TanhParams<float>& params)
{
    uint32_t tmpUbIndex = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        params.stackSize = params.tmpBufferSize / TANH_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        params.stackSize = params.tmpBufferSize / TANH_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }


      ;
    if constexpr (sizeof(T) == sizeof(half)) {
        params.tempTensorConv = params.sharedTmpBuffer[params.stackSize * (tmpUbIndex++)];
    }
    params.tmpClip = params.sharedTmpBuffer[params.stackSize * (tmpUbIndex++)];
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 124 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tanh/tanh_common_impl.h"
    TanhParams<float> params;
    params.calCount = calCount;
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);


      ;

    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    TanhFormulasTmpCalc<T>(params);

    const uint32_t round = params.calCount / params.stackSize;
    const uint32_t tail = params.calCount % params.stackSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        TanhCompute(dstTensor[offset], srcTensor[offset], params);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        TanhCompute(dstTensor[offset], srcTensor[offset], params);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TanhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Tanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TanhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Tanh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    TanhImpl(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atanh/atanh_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/atanh/atanh_common_impl.h"
namespace AscendC {
constexpr uint32_t ATANH_FLOAT_CALC_PROC = 1;
constexpr uint32_t ATANH_HALF_CALC_PROC = 4;




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AtanhCompute(const LocalTensor<T>& dstTensor,
    const LocalTensor<T>& srcTensor,
    const LocalTensor<float> &tmpBuffer,
    uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Adds<float, false>(tmpBuffer, srcTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(dstTensor, srcTensor, static_cast<T>(-1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(dstTensor, tmpBuffer, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, dstTensor, static_cast<T>(0.5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



template <>
[aicore] __inline__ __attribute__((always_inline)) void AtanhCompute(const LocalTensor<half>& dstTensor,
    const LocalTensor<half>& srcTensor,
    const LocalTensor<float> &tmpBuffer,
    uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpBuffer[calSize];


    Cast<float, half, false>(tmpFloatBuffer1, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, static_cast<float>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(-1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(0.5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<half, float, false>(dstTensor, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(T);

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / ATANH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ATANH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T>(0, splitCount);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    for (uint32_t i = 0; i < loopCount; i++) {
        AtanhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }

    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AtanhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AtanhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AtanhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Atanh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_v220_impl.h"
namespace AscendC {
namespace DeepNormAPI {
constexpr uint32_t BASIC_BLOCK_HLENGTH = 64;
constexpr uint32_t BASIC_BLOCK_BSLENGTH = 8;
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr uint8_t HALF_REPEAT_STRIDE = 4;
constexpr float SQRT_EXPONENT = -0.5;



[aicore] __inline__ __attribute__((always_inline)) void DeepNormBasicBlockVbrcb(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength)
{
    constexpr uint16_t brcbDstBlkStride = 8;
    constexpr uint16_t brcbDstRepStride = 64;
    constexpr uint16_t addSrcBlkStride = 0;
    const uint8_t repeatTimes = bsLength / 8;
    SetMaskNorm();
    ResetMask();

    BrcbRepeatParams brcbParams(brcbDstBlkStride, brcbDstRepStride);

    Brcb<float>(dst, src, repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);


    Adds<float, false>(dst, dst, 0, MASK_PLACEHOLDER, 1,
        {DEFAULT_BLK_STRIDE, addSrcBlkStride, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormVarianceBasicBlockByBrcb(const LocalTensor<float>& inputX,
    const LocalTensor<float>& inputMean, const DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    const uint8_t num = tiling.hLength / BASIC_BLOCK_HLENGTH;



    DeepNormBasicBlockVbrcb(params.tempTensorC, inputMean, tiling.bsCurLength);

    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src0RepStride = num * DEFAULT_REPEAT_STRIDE;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength / num);
    for (uint32_t i = 0; i < num; i++) {

        Sub<float, false>(params.tempTensorB[i * BASIC_BLOCK_HLENGTH], inputX[i * BASIC_BLOCK_HLENGTH],
            params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormOutputBasicBlockByBrcb(const LocalTensor<float>& xSubMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const uint8_t num = tiling.hLength / BASIC_BLOCK_HLENGTH;
    const UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src1RepStride = num * DEFAULT_REPEAT_STRIDE;



    DeepNormBasicBlockVbrcb(params.tempTensorC, params.tempTensorA, tiling.bsCurLength);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength / num);
    for (uint32_t i = 0; i < num; i++) {

        Mul<float, false>(params.tempTensorA[i * BASIC_BLOCK_HLENGTH], params.tempTensorC,
            xSubMean[i * BASIC_BLOCK_HLENGTH], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();


    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Adds<float, false>(params.tempTensorC, params.tempTensorA, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/deepnorm/deepnorm_common_impl.h" 2




namespace AscendC {
namespace DeepNormAPI {
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) bool IsDeepNormParamValid(DeepNormTiling& tiling)
{


                       ;


                       ;

    const bool hDivBy64 = (tiling.hLength % BASIC_BLOCK_HLENGTH == 0) &&
        (tiling.originalHLength % BASIC_BLOCK_HLENGTH == 0);
    const bool bsDivBy8 = ((tiling.bLength * tiling.sLength) % BASIC_BLOCK_BSLENGTH == 0);
    if constexpr (isBasicBlock) {



                           ;
    }

    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void IsStackBufferValid(const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling)
{



      ;
}


[aicore] __inline__ __attribute__((always_inline)) bool IsBasicBlockTmp8HBetter(const DeepNormTiling& tiling)
{
    bool bs8Check = (tiling.oneTmpSize % (tiling.hLength * BASIC_BLOCK_BSLENGTH)) == 0;

    bool bsWorse = tiling.bsCurLength > (tiling.hLength / BASIC_BLOCK_HLENGTH);
    return bs8Check && bsWorse;
}



template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormTensorInfo(const LocalTensor<half>& inputX, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling,
    DeepNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];




      ;

    IsStackBufferValid(stackBuffer, tiling);
}


template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormTensorInfo(const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling,
    DeepNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    if constexpr (isReuseSource) {
        params.tempTensorA = inputX;
        params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];



          ;
    } else {
        params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



          ;
    }

    IsStackBufferValid(stackBuffer, tiling);
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormExec(const LocalTensor<half>& inputX, const LocalTensor<half>& inputGx,
    const LocalTensor<half>& output, const half alpha, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);


    unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
    Cast<float, half, false>(params.tempTensorA, inputX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(params.tempTensorC, inputGx, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;

    Muls<float, false>(params.tempTensorA, params.tempTensorA, static_cast<float>(alpha), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tempTensorA, params.tempTensorC, params.tempTensorA, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormExec(const LocalTensor<float>& inputX, const LocalTensor<float>& inputGx,
    const LocalTensor<float>& output, const float alpha, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);


    Muls<float, false>(params.tempTensorB, inputX, alpha, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tempTensorA, inputGx, params.tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormBasicBlockReduceSum(const LocalTensor<float>& output, const LocalTensor<float>& tmp,
    const LocalTensor<float>& input, const UnaryRepeatParams& unaryParams, const uint32_t bsLength,
    const uint32_t hLength)
{
    const uint8_t num = hLength / BASIC_BLOCK_HLENGTH;

    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src0RepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src1RepStride = num * DEFAULT_REPEAT_STRIDE;





    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);
    for (uint32_t i = 1; i < num; i++) {
        Add<float, false>(input, input[i * BASIC_BLOCK_HLENGTH], input, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    RepeatReduceSum<float, false>(output, input, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
        num * DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    RepeatReduceSum<float, false>(tmp, input, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
        num * DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength);
}



[aicore] __inline__ __attribute__((always_inline)) void DeepNormReduceSumImpl(const LocalTensor<float>& dstMVTmp, const LocalTensor<float>& dst,
    const LocalTensor<float>& src, const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    for (uint32_t i = 0; i < bsLength; i++) {
        uint32_t totalNum = originalHLength;
        LocalTensor<float> srcTmp = src[i * hLength];
        LocalTensor<float> dstTmp = dst[i * hLength];

        while (totalNum > 1) {
            SetVectorMask<float, MaskMode::COUNTER>(0, totalNum);


            if (totalNum <= ONE_REPEAT_FLOAT_SIZE) {
                RepeatReduceSum<float, false>(dstMVTmp[i], srcTmp, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                dstTmp = dst[i];
            }

            RepeatReduceSum<float, false>(dstTmp, srcTmp, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();

            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
            srcTmp = dstTmp;
        }
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength);
}



template <bool isBasicBlock = false, uint8_t mode = 0>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormBshHCalc(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr(isBasicBlock) {
        const uint32_t loop = hLength / BASIC_BLOCK_HLENGTH;
        const uint16_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);

        SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);
        for (uint32_t i = 0; i < loop; i++) {
            uint32_t offset = i * BASIC_BLOCK_HLENGTH;
            if constexpr(mode) {
                Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
            } else {
                Add<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, hLength);
    } else {
        BinaryRepeatParams binaryParams;
        for (uint32_t i = 0; i < bsLength; i++) {
            uint32_t offset = i * hLength;
            if constexpr(mode) {
                Mul<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
            } else {
                Add<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
    }
}




template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputMean(const LocalTensor<float>& tmpMean, const LocalTensor<float>& inputX,
    const DeepNormTiling& tiling, const DeepNormParams<float>& params, const LocalTensor<float>& outputMean)
{
    const UnaryRepeatParams unaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);

    Muls<float, false>(params.tempTensorC, inputX, params.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    if constexpr(isBasicBlock) {
        DeepNormBasicBlockReduceSum(outputMean, tmpMean, params.tempTensorC, unaryParams, tiling.bsCurLength,
            tiling.hLength);
    } else {
        DeepNormReduceSumImpl(outputMean, tmpMean, params.tempTensorC, tiling.bsCurLength, tiling.hLength,
            tiling.originalHLength);
    }
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormVarianceByForLoop(const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean,
    const DeepNormTiling& tiling, const DeepNormParams<float>& params, const UnaryRepeatParams& unaryParams)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        Adds<float, false>(params.tempTensorB[i * tiling.hLength], inputX[i * tiling.hLength],
            (float)((inputMean.GetValue(i))*(-1)), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}


[aicore] __inline__ __attribute__((always_inline)) void DeepNormOutputByForLoop(const LocalTensor<float>& xSubMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params, const UnaryRepeatParams& unaryParams)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        Muls<float, false>(params.tempTensorC[i * tiling.hLength], xSubMean[i * tiling.hLength],
            (float)params.tempTensorA.GetValue(i), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}



template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputVariance(const LocalTensor<float>& tmpVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params, const LocalTensor<float>& outputVariance)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    if constexpr(isBasicBlock) {
        if ((IsBasicBlockTmp8HBetter(tiling))) {
            DeepNormVarianceBasicBlockByBrcb(inputX, inputMean, tiling, params);
        } else {
            DeepNormVarianceByForLoop(inputX, inputMean, tiling, params, unaryParams);
        }
    } else {
        DeepNormVarianceByForLoop(inputX, inputMean, tiling, params, unaryParams);
    }


    Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tempTensorA, params.tempTensorC, params.lastDimValueBack, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    if constexpr(isBasicBlock) {
        DeepNormBasicBlockReduceSum(outputVariance, tmpVariance, params.tempTensorA, unaryParams, tiling.bsCurLength,
            tiling.hLength);
    }
    else {
        DeepNormReduceSumImpl(outputVariance, tmpVariance, params.tempTensorA, tiling.bsCurLength, tiling.hLength,
            tiling.originalHLength);
    }
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputPre(const LocalTensor<float>& xSubMean,
    const LocalTensor<float>& inputVariance, const float epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bsCurLength);

    Adds<float, false>(params.tempTensorA, inputVariance, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tempTensorC, float(1.0), 1, 1, 1, 8);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(params.tempTensorA, params.tempTensorA, 1, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(params.tempTensorA, params.tempTensorC, params.tempTensorA, 1, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr(isBasicBlock) {

        if ((IsBasicBlockTmp8HBetter(tiling))) {
            DeepNormOutputBasicBlockByBrcb(xSubMean, tiling, params);
            return;
        }
    }
    DeepNormOutputByForLoop(xSubMean, tiling, params, unaryParams);
}


template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutput(const LocalTensor<half>& output, const LocalTensor<float>& inputY,
    const LocalTensor<half>& gamm, const LocalTensor<half>& beta, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);

    unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
    Cast<float, half, false>(params.tempTensorA, gamm, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    DeepNormBshHCalc<isBasicBlock, 1>(params.tempTensorB, inputY, params.tempTensorA, tiling.bsCurLength,
        tiling.hLength);


    Cast<float, half, false>(params.tempTensorC, beta, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    DeepNormBshHCalc<isBasicBlock, 0>(params.tempTensorA, params.tempTensorB, params.tempTensorC, tiling.bsCurLength,
        tiling.hLength);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;


    Cast<half, float, false>(output, params.tempTensorA, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutput(const LocalTensor<float>& output, const LocalTensor<float>& inputY,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    DeepNormBshHCalc<isBasicBlock, 1>(params.tempTensorA, inputY, gamm, tiling.bsCurLength, tiling.hLength);
    DeepNormBshHCalc<isBasicBlock, 0>(output, params.tempTensorA, beta, tiling.bsCurLength, tiling.hLength);
}


[aicore] __inline__ __attribute__((always_inline)) void GetDeepNormOutputMeanVariance(const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.meanVarSize);

    Cast<half, float, false>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
}



template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormLayerNormExec(const LocalTensor<float>& inputX, const LocalTensor<half>& gamm,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const half epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    GetDeepNormOutputMean<isBasicBlock>(params.tempTensorC, params.tempTensorA, tiling, params, outputMean);
    GetDeepNormOutputVariance<isBasicBlock>(params.tempTensorC, params.tempTensorA, outputMean, tiling, params,
        outputVariance);
    GetDeepNormOutputPre<isBasicBlock>(params.tempTensorB, params.tempTensorC, static_cast<float>(epsilon), tiling,
        params);
    GetDeepNormOutput<isBasicBlock>(output, params.tempTensorC, gamm, beta, tiling, params);
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormLayerNormExec(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    GetDeepNormOutputMean<isBasicBlock>(params.tempTensorC, inputX, tiling, params, outputMean);
    GetDeepNormOutputVariance<isBasicBlock>(params.tempTensorC, inputX, outputMean, tiling, params, outputVariance);
    GetDeepNormOutputPre<isBasicBlock>(params.tempTensorB, params.tempTensorC, epsilon, tiling, params);
    GetDeepNormOutput<isBasicBlock>(output, params.tempTensorC, gamm, beta, tiling, params);
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormND(const LocalTensor<T>& inputX, const LocalTensor<T>& inputGx,
    const LocalTensor<T>& gamm, const LocalTensor<T>& beta, const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance, const T alpha, const T epsilon,
    DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    uint32_t BSHOffset = 0;
    uint32_t BSOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        DeepNormExec(inputX[BSHOffset], inputGx[BSHOffset], output, alpha, tiling, params);
        DeepNormLayerNormExec<isBasicBlock>(params.tempTensorA, gamm, beta, output[BSHOffset],
            params.meanTmpTensor[BSOffset], params.varianceTmpTensor[BSOffset], epsilon, tiling, params);
        BSHOffset += tiling.inputRoundSize;
        BSOffset += tiling.meanVarRoundSize;
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        BSHOffset = tiling.inputTailPos;
        BSOffset = tiling.meanVarTailPos;
        DeepNormExec(inputX[BSHOffset], inputGx[BSHOffset], output, alpha, tiling, params);
        DeepNormLayerNormExec<isBasicBlock>(params.tempTensorA, gamm, beta, output[BSHOffset],
            params.meanTmpTensor[BSOffset], params.varianceTmpTensor[BSOffset], epsilon, tiling, params);
    }


    if constexpr(IsSameType<T, half>::value) {
        GetDeepNormOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSrc, bool isBasicBlock>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const T alpha, const T epsilon, DeepNormTiling& tiling)
{
    if (!DeepNormAPI::IsDeepNormParamValid<T, isBasicBlock>(tiling)) {
        return;
    }
                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    DeepNormParams<float> deepnormParams;
    DeepNormAPI::GetDeepNormTensorInfo<isReuseSrc>(srcLocal, meanLocal, rstdLocal, stackBuffer, tiling, deepnormParams);
    deepnormParams.lastDimValueBack = tiling.lastDimValueBack;

    SetMaskCount();
    DeepNormAPI::DeepNormND<T, isBasicBlock>(srcLocal, gxLocal, gammaLocal, betaLocal, dstLocal, meanLocal, rstdLocal,
        alpha, epsilon, tiling, deepnormParams);
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSrc, bool isBasicBlock>
[aicore] __inline__ __attribute__((always_inline)) void DeepNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const T alpha, const T epsilon,
    DeepNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, sharedTmpBuffer, alpha, epsilon, tiling);
}

}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h"
template <typename T, bool isReuseSrc = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const T alpha, const T epsilon, DeepNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DeepNormAPI::DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, sharedTmpBuffer, alpha, epsilon, tiling);
}
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/deepnorm.h"
template <typename T, bool isReuseSrc = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void DeepNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const T alpha, const T epsilon,
    DeepNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DeepNormAPI::DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, alpha, epsilon, tiling);
}
#pragma end_pipe
}
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/exp/exp_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/exp/exp_common_impl.h"
namespace AscendC {
namespace ExpAPI {
constexpr uint8_t HALF_REPEAT_STRIDE = 4;
constexpr uint32_t EXP_TWO = 2;
constexpr uint32_t EXP_THREE = 3;
constexpr uint32_t EXP_FOUR = 4;


template <typename T, bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] __inline__ __attribute__((always_inline)) void UpdataExpParams(const LocalTensor<T>& src, const uint32_t calCount,
    const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    uint32_t alignNum = ONE_BLK_SIZE / sizeof(T);



    bool isFloat = IsSameType<T, float>::value;
    uint32_t numberOfTmpBuf = EXP_FOUR;
    if (isFloat) {
        numberOfTmpBuf = isReuseSource ? EXP_TWO : EXP_THREE;
    }

    uint32_t inputSize = calCount;
    uint32_t stackBufferSize = stackBuffer.GetSize();
    uint32_t oneTmpSize = stackBufferSize / numberOfTmpBuf;
    oneTmpSize = oneTmpSize / alignNum * alignNum;
    uint32_t secondOffset = (isFloat && isReuseSource)? 0 : oneTmpSize;
    uint32_t fourthOffset = isFloat ? 0 : oneTmpSize;



      ;

    params.inputSize = inputSize;
    params.oneTmpSize = oneTmpSize;
    params.firstTmpStartPos = 0;
    params.secondTmpStartPos = secondOffset;
    params.thirdTmpStartPos = params.secondTmpStartPos + oneTmpSize;
    params.fourthTmpStartPos = params.thirdTmpStartPos + fourthOffset;
    params.loopNum = inputSize / oneTmpSize;
    params.tailSize = inputSize % oneTmpSize;
    params.tailPos = inputSize - params.tailSize;
    params.curDataLength = oneTmpSize;
    params.expandLevel = expandLevel;
}

template <bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] __inline__ __attribute__((always_inline)) void GetExpTensorInfo(const LocalTensor<half>& src, const LocalTensor<half>& dst,
    const uint32_t calCount, const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    UpdataExpParams<half, isReuseSource, expandLevel>(src, calCount, stackBuffer, params);
    params.tempTensorFloorX = stackBuffer[params.firstTmpStartPos];
    params.tempTensorFloorXPow = stackBuffer[params.secondTmpStartPos];
    params.tempTensorRes = stackBuffer[params.thirdTmpStartPos];
    params.tempTensorIntPart = stackBuffer[params.fourthTmpStartPos];
}

template <bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] __inline__ __attribute__((always_inline)) void GetExpTensorInfo(const LocalTensor<float>& src, const LocalTensor<float>& dst,
    const uint32_t calCount, const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    UpdataExpParams<float, isReuseSource, expandLevel>(src, calCount, stackBuffer, params);
    if constexpr(isReuseSource) {
        params.tempTensorFloorX = src;
    } else {
        params.tempTensorFloorX = stackBuffer[params.firstTmpStartPos];
    }
    params.tempTensorFloorXPow = stackBuffer[params.secondTmpStartPos];
    params.tempTensorRes = dst;
    params.tempTensorIntPart = stackBuffer[params.fourthTmpStartPos];
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetExpInputInTmp(const LocalTensor<T>& src, const ExpParams<float>& params, uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);
    if constexpr (IsSameType<T, half>::value) {
        unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
        Cast<float, half, false>(params.tempTensorFloorX, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    } else {
        Adds<float, false>(params.tempTensorFloorX, src, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}




[aicore] __inline__ __attribute__((always_inline)) void GetExpFloorInput(const ExpParams<float>& params, uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);



    Cast<float, float, false>(params.tempTensorIntPart, params.tempTensorFloorX, RoundMode::CAST_FLOOR,
        MASK_PLACEHOLDER, 1, unaryParams);







    PipeBarrier<PIPE_V>();


    Sub<float, false>(params.tempTensorFloorX, params.tempTensorFloorX, params.tempTensorIntPart, MASK_PLACEHOLDER, 1,
        binaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(params.tempTensorIntPart, params.tempTensorIntPart, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}




[aicore] __inline__ __attribute__((always_inline)) void ExpHighPrecisionExec(const ExpParams<float>& params, uint32_t maskLength, uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);


    Adds<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorX, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tempTensorRes[offset], params.tempTensorFloorX, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    for (int32_t i = 2; i < params.expandLevel + 1; i++) {

        Mul<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorX, params.tempTensorFloorXPow,
            MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();


        Muls<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorXPow, float(1.0) / float(i),
            MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();


        Add<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], params.tempTensorFloorXPow,
            MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], params.tempTensorIntPart,
        MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void GetExpCastedResult(const LocalTensor<half>& dst, const ExpParams<float>& params,
    uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;
    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);
    Cast<half, float, false>(dst, params.tempTensorRes, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ExpHighPrecisionND(const LocalTensor<T>& src, const LocalTensor<T>& dst,
    const ExpParams<float>& params, uint32_t offset, uint32_t maskLength)
{
    GetExpInputInTmp(src[offset], params, maskLength);
    GetExpFloorInput(params, maskLength);


    if constexpr(IsSameType<T, half>::value) {
        ExpHighPrecisionExec(params, maskLength, 0);
        GetExpCastedResult(dst[offset], params, maskLength);
    } else {
        ExpHighPrecisionExec(params, maskLength, offset);
    }
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ExpND(const LocalTensor<T>& src, const LocalTensor<T>& dst, const ExpParams<float>& params)
{
    SetMaskCount();

    uint32_t offset = 0;
    for (uint32_t index = 0; index < params.loopNum; index++) {
        ExpHighPrecisionND(src, dst, params, offset, params.curDataLength);
        offset += params.oneTmpSize;
    }

    if (params.tailSize > 0) {
        ExpHighPrecisionND(src, dst, params, offset, params.tailSize);
    }
}

template <typename T, uint8_t taylorExpandLevel, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (taylorExpandLevel == 0) {
        Exp<T>(dstLocal, srcLocal, calCount);
        return;
    }


                 ;
                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    ExpParams<float> expParams;

    ExpAPI::GetExpTensorInfo<isReuseSource, taylorExpandLevel>(srcLocal, dstLocal, calCount, stackBuffer, expParams);
    ExpAPI::ExpND<T>(srcLocal, dstLocal, expParams);

    SetMaskNorm();
    ResetMask();
}

template <typename T, uint8_t taylorExpandLevel, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void ExpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, sharedTmpBuffer, calCount);
}

}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h" 2

namespace AscendC {

#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h"
template <typename T, uint8_t taylorExpandLevel, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value)
        && "Exp input datatype must be half or float!");
    ExpAPI::ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, calCount);
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/exp.h"
template <typename T, uint8_t taylorExpandLevel, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value)
        && "Exp input datatype must be half or float!");
    ExpAPI::ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/../../impl/normalization/layernorm/layernorm_common_impl.h"
namespace AscendC {
template <bool isRelocate = true, bool isTransposeDst = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormReduceSumImpl(const LocalTensor<float>& dstMVTmp, const LocalTensor<float>& dst,
    const LocalTensor<float>& src, const uint32_t bsLength, const uint32_t hLength)
{
    ResetMask();
    SetMaskNorm();
    constexpr uint32_t rightShiftSix = 6;
    if (hLength > ONE_REPEAT_FLOAT_SIZE) {
        uint32_t addRepeatTime = (hLength >> rightShiftSix) - 1;
        uint32_t addTailNumber = (hLength & 0x3f);
        if ((hLength & 0x3F) == 0) {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTmp = src[i + ONE_REPEAT_FLOAT_SIZE];
                Add(dstTmp, srcTmp, dstTmp, ONE_REPEAT_FLOAT_SIZE, addRepeatTime,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        } else if (addRepeatTime > 0) {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTmp = src[i + ONE_REPEAT_FLOAT_SIZE];
                LocalTensor<float> srcTailTmp = src[i + hLength & 0xFFFFFFC0];
                Add(dstTmp, srcTmp, dstTmp, ONE_REPEAT_FLOAT_SIZE, addRepeatTime,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
                Add(dstTmp, srcTailTmp, dstTmp, addTailNumber, 1,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        } else {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTailTmp = src[i + hLength & 0xFFFFFFC0];
                Add(dstTmp, srcTailTmp, dstTmp, addTailNumber, 1,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        }
    }

    uint32_t repeatTime = bsLength;
    uint32_t cursorSrc = 0;
    uint32_t wholeReduceSumHLength = (hLength > ONE_REPEAT_FLOAT_SIZE) ? ONE_REPEAT_FLOAT_SIZE : hLength;
    constexpr uint32_t rightShiftThree = 3;
    const uint32_t reduceSumSrcRepeatStride = hLength >> rightShiftThree;

    while (repeatTime >= MAX_REPEAT_TIMES) {
        LocalTensor<float> srcTmp = src[cursorSrc * MAX_REPEAT_TIMES * hLength];
        LocalTensor<float> dstTmp = dst[cursorSrc * MAX_REPEAT_TIMES * hLength];
        if constexpr (isRelocate) {
            WholeReduceSum<float>(dstMVTmp[cursorSrc * MAX_REPEAT_TIMES], srcTmp, wholeReduceSumHLength,
                MAX_REPEAT_TIMES, 1, DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        }
        WholeReduceSum<float>(dstTmp, srcTmp, wholeReduceSumHLength, MAX_REPEAT_TIMES, hLength, DEFAULT_BLK_STRIDE,
            reduceSumSrcRepeatStride);
        PipeBarrier<PIPE_V>();
        repeatTime -= MAX_REPEAT_TIMES;
        ++cursorSrc;
    }

    uint32_t reduceSumSrcRepeatTimeTail = bsLength - cursorSrc * MAX_REPEAT_TIMES;
    if (reduceSumSrcRepeatTimeTail > 0) {
        LocalTensor<float> srcTmp = src[cursorSrc * MAX_REPEAT_TIMES * hLength];
        LocalTensor<float> dstTmp = dst[cursorSrc * MAX_REPEAT_TIMES * hLength];
        if constexpr (isRelocate) {
            WholeReduceSum<float>(dstMVTmp[cursorSrc * MAX_REPEAT_TIMES], srcTmp, wholeReduceSumHLength,
                reduceSumSrcRepeatTimeTail, 1, DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        }
        WholeReduceSum<float>(dstTmp, srcTmp, wholeReduceSumHLength, reduceSumSrcRepeatTimeTail, hLength,
            DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
}

[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutputMean(const LocalTensor<float>& outputMean, const LocalTensor<float>& inputX,
    const LayerNormTiling& tiling, const LayerNormParams<float>& params, const LocalTensor<float>& tmpMean)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const UnaryRepeatParams unaryParams;
    Muls<float, false>(params.tempTensorC, inputX, tiling.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(tmpMean, outputMean, params.tempTensorC, tiling.bsCurLength, tiling.hLength);
}

[aicore] __inline__ __attribute__((always_inline)) void BroadcastLastDim(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, hLength);

    SetCmpMask<float>(src);
    PipeBarrier<PIPE_V>();

    LocalTensor<int16_t> maskLocal = src.ReinterpretCast<int16_t>();

    const UnaryRepeatParams unaryParams;
    Muls<int16_t, false>(maskLocal, maskLocal, 0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Select<float, int16_t>(dst, maskLocal, dst, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 1; i < bsLength; i++) {
        SetCmpMask<float>(src[i * hLength]);
        PipeBarrier<PIPE_V>();

        Select<float, int16_t>(dst[i * hLength], maskLocal, dst, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutputVariance(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params, const LocalTensor<float>& tmpVariance)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    BroadcastLastDim(tempTensorC, inputMean, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const BinaryRepeatParams binaryParams;
    Sub<float, false>(tempTensorB, inputX, tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempTensorC, tempTensorB, tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    const UnaryRepeatParams unaryParams;
    Muls<float, false>(tempTensorA, tempTensorC, tiling.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(tmpVariance, outputVariance, tempTensorA, tiling.bsCurLength, tiling.hLength);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutputPre(const LocalTensor<float>& xSubMean,
    const LocalTensor<float>& inputVariance, const float epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    const float exponent = -0.5;
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    BroadcastLastDim(tempTensorA, inputVariance, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const UnaryRepeatParams unaryParams;
    Adds<float, false>(tempTensorC, tempTensorA, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<float, false>(tempTensorA, tempTensorC, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_BLOCK);
    Duplicate<float, false>(tempTensorC, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Div<float, false>(tempTensorA, tempTensorC, tempTensorA, MASK_PLACEHOLDER, 1,
        { 1, 0, 1, DEFAULT_REPEAT_STRIDE, 0, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<float, false>(tempTensorC, tempTensorA, xSubMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DuplicateMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < bsLength; i++) {
        Mul<float, false>(dst[i * hLength], src0[i * hLength], src1, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DuplicateAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < bsLength; i++) {
        Add<float, false>(dst[i * hLength], src0[i * hLength], src1, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutput(const LocalTensor<T>& output, const LocalTensor<float>& inputY,
    const LocalTensor<T>& gamma, const LocalTensor<T>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutput<half>(const LocalTensor<half>& output, const LocalTensor<float>& inputY,
    const LocalTensor<half>& gamma, const LocalTensor<half>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.hLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<float, half, false>(tempTensorA, gamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    DuplicateMulImpl(tempTensorB, inputY, tempTensorA, tiling.bsCurLength, tiling.hLength);

    Cast<float, half, false>(tempTensorC, beta, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    DuplicateAddImpl(tempTensorA, tempTensorB, tempTensorC, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, float, false>(output, tempTensorA, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormOutput<float>(const LocalTensor<float>& output, const LocalTensor<float>& inputY,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.hLength);

    DuplicateMulImpl(tempTensorA, inputY, gamma, tiling.bsCurLength, tiling.hLength);

    DuplicateAddImpl(output, tempTensorA, beta, tiling.bsCurLength, tiling.hLength);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormExe(const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const T epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormExe<half>(const LocalTensor<half>& inputX, const LocalTensor<half>& gamma,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const half epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<float, half, false>(tempTensorA, inputX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    GetLayerNormOutputMean(tempTensorB, tempTensorA, tiling, params, outputMean);

    GetLayerNormOutputVariance(tempTensorC, tempTensorA, tempTensorB, tiling, params, outputVariance);

    GetLayerNormOutputPre(tempTensorB, tempTensorC, static_cast<float>(epsilon), tiling, params);

    GetLayerNormOutput(output, tempTensorC, gamma, beta, tiling, params);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormExe<float>(const LocalTensor<float>& inputX, const LocalTensor<float>& gamma,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    GetLayerNormOutputMean(tempTensorB, inputX, tiling, params, outputMean);

    GetLayerNormOutputVariance(tempTensorC, inputX, tempTensorB, tiling, params, outputVariance);

    GetLayerNormOutputPre(tempTensorB, tempTensorC, epsilon, tiling, params);

    GetLayerNormOutput(output, tempTensorC, gamma, beta, tiling, params);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormNDTensorInfo(const LocalTensor<T>& inputX, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<float>& stackBuffer, const LayerNormTiling& tiling,
    LayerNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];



      ;



      ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormNDTensorInfo<float, false>(const LocalTensor<float> &inputX,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &stackBuffer, const LayerNormTiling &tiling, LayerNormParams<float> &params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];




      ;




      ;
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void GetLayerNormNDTensorInfo<float, true>(const LocalTensor<float> &inputX,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &stackBuffer, const LayerNormTiling &tiling, LayerNormParams<float> &params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = inputX;
    params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];




      ;




      ;
}

[aicore] __inline__ __attribute__((always_inline)) void GetOutputMeanVariance(const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LayerNormTiling& tiling, const LayerNormParams<float>& params)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.meanVarSize);

    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, float, false>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormND(const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const T epsilon, LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        LayerNormExe<T>(inputX[inputOffset], gamma, beta, output[inputOffset], params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params);

        inputOffset += tiling.inputRoundSize;
        mvOffset += tiling.meanVarRoundSize;
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        LayerNormExe<T>(inputX[inputOffset], gamma, beta, output[inputOffset], params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params);
    }

    if constexpr (sizeof(T) == sizeof(half)) {
        GetOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, LayerNormTiling& tiling)
{
                                   ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
                                      ;
        return;
    }

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
                                                                                                                ;

    LayerNormParams<float> params;
    GetLayerNormNDTensorInfo<T, isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);

    SetMaskCount();
    LayerNormND<T>(inputX, gamma, beta, output, outputMean, outputVariance, epsilon, tiling, params);

    SetMaskNorm();
    ResetMask();
                                  ;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, LayerNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, LayerNormTiling& tiling)
{
    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/normalization/layernorm.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LayerNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, LayerNormTiling& tiling)
{
    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, epsilon, tiling);
}
#pragma end_pipe
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h" 2




namespace AscendC {
struct SumParams {
    uint32_t outter = 1;
    uint32_t inner;
    uint32_t n;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckParamsIsValid(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/sum/sum_common_impl.h"
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SumForOneRepeatTime(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const SumParams &sumParams)
{
    SetVectorMask<T>(0, sumParams.n);
    for (uint32_t row = 0; row < sumParams.outter; ++row) {
        RepeatReduceSum<T, false>(dstTensor[row], srcTensor[row * sumParams.inner], 1, MASK_PLACEHOLDER,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SumCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    CheckParamsIsValid(dstTensor, srcTensor, sharedTmpBuffer, sumParams);

    uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t firstRepeatTimes = (sumParams.n + elementNumPerRep - 1) / elementNumPerRep;
    SetMaskCount();
    if (firstRepeatTimes == 1) {
        return SumForOneRepeatTime(dstTensor, srcTensor, sumParams);
    }
    uint32_t totalCnt = 1;
    uint32_t dataSize = firstRepeatTimes;
    while (dataSize > 1) {
        ++totalCnt;
        dataSize = (dataSize + elementNumPerRep - 1) / elementNumPerRep;
    }
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    for (uint32_t row = 0; row < sumParams.outter; ++row) {
        uint32_t cnt = totalCnt;
        uint64_t lowMask = sumParams.n;
        SetVectorMask<T>(0, lowMask);
        RepeatReduceSum<T, false>(tmpTensor, srcTensor[row * sumParams.inner], 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);

        PipeBarrier<PIPE_V>();
        lowMask = (lowMask + elementNumPerRep - 1) / elementNumPerRep;
        --cnt;
        while (cnt != 0) {
            SetVectorMask<T>(0, lowMask);
            if (cnt == 1) {
                RepeatReduceSum<T, false>(dstTensor[row], tmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            } else {
                RepeatReduceSum<T, false>(tmpTensor, tmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();
            lowMask = (lowMask + elementNumPerRep - 1) / elementNumPerRep;
            --cnt;
        }
    }
    SetMaskNorm();

}

}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h" 2




namespace AscendC {
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h"
#pragma begin_pipe(V)
template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void Sum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SumCompute<T, reduceDim, isReuseSource, isBasicBlock>(dstTensor, srcTensor, sharedTmpBuffer, sumParams);
}
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/sum.h"
template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void Sum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t repeatTimes = (sumParams.n + elementNumPerRep - 1) / elementNumPerRep;
    uint32_t finalWorkSize = (repeatTimes + elementNumPerBlk - 1) / elementNumPerBlk * elementNumPerBlk * sizeof(T);

                                                                                                             ;
    sharedTmpBuffer.SetSize(finalWorkSize);
    Sum<T, reduceDim, isReuseSource, isBasicBlock>(dstTensor, srcTensor, sharedTmpBuffer, sumParams);

}
#pragma end_pipe
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h" 2




namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SiluCalcSimplified(const LocalTensor<T> &dstAddr, const LocalTensor<T> &srcAddr,
    uint32_t repeatTimes)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Muls<T, false>(dstAddr, srcAddr, T(-1), MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(dstAddr, dstAddr, 1.0, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<T, false>(dstAddr, srcAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void SiluCompute(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    uint32_t dataSize)
{
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h"
    SetMaskCount();
    SetVectorMask<T>(0, dataSize);
    SiluCalcSimplified<T>(dstLocal, srcLocal, 1);
    SetMaskNorm();
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/silu/silu_common_impl.h"
    ResetMask();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h" 2

namespace AscendC {
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/silu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Silu(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    uint32_t dataSize)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SiluCompute<T, false>(dstLocal, srcLocal, dataSize);
}

}
# 58 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/gelu/gelu_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/gelu/gelu_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/gelu/gelu_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcTanhParams(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const LocalTensor<T>& srcLocal, const GeluParams<T>& params)
{
    const T coefficientsA = 0.044715;
    const T coefficientsB = 1.5957691216057308;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Mul<T, false>(tempTensorA, srcLocal, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tempTensorB, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorB, coefficientsA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(tempTensorB, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorB, coefficientsB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcYGreaterThanZero(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const GeluParams<T>& params)
{
    const UnaryRepeatParams unaryParams;


    Mins<T, false>(tempTensorB, tempTensorA, 0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tempTensorB, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcYLessThanZero(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const LocalTensor<T>& srcLocal, const GeluParams<T>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Abs<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorA, -1, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, 1, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr (highPerformance) {
        Reciprocal<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(tempTensorA, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(tempTensorA, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluCalcSimplifiedAvoid(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const BinaryRepeatParams binaryParams;
    const LocalTensor<T>& tempTensorA = params.tempTensorA;
    const LocalTensor<T>& tempTensorB = params.tempTensorB;



    GeluCalcTanhParams(tempTensorA, tempTensorB, srcLocal, params);


    GeluCalcYGreaterThanZero(tempTensorA, tempTensorB, params);


    GeluCalcYLessThanZero<T, highPerformance>(tempTensorA, tempTensorB, srcLocal, params);


    Mul<T, false>(dstLocal, tempTensorA, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FastGeluCalcSimplified(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const LocalTensor<T>& stackBuffer = params.tempTensorA;


    const T coefficients = -1.702;


    const UnaryRepeatParams unaryParams;
    Muls<T, false>(stackBuffer, srcLocal, coefficients, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(stackBuffer, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(stackBuffer, stackBuffer, 1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    const BinaryRepeatParams binaryParams;
    if constexpr (highPerformance) {
        Reciprocal<T, false>(stackBuffer, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(dstLocal, srcLocal, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(dstLocal, srcLocal, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FastGeluV2ClipParams(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const T coefficientsA = -0.1444;
    const T coefficientsB = -1.769;
    const T coefficientsBInv = 1.769;
    const T coefficientsC = 0.7071;
    const T coefficientsD = 0.5;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Muls<T, false>(tempTensorA, srcLocal, coefficientsC, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mins<T, false>(tempTensorA, tempTensorA, coefficientsBInv, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tempTensorA, tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorA, coefficientsA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsD, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FastGeluV2CalcSimplified(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const T coefficients = 0.000000000001;
    const T coefficientsHalf = 0.5;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    const LocalTensor<T>& tempTensorA = params.tempTensorA;
    const LocalTensor<T>& tempTensorB = params.tempTensorB;
    const LocalTensor<T>& tempTensorC = params.tempTensorC;


    FastGeluV2ClipParams(tempTensorA, srcLocal, params);


    Adds<T, false>(tempTensorB, srcLocal, coefficients, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<T, false>(tempTensorC, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr (highPerformance) {
        Reciprocal<T, false>(tempTensorC, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(tempTensorB, tempTensorB, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(tempTensorB, tempTensorB, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<T, false>(tempTensorA, tempTensorA, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsHalf, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dstLocal, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPrecision = false, uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluFormulasTmpCalc(GeluParams<T>& params)
{
    uint32_t needConvBuffer = bufferNumber;
    if constexpr (highPrecision) {
        needConvBuffer += 1;
    }

    params.tempTensorA = params.sharedTmpBuffer;
    params.stackSize = params.tmpBufferSize / needConvBuffer / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                       ;

    uint32_t nextTmpPos = params.stackSize;
    if constexpr (bufferNumber == TWO_OF_STACK_BUFFER) {
        params.tempTensorB = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
    }

    if constexpr (bufferNumber >= THREE_OF_STACK_BUFFER) {
        params.tempTensorB = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
        params.tempTensorC = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
    }

    if constexpr (highPrecision) {
        params.tempTensorConv = params.sharedTmpBuffer[nextTmpPos];
    }
}

[aicore] __inline__ __attribute__((always_inline)) void GeluCastIntrinsicsImpl(const LocalTensor<float>& dstLocal, const LocalTensor<half>& srcLocal)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<float, half, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void GeluCastIntrinsicsImpl(const LocalTensor<half>& dstLocal, const LocalTensor<float>& srcLocal)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<half, float, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluFormulasHighPrecision(const LocalTensor<half>& dstLocal, const LocalTensor<half>& srcLocal,
    GeluParams<float>& params,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const GeluParams<float>&))
{
    GeluFormulasTmpCalc<float, true, bufferNumber>(params);

    const LocalTensor<float>& stackBufferConv = params.tempTensorConv;

    const uint32_t round = params.dataSize / params.stackSize;
    const uint32_t tail = params.dataSize % params.stackSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        GeluCastIntrinsicsImpl(stackBufferConv, srcLocal[offset]);

        func(stackBufferConv, stackBufferConv, params);

        GeluCastIntrinsicsImpl(dstLocal[offset], stackBufferConv);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);

        GeluCastIntrinsicsImpl(stackBufferConv, srcLocal[offset]);

        func(stackBufferConv, stackBufferConv, params);

        GeluCastIntrinsicsImpl(dstLocal[offset], stackBufferConv);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluFormulas(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    GeluParams<T>& params, void (*func)(const LocalTensor<T>&, const LocalTensor<T>&, const GeluParams<T>&))
{
    GeluFormulasTmpCalc<T, false, bufferNumber>(params);

    const uint32_t round = params.dataSize / params.stackSize;
    const uint32_t tail = params.dataSize % params.stackSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        func(dstLocal[offset], srcLocal[offset], params);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);
        func(dstLocal[offset], srcLocal[offset], params);
    }

    SetMaskNorm();
    ResetMask();
}

template <uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluClass(const LocalTensor<half>& dstLocal, const LocalTensor<half>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const GeluParams<float>&))
{
    GeluParams<float> params;
    params.dataSize = dataSize;
    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);

                                                                                                               ;
    GeluFormulasHighPrecision<bufferNumber>(dstLocal, srcLocal, params, func);
}

template <typename T, uint32_t bufferNumber = 1>
[aicore] __inline__ __attribute__((always_inline)) void GeluClass(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize,
    void (*func)(const LocalTensor<T>&, const LocalTensor<T>&, const GeluParams<T>&))
{
    GeluParams<T> params;
    params.dataSize = dataSize;
    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                               ;
    GeluFormulas<T, bufferNumber>(dstLocal, srcLocal, params, func);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr (highPrecision && (sizeof(T) == sizeof(half))) {
        GeluClass<TWO_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            GeluCalcSimplifiedAvoid<float, highPerformance>);
    } else {
        GeluClass<T, TWO_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            GeluCalcSimplifiedAvoid<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void GeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr (highPrecision && (sizeof(T) == sizeof(half))) {
        GeluClass(dstLocal, srcLocal, sharedTmpBuffer, dataSize, FastGeluCalcSimplified<float, highPerformance>);
    } else {
        GeluClass(dstLocal, srcLocal, sharedTmpBuffer, dataSize, FastGeluCalcSimplified<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2Impl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr (highPrecision && (IsSameType<T, half>::value)) {
        GeluClass<THREE_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            FastGeluV2CalcSimplified<float, highPerformance>);
    } else {
        GeluClass<T, THREE_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            FastGeluV2CalcSimplified<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2Impl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
#pragma end_pipe
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void Gelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void Gelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
# 105 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                         ;
    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 126 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] __inline__ __attribute__((always_inline)) void FasterGeluV2(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                         ;
    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
#pragma end_pipe
}
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_v220_impl.h"
namespace AscendC {

[aicore] __inline__ __attribute__((always_inline)) void AscendQuantIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<half>& stackBuffer, half scale, half offset)
{
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams f162s8Params;
    f162s8Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Muls<half, false>(stackBuffer, srcTensor, scale, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackBuffer, stackBuffer, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<half>& stackBuffer, half scale, half offset)
{
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams f162s8Params;
    f162s8Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<half, float, false>(stackBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
    Muls<half, false>(stackBuffer, stackBuffer, scale, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackBuffer, stackBuffer, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const half offset)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<half, float, false>(stackTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
    Mul<half, false>(stackTensor, stackTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackTensor, stackTensor, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const half offset)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Mul<half, false>(stackTensor, srcTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackTensor, stackTensor, static_cast<half>(offset), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const LocalTensor<half>& offsetTensor)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<half, float, false>(stackTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
    Mul<half, false>(stackTensor, stackTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Add<half, false>(stackTensor, stackTensor, offsetTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const LocalTensor<half>& offsetTensor)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Mul<half, false>(stackTensor, srcTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Add<half, false>(stackTensor, stackTensor, offsetTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}


template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LocalTensor<half>& scaleTensor, const LocalTensor<half>& offsetTensor, const uint32_t calCount)
{
    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[i * splitSize],
            offsetTensor[i * splitSize]);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[loopCount * splitSize],
            offsetTensor[loopCount * splitSize]);
    }
}
template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantPerChannelImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LocalTensor<half>& scaleTensor, const half offset, const uint32_t calCount)
{
    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[i * splitSize],
            offset);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[loopCount * splitSize],
            offset);
    }
}


template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }




      ;

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;
    uint32_t loopCount = calCount / splitSize;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantIntrinsicsImpl(dstTensor[splitSize * i], srcTensor[splitSize * i],
            sharedTmpBuffer.ReinterpretCast<half>(), static_cast<half>(scale), static_cast<half>(offset));
    }
    if (calCount % splitSize > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount % splitSize);
        AscendQuantIntrinsicsImpl(dstTensor[splitSize * loopCount], srcTensor[splitSize * loopCount],
            sharedTmpBuffer.ReinterpretCast<half>(),
            static_cast<half>(scale), static_cast<half>(offset));
    }

    SetMaskNorm();
    ResetMask();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    uint32_t N = calCount / scaleCount;
    SetMaskCount();
    if constexpr (IsSameType<T, float>::value) {

        LocalTensor<half> halfScaleTensor = scaleTensor.template ReinterpretCast<half>();
        UnaryRepeatParams f162s8Param;
        f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        SetVectorMask<half, MaskMode::COUNTER>(0, scaleCount);
        Cast<half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < N; ++i) {
            AscendQuantPerChannelImpl(dstTensor[i * scaleCount], srcTensor[i * scaleCount], sharedTmpBuffer,
                halfScaleTensor, static_cast<half>(offset), scaleCount);
        }
    } else {
        for (uint32_t i = 0; i < N; ++i) {
            AscendQuantPerChannelImpl(dstTensor[i * scaleCount], srcTensor[i * scaleCount], sharedTmpBuffer,
                scaleTensor, static_cast<half>(offset), scaleCount);
        }
    }
    SetMaskNorm();
    ResetMask();
}
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    uint32_t N = calCount / scaleCount;
    SetMaskCount();
    if constexpr (IsSameType<T, float>::value) {
        SetVectorMask<half, MaskMode::COUNTER>(0, scaleCount);
        UnaryRepeatParams f162s8Param;
        f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;

        LocalTensor<half> halfScaleTensor = scaleTensor.template ReinterpretCast<half>();
        Cast<half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);

        LocalTensor<half> halfOffsetTensor = offsetTensor.template ReinterpretCast<half>();
        Cast<half, float, false>(halfOffsetTensor, offsetTensor, RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1, f162s8Param);

        for (uint32_t i = 0; i < N; ++i) {
            AscendQuantPerChannelImpl(dstTensor[i * scaleCount], srcTensor[i * scaleCount], sharedTmpBuffer,
                halfScaleTensor, halfOffsetTensor, scaleCount);
        }
    } else {
        for (uint32_t i = 0; i < N; ++i) {
            AscendQuantPerChannelImpl(dstTensor[i * scaleCount], srcTensor[i * scaleCount], sharedTmpBuffer,
                scaleTensor, offsetTensor, scaleCount);
        }
    }
    SetMaskNorm();
    ResetMask();
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h" 2






namespace AscendC {
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const float scale, const float offset, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, stackTensor, scale, offset, calCount);
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, stackTensor, scaleTensor, offset, scaleCount, calCount);
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor, const uint32_t scaleCount,
    const uint32_t offsetCount, const uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, stackTensor, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 121 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/quant/ascend_quant_common_impl.h"
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, calCount);
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const float scale, const float offset, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, scale, offset, calCount);
}
# 90 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset)
{
    AscendQuant<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, srcTensor.GetSize());
}
# 108 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const float scale, const float offset)
{
    AscendQuant<T, isReuseSource>(dstTensor, srcTensor, scale, offset, srcTensor.GetSize());
}
# 133 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset,
        scaleCount, calCount);
}
# 155 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, scaleTensor, offset, scaleCount, calCount);
}
# 178 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor, const T offset)
{
    AscendQuant<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset,
        scaleTensor.GetSize(), srcTensor.GetSize());
}
# 197 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<T>& scaleTensor, const T offset)
{
    AscendQuant<T, isReuseSource>(dstTensor, srcTensor, scaleTensor, offset,
        scaleTensor.GetSize(), srcTensor.GetSize());
}
# 224 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 248 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor, const uint32_t scaleCount,
    const uint32_t offsetCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource>(dstTensor, srcTensor, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 273 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    AscendQuant<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleTensor.GetSize(), offsetTensor.GetSize(), srcTensor.GetSize());
}
# 293 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor)
{
    AscendQuant<T, isReuseSource>(dstTensor, srcTensor, scaleTensor, offsetTensor,
        scaleTensor.GetSize(), offsetTensor.GetSize(), srcTensor.GetSize());
}

#pragma end_pipe
}
# 60 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/dequant/ascend_dequant_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/dequant/ascend_dequant_common_impl.h"
namespace AscendC {
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr uint32_t FLOAT_PER_REPEAT = 64;

struct DequantParams {
    uint32_t m;
    uint32_t n;
    uint32_t calCount;
};

[aicore] __inline__ __attribute__((always_inline)) bool IsCalCountValid(const LocalTensor<int32_t>& srcTensor, uint32_t calCount)
{



                       ;
    return true;
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) bool IsWithoutDequantParamsValid(const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale)
{



                                                                                         ;

    if constexpr(IsSameType<scaleT, uint64_t>::value) {


                                                                    ;
    }
    return true;
}


template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) bool IsDequantParamsValid(const LocalTensor<int32_t>& srcTensor, const LocalTensor<dstT>& dstTensor,
    DequantParams& params)
{


                                                                        ;


                                                                                                                       ;


                                                                                                                       ;

    uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
    uint32_t alignInner = (params.n + oneBlockNum - 1) / oneBlockNum * oneBlockNum;

                                                                                                   ;

    return true;
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) bool IsDeqscaleTensorValid(const LocalTensor<scaleT>& deqScale, DequantParams& params)
{


                                                                                                   ;
    return true;
}


template <typename dstT, typename scaleT, bool isTensor>
[aicore] __inline__ __attribute__((always_inline)) constexpr bool IsTemplateValid()
{
    if constexpr(isTensor) {


        constexpr bool isValid1 = (IsSameType<scaleT, uint64_t>::value) && (IsSameType<dstT, half>::value);
        constexpr bool isValid2 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, float>::value);



        constexpr bool isValid3 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid4 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, bfloat16_t>::value);
        constexpr bool isValid5 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, bfloat16_t>::value);
        return isValid1 || isValid2 || isValid3 || isValid4 || isValid5;

    } else {






        constexpr bool isValid1 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, bfloat16_t>::value);
        constexpr bool isValid2 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid3 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid4 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, bfloat16_t>::value);
        return isValid1 || isValid2 || isValid3 || isValid4;

    }
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantTmpCalc(const LocalTensor<float>& stackBuffer, DequantParams& dqParams,
    AscendDequantParams<float>& params, uint32_t srcSize, uint32_t deqScaleSize)
{
    uint32_t base = dqParams.n;

    deqScaleSize = (deqScaleSize + FLOAT_PER_BLOCK - 1) / FLOAT_PER_BLOCK * FLOAT_PER_BLOCK;

    uint32_t tmpSrcSize = (stackBuffer.GetSize() - deqScaleSize) / base * base;
                                                                                                           ;
    tmpSrcSize = (tmpSrcSize > srcSize) ? srcSize : tmpSrcSize;
    params.tmpSize = tmpSrcSize;
    params.tmpAddrA = stackBuffer;
    params.tmpAddrB = stackBuffer[deqScaleSize];
}


template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantTmpCalc(const LocalTensor<int32_t>& srcTensor, const scaleT deqScale,
    const LocalTensor<float>& stackBuffer, DequantParams& dqParams, AscendDequantParams<float>& params)
{
    uint32_t srcSize = dqParams.m * dqParams.n;
    uint32_t deqScaleSize = (dqParams.calCount + FLOAT_PER_BLOCK - 1) / FLOAT_PER_BLOCK * FLOAT_PER_BLOCK;

    AscendDequantTmpCalc<scaleT>(stackBuffer, dqParams, params, srcSize, deqScaleSize);

    if constexpr(IsSameType<scaleT, float>::value) {
        Duplicate<float>(params.tmpAddrA, deqScale, static_cast<int32_t>(dqParams.calCount));
    } else {
        Duplicate<float>(params.tmpAddrA, ToFloat(deqScale), static_cast<int32_t>(dqParams.calCount));
    }
    PipeBarrier<PIPE_V>();
}



template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) RoundMode GetFP32CastMode()
{



    constexpr RoundMode castMode = IsSameType<dstT, bfloat16_t>::value ? RoundMode::CAST_RINT: RoundMode::CAST_NONE;
    return castMode;

}


template <typename dstT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void UpdateDequantParams(DequantParams& params)
{
    if constexpr(mode == DeQuantMode::DEQUANT_WITH_SINGLE_ROW) {
        constexpr uint32_t ONE_BLK_SIZE = 32;
        uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
        bool isCalCountAlign = (params.calCount % oneBlockNum == 0);
        bool isNDivisible = (params.n % params.calCount == 0);



        if (params.m == 1 && isCalCountAlign && isNDivisible) {
            params.m = params.n / params.calCount;
            params.n = params.calCount;
        }
    }
}



template <typename scaleT>
[aicore] __inline__ __attribute__((always_inline)) void CastDeqscale(const LocalTensor<scaleT>& deqScale, AscendDequantParams<float>& params,
    uint32_t scaleSize)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = IsSameType<scaleT, float>::value ? DEFAULT_REPEAT_STRIDE: HALF_DEFAULT_REPEAT_STRIDE;

    if constexpr(IsSameType<scaleT, float>::value) {
        SetVectorMask<float, MaskMode::COUNTER>(0, scaleSize);
        Adds<float, false>(params.tmpAddrA, deqScale, 0, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    } else if constexpr(IsSameType<scaleT, uint64_t>::value) {

        LocalTensor<float> deqScaleFP32 = deqScale.template ReinterpretCast<float>();


        GatherMaskParams reducev2Params;
        reducev2Params.repeatTimes = 1;
        uint64_t rsvdCnt = 0;
        GatherMask<float>(params.tmpAddrA, deqScaleFP32, 1, true, scaleSize * 2, reducev2Params, rsvdCnt);
        PipeBarrier<PIPE_V>();
        SetMaskCount();
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, scaleSize);
        Cast<float, scaleT, false>(params.tmpAddrA, deqScale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void CastSrc(const LocalTensor<int32_t>& srcTensor, const LocalTensor<float>& dstTensor,
    UnaryRepeatParams& unaryParams, uint64_t counter)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, counter);
    Cast<float, int32_t, false>(dstTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] __inline__ __attribute__((always_inline)) void DequantMul(const LocalTensor<float>& srcTensor, const LocalTensor<float>& deqScaleTensor,
    const LocalTensor<float>& dstTensor, BinaryRepeatParams& binaryParams, DequantParams& dqParams, uint32_t k,
    uint32_t loopCount, uint32_t tail)
{
    if (k == 0) {
        return;
    }


    if (dqParams.n > MAX_REPEAT_TIMES * FLOAT_PER_BLOCK) {
        BinaryRepeatParams binaryParamsDefault;
        SetVectorMask<float, MaskMode::COUNTER>(0, dqParams.calCount);
        for (uint32_t i = 0; i < k; i++) {
            Mul<float, false>(dstTensor[i * dqParams.n], srcTensor[i * dqParams.n], deqScaleTensor, MASK_PLACEHOLDER, 1,
                binaryParamsDefault);
        }
        PipeBarrier<PIPE_V>();
        return;
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, FLOAT_PER_REPEAT * k);
    for (uint32_t i = 0; i < loopCount; i++) {
        Mul<float, false>(dstTensor[i * FLOAT_PER_REPEAT], srcTensor[i * FLOAT_PER_REPEAT],
            deqScaleTensor[i * FLOAT_PER_REPEAT], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();

    if (tail != 0) {
        SetMaskNorm();

        uint32_t kTimes = k / MAX_REPEAT_TIMES;
        uint32_t kRemains = k % MAX_REPEAT_TIMES;
        SetVectorMask<float, MaskMode::NORMAL>(0, ((uint64_t)1 << tail) - 1);

        uint32_t baseIndex = loopCount * FLOAT_PER_REPEAT;
        for (uint32_t i = 0; i < kTimes; i++) {
            uint32_t index = baseIndex + MAX_REPEAT_TIMES * i * dqParams.n;
            Mul<float, false>(dstTensor[index], srcTensor[index], deqScaleTensor[baseIndex], MASK_PLACEHOLDER,
                MAX_REPEAT_TIMES, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        if (kRemains > 0) {
            uint32_t index = baseIndex + MAX_REPEAT_TIMES * kTimes * dqParams.n;
            Mul<float, false>(dstTensor[index], srcTensor[index], deqScaleTensor[baseIndex], MASK_PLACEHOLDER, kRemains,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
}


template <typename dstT>
[aicore] __inline__ __attribute__((always_inline)) void CastDst(const LocalTensor<dstT>& dstTensor, const LocalTensor<float>& srcFP32,
    UnaryRepeatParams& unaryParams, uint32_t srcInner, uint32_t dstInner, uint32_t dataNum)
{
    if constexpr(IsSameType<dstT, float>::value) {
        SetVectorMask<float, MaskMode::COUNTER>(0, dataNum);
        Adds<float, false>(dstTensor, srcFP32, 0, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        return;
    }

    RoundMode castMode = GetFP32CastMode<dstT>();
    if (srcInner == dstInner) {
        SetVectorMask<float, MaskMode::COUNTER>(0, dataNum);
        Cast<dstT, float, false>(dstTensor, srcFP32, castMode, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        uint32_t loopNum = dataNum / srcInner;
        uint32_t tailPart = dataNum % srcInner;
        SetVectorMask<float, MaskMode::COUNTER>(0, srcInner);
        for (uint32_t i = 0; i < loopNum; i++) {
            Cast<dstT, float, false>(dstTensor[i * dstInner], srcFP32[i * srcInner], castMode, MASK_PLACEHOLDER, 1,
                unaryParams);
        }
        PipeBarrier<PIPE_V>();

        if (tailPart > 0) {
            SetVectorMask<float, MaskMode::COUNTER>(0, tailPart);
            Cast<dstT, float, false>(dstTensor[loopNum * dstInner], srcFP32[loopNum * srcInner], castMode,
                MASK_PLACEHOLDER, 1, unaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
}


template <typename dstT, typename scaleT, bool isPureDqParams = false>
[aicore] __inline__ __attribute__((always_inline)) void CalculateByInner(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams& dqParams, AscendDequantParams<float>& ascendDqParams,
    uint32_t calCount)
{
    LocalTensor<float> deqScaleFP32 = ascendDqParams.tmpAddrA;
    LocalTensor<float> srcFP32 = ascendDqParams.tmpAddrB;

    uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
    uint32_t dstInner = (dqParams.n + oneBlockNum - 1) / oneBlockNum * oneBlockNum;
    uint32_t tmpSize = ascendDqParams.tmpSize;
    uint32_t loopCount = calCount / tmpSize;
    uint32_t tailSize = calCount % tmpSize;
    uint32_t k = tmpSize / dqParams.n;

    uint32_t mainBlockLoopCount = dqParams.calCount / FLOAT_PER_REPEAT;
    uint32_t mainBlockTail = dqParams.calCount % FLOAT_PER_REPEAT;
    BinaryRepeatParams binaryParams;
    BinaryRepeatParams binaryParamsMul(1, 1, 1, dqParams.n / FLOAT_PER_BLOCK, dqParams.n / FLOAT_PER_BLOCK, 0);
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams unaryParamsDst;
    if constexpr(!IsSameType<dstT, float>::value) {
        unaryParamsDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    }

    CastDeqscale(deqScale, ascendDqParams, dqParams.calCount);


    uint32_t castDstIndex = (dqParams.n == dstInner) ? tmpSize : k * dstInner;
    for (uint32_t i = 0; i < loopCount; i++) {
        SetMaskCount();
        CastSrc(srcTensor[i * tmpSize], srcFP32, unaryParams, tmpSize);

        DequantMul(srcFP32, deqScaleFP32, srcFP32, binaryParamsMul, dqParams, k, mainBlockLoopCount, mainBlockTail);

        SetMaskCount();
        CastDst<dstT>(dstTensor[i * castDstIndex], srcFP32, unaryParamsDst, dqParams.n, dstInner, tmpSize);
    }


    if (tailSize > 0) {
        CastSrc(srcTensor[calCount - tailSize], srcFP32, unaryParams, tailSize);

        k = tailSize / dqParams.n;
        DequantMul(srcFP32, deqScaleFP32, srcFP32, binaryParamsMul, dqParams, k, mainBlockLoopCount, mainBlockTail);

        if constexpr(!isPureDqParams) {
            uint32_t tailK = tailSize % dqParams.n;
            if (tailK != 0) {
                SetMaskCount();
                SetVectorMask<float, MaskMode::COUNTER>(0, tailK);
                uint32_t idxMul = tailSize - tailK;
                Mul<float, false>(srcFP32[idxMul], srcFP32[idxMul], deqScaleFP32, MASK_PLACEHOLDER, 1, binaryParams);
                PipeBarrier<PIPE_V>();
            }
        }

        SetMaskCount();

        uint32_t index = (dqParams.n == dstInner) ? calCount - tailSize :
            (calCount - tailSize) / dqParams.n * dstInner;
        CastDst<dstT>(dstTensor[index], srcFP32, unaryParamsDst, dqParams.n, dstInner, tailSize);
    }
}



template <typename dstT, typename scaleT, bool isPureDqParams, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantImpl(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams& params,
    uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert(IsTemplateValid<dstT, scaleT, true>(),
        "current combination of deqScale dtype and dstTensor dtype is not supported, please check the document");

    if (!IsDequantParamsValid<dstT>(srcTensor, dstTensor, params) || !IsDeqscaleTensorValid(deqScale, params)) {
        return;
    }

                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    UpdateDequantParams<dstT, mode>(params);

    AscendDequantParams<float> ascendDqParams;
    AscendDequantTmpCalc<scaleT>(stackBuffer, params, ascendDqParams, params.m * params.n, params.calCount);

    SetMaskCount();
    CalculateByInner<dstT, scaleT, isPureDqParams>(dstTensor, srcTensor, deqScale, params, ascendDqParams, calCount);

    SetMaskNorm();
    ResetMask();
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantImpl(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams params)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                ;
    AscendDequantImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        params.m * params.n);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (!IsCalCountValid(srcTensor, calCount) || !IsWithoutDequantParamsValid<scaleT>(srcTensor, deqScale)) {
        return;
    }
    DequantParams params = {srcTensor.GetSize() / deqScale.GetSize(), deqScale.GetSize(), deqScale.GetSize()};
    AscendDequantImpl<dstT, scaleT, false, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params, calCount);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                ;
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, calCount);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantNoCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    if (!IsWithoutDequantParamsValid<scaleT>(srcTensor, deqScale)) {
        return;
    }
    DequantParams params = {srcTensor.GetSize() / deqScale.GetSize(), deqScale.GetSize(), deqScale.GetSize()};
    AscendDequantImpl<dstT, scaleT, false, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        srcTensor.GetSize());
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantNoCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer);
}


template <typename dstT, typename scaleT, bool isPureDqParams, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantScalarImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const scaleT deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer,
    DequantParams& params)
{
    static_assert(IsTemplateValid<dstT, scaleT, false>(),
        "current combination of deqScale dtype and dstTensor dtype is not supported, please check the document");

    if (!IsDequantParamsValid<dstT>(srcTensor, dstTensor, params)) {
        return;
    }

                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    UpdateDequantParams<dstT, mode>(params);

    SetMaskCount();
    AscendDequantParams<float> ascendDqParams;
    AscendDequantTmpCalc<scaleT>(srcTensor, deqScale, stackBuffer, params, ascendDqParams);
    LocalTensor<float> deqScaleFP32 = ascendDqParams.tmpAddrA;

    SetMaskCount();
    CalculateByInner<dstT, float, true>(dstTensor, srcTensor, deqScaleFP32, params, ascendDqParams,
        params.m * params.n);

    SetMaskNorm();
    ResetMask();
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequantScalarImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const scaleT deqScale, DequantParams& params)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AscendDequantScalarImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params);
}

}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams params)
{
    AscendDequantImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        params.m * params.n);
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, params);
}
# 100 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, calCount);
}
# 122 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, calCount);
}
# 152 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer);
}
# 174 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale);
}
# 205 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const scaleT deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantScalarImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params);
}
# 231 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] __inline__ __attribute__((always_inline)) void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const scaleT deqScale, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantScalarImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, params);
}

#pragma end_pipe
}
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_common.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_common.h"
namespace AscendC {
constexpr uint32_t ANTIQUANT_TWO = 2;
constexpr uint32_t ANTIQUANT_FOUR = 4;
constexpr uint32_t ANTIQUANT_BRCB_BASE = 8;
constexpr uint32_t ANTIQUANT_MIN_METHOD2 = 80;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE = 64;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE_BF16 = 64;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE_FP16 = 128;
constexpr uint32_t ANTIQUANT_MAX_K = 255;
constexpr uint32_t MAX_K_FOR_FP16_BRCB = 4096;

struct AntiQuantShapeInfo {
    uint32_t offsetHeight{0};
    uint32_t offsetWidth{0};
    uint32_t scaleHeight{0};
    uint32_t scaleWidth{0};
};

}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_c220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_c220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_c220_impl.h" 2



namespace AscendC {
template <typename InputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<bfloat16_t> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    uint32_t srcFp16Pos = calCount * sizeof(bfloat16_t);
    uint32_t offsetFp32Pos = calCount * sizeof(float);
    auto fp16TmpBuffer = sharedTmpBuffer[srcFp16Pos].ReinterpretCast<half>();
    auto offsetBuffer = sharedTmpBuffer[offsetFp32Pos].ReinterpretCast<float>();
    auto resultBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    UnaryRepeatParams src2f16unaryParams;
    if constexpr(IsSameType<InputDataType, int8_t>::value) {
        src2f16unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        src2f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    src2f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<half, InputDataType, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, src2f16unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Cast<float, bfloat16_t, false>(offsetBuffer, offset, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Cast<float, bfloat16_t, false>(offsetBuffer, scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, f322f16Params);
    PipeBarrier<PIPE_V>();
}

template <typename InputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<bfloat16_t> &dst, const LocalTensor<InputDataType> &src,
    const bfloat16_t offset, const bfloat16_t scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{
    uint32_t srcFp16Pos = calCount * sizeof(bfloat16_t);
    auto fp16TmpBuffer = sharedTmpBuffer[srcFp16Pos].ReinterpretCast<half>();
    auto resultBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    UnaryRepeatParams src2f16unaryParams;
    if constexpr(IsSameType<InputDataType, int8_t>::value) {
        src2f16unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        src2f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParamsScalar;

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<half, InputDataType, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, src2f16unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Adds<float, false>(resultBuffer, resultBuffer, ToFloat(offset), MASK_PLACEHOLDER, 1, unaryParamsScalar);
        PipeBarrier<PIPE_V>();
    }
    Muls<float, false>(resultBuffer, resultBuffer, ToFloat(scale), MASK_PLACEHOLDER, 1, unaryParamsScalar);
    PipeBarrier<PIPE_V>();
    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, f322f16Params);
    PipeBarrier<PIPE_V>();
}

template <typename InputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformance(const LocalTensor<bfloat16_t> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N)
{
    uint32_t posOffsetScale = N * sizeof(float) * ANTIQUANT_TWO;
    uint32_t posCast = posOffsetScale + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(half);
    auto fp16TmpBuffer = sharedTmpBuffer[posCast].ReinterpretCast<half>();
    auto resultBuffer = sharedTmpBuffer[posOffsetScale].ReinterpretCast<float>();

    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = N / ANTIQUANT_TWO / ONE_BLK_SIZE;
    s42f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams s82f16unaryParams;
    s82f16unaryParams.srcRepStride = N * sizeof(int8_t) / ONE_BLK_SIZE;
    s82f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f162f32unaryParams;
    f162f32unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;
    binaryParams.src1RepStride = 0;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = N * sizeof(half) / ONE_BLK_SIZE;

    for (uint32_t i = 0; i < N / ANTIQUANT_SINGLE_N_SIZE_BF16; i++) {
        SetMaskNorm();
        SetVectorMask<half, MaskMode::NORMAL>(ANTIQUANT_SINGLE_N_SIZE_BF16);
        if constexpr (IsSameType<InputDataType, int4b_t>::value) {

            Cast<half, int4b_t, false>(fp16TmpBuffer, src[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, K, s42f16unaryParams);
        } else {

            Cast<half, int8_t, false>(fp16TmpBuffer, src[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, K, s82f16unaryParams);
        }
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE_BF16 * K);
        Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, K, f162f32unaryParams);
        PipeBarrier<PIPE_V>();

        auto offsetBuffer = sharedTmpBuffer[ANTIQUANT_SINGLE_N_SIZE_BF16 * i * sizeof(float)].ReinterpretCast<float>();
        Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        auto scaleBuffer = sharedTmpBuffer[N * sizeof(float) +
            ANTIQUANT_SINGLE_N_SIZE_BF16 * i * sizeof(float)].ReinterpretCast<float>();
        Mul<float, false>(resultBuffer, resultBuffer, scaleBuffer, MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        Cast<bfloat16_t, float, false>(dst[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], resultBuffer,
            RoundMode::CAST_RINT, MASK_PLACEHOLDER, K, f322f16Params);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformanceTail(const LocalTensor<bfloat16_t> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N, const uint32_t mask)
{
    uint32_t index = N / ANTIQUANT_SINGLE_N_SIZE_BF16 * ANTIQUANT_SINGLE_N_SIZE_BF16;
    uint32_t posOffset = N * sizeof(float);
    uint32_t posOffsetScale = posOffset * ANTIQUANT_TWO;
    uint32_t posCast = posOffsetScale + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(half);
    auto fp16TmpBuffer = sharedTmpBuffer[posCast].ReinterpretCast<half>();
    auto resultBuffer = sharedTmpBuffer[posOffsetScale].ReinterpretCast<float>();
    auto offsetBuffer = sharedTmpBuffer[index * sizeof(float)].ReinterpretCast<float>();
    auto scaleBuffer = sharedTmpBuffer[posOffset + index * sizeof(float)].ReinterpretCast<float>();

    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = N / ANTIQUANT_TWO / ONE_BLK_SIZE;
    UnaryRepeatParams s82f16unaryParams;
    s82f16unaryParams.srcRepStride = N * sizeof(int8_t) / ONE_BLK_SIZE;
    s82f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f162f32unaryParams;
    f162f32unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;
    binaryParams.src1RepStride = 0;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = N * sizeof(bfloat16_t) / ONE_BLK_SIZE;


    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(mask);
    if constexpr (IsSameType<InputDataType, int4b_t>::value) {
        Cast<half, int4b_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K, s42f16unaryParams);
    } else {
        Cast<half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K, s82f16unaryParams);
    }
    PipeBarrier<PIPE_V>();


    Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE,
                             MASK_PLACEHOLDER, K, f162f32unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(resultBuffer, resultBuffer, scaleBuffer, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, K, f322f16Params);
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <typename InputDataType>
[aicore] __inline__ __attribute__((always_inline)) void PreCast(const LocalTensor<bfloat16_t> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    uint32_t posOffset = offset.GetSize() * sizeof(float);
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE / sizeof(bfloat16_t);
    uint32_t repeatTimes =
        offset.GetSize() % repeatEle == 0 ? offset.GetSize() / repeatEle : offset.GetSize() / repeatEle + 1;
    auto offsetBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    auto scaleBuffer = sharedTmpBuffer[posOffset].ReinterpretCast<float>();

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, offset.GetSize());
    Cast<float, bfloat16_t, false>(
        offsetBuffer, offset, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, bfloat16_t, false>(
        scaleBuffer, scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) bool AntiQuantCheckPerformanceMode(const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    if constexpr (IsSameType<OutputDataType, bfloat16_t>::value) {
        uint32_t maxTmpBufferSize =
            scale.GetSize() * ANTIQUANT_TWO * sizeof(float) + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(float);
        return sharedTmpBuffer.GetSize() >= maxTmpBufferSize;
    }
    return true;
}



template <typename InputDataType, typename OutputDataType, bool isOffset>
[aicore] __inline__ __attribute__((always_inline)) void CalculationMax(const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &dst,
    AntiquantParams<float> &params, const uint32_t calCount, const uint32_t N, const uint32_t K, const uint32_t NOffset)
{

    uint32_t srcFp16Pos = calCount / ANTIQUANT_TWO;
    auto fp16TmpBuffer = params.tempTensorInput[srcFp16Pos].ReinterpretCast<half>();

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t count = K / ANTIQUANT_SINGLE_N_SIZE;



    BinaryRepeatParams binaryParams(1, 1, 0, count * DEFAULT_REPEAT_STRIDE, count * DEFAULT_REPEAT_STRIDE, 1);

    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);

    Cast<half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(params.tempTensorInput, fp16TmpBuffer, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE * N);
    for (uint32_t i = 0; i < count; i++) {

        uint32_t curOffset = i * ANTIQUANT_SINGLE_N_SIZE;

        if constexpr (isOffset) {
            Add<float, false>(params.tempTensorInput[curOffset], params.tempTensorInput[curOffset],
                params.tempTensorOffset[NOffset], MASK_PLACEHOLDER, N, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<float, false>(params.tempTensorInput[curOffset], params.tempTensorInput[curOffset],
            params.tempTensorScale[NOffset], MASK_PLACEHOLDER, N, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<bfloat16_t, float, false>(dst, params.tempTensorInput, RoundMode::CAST_RINT,
        MASK_PLACEHOLDER, 1, f322f16Params);
    PipeBarrier<PIPE_V>();
}



template <typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void GetAntiquantTensorInfo(const LocalTensor<OutputDataType> &scale,
    const LocalTensor<float> &stackBuffer, AntiquantParams<float> &params)
{
    uint32_t N = scale.GetSize();
    params.tempTensorOffset = stackBuffer[0];
    params.tempTensorScale = stackBuffer[ANTIQUANT_BRCB_BASE * N];
    params.tempTensorInput = stackBuffer[ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO * N];
}



template <typename OutputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void CastAndBrcb(const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    AntiquantParams<float> &params, const uint32_t nLength)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t N = offset.GetSize();


    SetVectorMask<half, MaskMode::COUNTER>(0, nLength);
    if constexpr (withOffset) {
        Cast<float, OutputDataType, false>(params.tempTensorOffset[ANTIQUANT_BRCB_BASE * N - nLength], offset,
            RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    }
    Cast<float, OutputDataType, false>(params.tempTensorScale[ANTIQUANT_BRCB_BASE * N - nLength], scale,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    constexpr uint16_t brcbDstBlkStride = 1;
    constexpr uint16_t brcbDstRepStride = ANTIQUANT_BRCB_BASE;
    const uint8_t repeatTimes = nLength / ANTIQUANT_BRCB_BASE;
    BrcbRepeatParams brcbParams(brcbDstBlkStride, brcbDstRepStride);

    SetMaskNorm();
    ResetMask();

    if constexpr (withOffset) {
        Brcb(params.tempTensorOffset, params.tempTensorOffset[ANTIQUANT_BRCB_BASE * N - nLength],
            repeatTimes, brcbParams);
        PipeBarrier<PIPE_V>();
    }
    Brcb(params.tempTensorScale, params.tempTensorScale[ANTIQUANT_BRCB_BASE * N - nLength], repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
}



template <typename InputDataType, typename OutputDataType, bool withOffset>
[aicore] __inline__ __attribute__((always_inline)) void CalculationMin(const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &dst,
    AntiquantParams<float> &params, const uint32_t calCount, const uint32_t N, const uint32_t srcN, const uint32_t K)
{

    uint32_t srcFp16Pos = calCount / ANTIQUANT_TWO;
    uint32_t n = K / ANTIQUANT_SINGLE_N_SIZE;
    UnaryRepeatParams unaryParamsInt8Fp16;
    unaryParamsInt8Fp16.srcRepStride = ANTIQUANT_TWO * n;

    unaryParamsInt8Fp16.dstRepStride = ANTIQUANT_SINGLE_N_SIZE / (ONE_BLK_SIZE / sizeof(half));
    UnaryRepeatParams unaryParamsFp16Fp32;
    unaryParamsFp16Fp32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;


    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(0, FULL_MASK);

    auto fp16TmpBuffer = params.tempTensorInput[srcFp16Pos].ReinterpretCast<half>();
    Cast<half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, N, unaryParamsInt8Fp16);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(
        params.tempTensorInput, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, N, unaryParamsFp16Fp32);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    BinaryRepeatParams binaryParams;
    binaryParams.src1BlkStride = 0;
    binaryParams.src1RepStride = 1;

    SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE * N);

    if constexpr (withOffset) {
        Add<float, false>(
            params.tempTensorInput, params.tempTensorInput, params.tempTensorOffset, MASK_PLACEHOLDER, N, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<float, false>(
        params.tempTensorInput, params.tempTensorInput, params.tempTensorScale, MASK_PLACEHOLDER, N, binaryParams);
    PipeBarrier<PIPE_V>();


    SetMaskNorm();
    SetVectorMask<float, MaskMode::NORMAL>(0, FULL_MASK);
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = ANTIQUANT_SINGLE_N_SIZE * n / (ONE_BLK_SIZE / sizeof(half));
    Cast<OutputDataType, float, false>(
        dst, params.tempTensorInput, RoundMode::CAST_RINT, MASK_PLACEHOLDER, srcN, f322f16Params);
    PipeBarrier<PIPE_V>();
}


template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void CalculateByBrcbMin(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<float> &stackBuffer, const uint32_t calCount, const uint32_t N, const uint32_t K)
{
    AntiquantParams<float> antiquantParams;
    GetAntiquantTensorInfo<OutputDataType>(scale, stackBuffer, antiquantParams);

    SetMaskCount();
    CastAndBrcb<OutputDataType, true>(offset, scale, antiquantParams, N);

    uint32_t curNKOffset = 0;
    uint32_t loopNum = K / ANTIQUANT_SINGLE_N_SIZE;
    uint32_t srcN = src.GetSize() / K;

    for (uint32_t i = 0; i < loopNum; i++) {
        curNKOffset = ANTIQUANT_SINGLE_N_SIZE * i;
        CalculationMin<InputDataType, OutputDataType, true>(src[curNKOffset], dst[curNKOffset], antiquantParams,
            ANTIQUANT_SINGLE_N_SIZE * N, N, srcN, K);
    }
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void CalculateByBrcbMin(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<float> &stackBuffer,
    const uint32_t calCount, const uint32_t N, const uint32_t K)
{
    AntiquantParams<float> antiquantParams;
    GetAntiquantTensorInfo<OutputDataType>(scale, stackBuffer, antiquantParams);

    SetMaskCount();
    CastAndBrcb<OutputDataType, false>(scale, scale, antiquantParams, N);

    uint32_t curNKOffset = 0;
    uint32_t loopNum = K / ANTIQUANT_SINGLE_N_SIZE;
    uint32_t srcN = src.GetSize() / K;

    for (uint32_t i = 0; i < loopNum; i++) {
        curNKOffset = ANTIQUANT_SINGLE_N_SIZE * i;
        CalculationMin<InputDataType, OutputDataType, false>(src[curNKOffset], dst[curNKOffset], antiquantParams,
            ANTIQUANT_SINGLE_N_SIZE * N, N, srcN, K);
    }
}

template <typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void CalculateByBrcbMin(const LocalTensor<OutputDataType> &dst, const LocalTensor<int4b_t> &src,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<float> &stackBuffer,
    const uint32_t calCount, const uint32_t N, const uint32_t K)
{
                                                                                                   ;
}

template <typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void CalculateByBrcbMin(const LocalTensor<OutputDataType> &dst, const LocalTensor<int4b_t> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<float> &stackBuffer, const uint32_t calCount, const uint32_t N, const uint32_t K)
{
                                                                                                   ;
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantFp16Brcb(const LocalTensor<half> &scale, const LocalTensor<half> &offset,
    AntiquantParams<half> &params, const uint32_t scaleN)
{

    const uint8_t repeatTimes = scaleN / BRCB_BROADCAST_NUMBER;
    BrcbRepeatParams brcbParams(DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
    Brcb(params.tempTensorScale, scale, repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Brcb(params.tempTensorOffset, offset, repeatTimes, brcbParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantTranspose(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &offset,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t calCount = src.GetSize();
    uint32_t N = offset.GetSize();
    if constexpr (IsSameType<OutputDataType, half>::value || IsSameType<InputDataType, int4b_t>::value) {
        return AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }
    if (K > ANTIQUANT_MAX_K * ANTIQUANT_BRCB_BASE || (K % ANTIQUANT_SINGLE_N_SIZE != 0)) {
        return AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }

    auto stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t stackBufferSize = N * ANTIQUANT_SINGLE_N_SIZE + N * ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO;
    stackBuffer.SetSize(stackBufferSize);
    CalculateByBrcbMin(dst, src, offset, scale, stackBuffer, calCount, N, K);
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantTranspose(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t calCount = src.GetSize();
    uint32_t N = scale.GetSize();
    if constexpr (IsSameType<OutputDataType, half>::value || IsSameType<InputDataType, int4b_t>::value) {
        return AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }
    if (K > ANTIQUANT_MAX_K * ANTIQUANT_BRCB_BASE || (K % ANTIQUANT_SINGLE_N_SIZE != 0) ||
        IsSameType<InputDataType, int4b_t>::value) {
        return AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }

    auto stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackBufferSize = N * ANTIQUANT_SINGLE_N_SIZE + N * ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO;
    stackBuffer.SetSize(stackBufferSize);
    CalculateByBrcbMin(dst, src, scale, stackBuffer, calCount, N, K);
}

}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/../../impl/quantization/antiquant/ascend_antiquant_impl.h" 2




namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoopF16(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<half> &offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{
    BinaryRepeatParams binaryParams;
    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    Add<half, false>(dst, offset, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<half, false>(dst, scale, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename InputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<half> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<half> &offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{
    UnaryRepeatParams unaryParams;
    if constexpr(IsSameType<InputDataType, int8_t>::value) {
        unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    BinaryRepeatParams binaryParams;

    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    Cast<half, InputDataType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Add<half, false>(dst, offset, dst, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<half, false>(dst, scale, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename InputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantInnerLoop(const LocalTensor<half> &dst, const LocalTensor<InputDataType> &src,
    const half offset, const half scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    UnaryRepeatParams unaryParams;
    if constexpr(IsSameType<InputDataType, int8_t>::value) {
        unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    UnaryRepeatParams unaryParamsScalar;

    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    Cast<half, InputDataType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Adds<half, false>(dst, dst, offset, MASK_PLACEHOLDER, 1, unaryParamsScalar);
        PipeBarrier<PIPE_V>();
    }
    Muls<half, false>(dst, dst, scale, MASK_PLACEHOLDER, 1, unaryParamsScalar);
    PipeBarrier<PIPE_V>();
}

template <typename InputDataType, typename OutputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantOuterLoop(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr (IsSameType<OutputDataType, half>::value) {
        AntiQuantInnerLoop<InputDataType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
    } else {
        uint32_t tmpSize = sharedTmpBuffer.GetSize() / sizeof(OutputDataType) / ANTIQUANT_FOUR;
        uint32_t loopCount = calCount / tmpSize;
        uint32_t tailSize = calCount % tmpSize;

        for (uint32_t i = 0; i < loopCount; i++) {
            AntiQuantInnerLoop<InputDataType, withOffset>(
                dst[i * tmpSize], src[i * tmpSize], offset, scale, sharedTmpBuffer, tmpSize);
        }
        if (tailSize > 0) {
            AntiQuantInnerLoop<InputDataType, withOffset>(
                dst[loopCount * tmpSize], src[loopCount * tmpSize], offset, scale, sharedTmpBuffer, tailSize);
        }
    }
}

template <typename InputDataType, typename OutputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantOuterLoop(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{
    if constexpr (IsSameType<OutputDataType, half>::value) {
        AntiQuantInnerLoop<InputDataType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
    } else {
        uint32_t tmpSize = sharedTmpBuffer.GetSize() / sizeof(OutputDataType) / ANTIQUANT_FOUR;
        uint32_t loopCount = calCount / tmpSize;
        uint32_t tailSize = calCount % tmpSize;

        for (uint32_t i = 0; i < loopCount; i++) {
            AntiQuantInnerLoop<InputDataType, withOffset>(
                dst[i * tmpSize], src[i * tmpSize], offset, scale, sharedTmpBuffer, tmpSize);
        }
        if (tailSize > 0) {
            AntiQuantInnerLoop<InputDataType, withOffset>(
                dst[loopCount * tmpSize], src[loopCount * tmpSize], offset, scale, sharedTmpBuffer, tailSize);
        }
    }
}

template <typename InputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformance(const LocalTensor<half> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<half> &offset, const LocalTensor<half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N)
{
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = N * sizeof(half) / ONE_BLK_SIZE;
    binaryParams.src1RepStride = 0;
    binaryParams.dstRepStride = N * sizeof(half) / ONE_BLK_SIZE;
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE;
    uint32_t repeatTimes = src.GetSize() % repeatEle == 0 ? src.GetSize() / repeatEle : src.GetSize() / repeatEle + 1;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE_FP16 * K);
    uint32_t loopN = N / ANTIQUANT_SINGLE_N_SIZE_FP16;
    for (uint32_t i = 0; i < loopN; i++) {
        uint32_t loopOffset = ANTIQUANT_SINGLE_N_SIZE_FP16 * i;

        Add<half, false>(dst[loopOffset], dst[loopOffset], offset[loopOffset], MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        Mul<half, false>(dst[loopOffset], dst[loopOffset], scale[loopOffset], MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTransposePerformanceTail(const LocalTensor<half> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<half> &offset, const LocalTensor<half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N, const uint32_t mask)
{
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = N * sizeof(half) / ONE_BLK_SIZE;
    binaryParams.src1RepStride = 0;
    binaryParams.dstRepStride = N * sizeof(half) / ONE_BLK_SIZE;


    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(mask);

    Add<half, false>(dst, dst, offset, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<half, false>(dst, dst, scale, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <typename InputDataType>
[aicore] __inline__ __attribute__((always_inline)) void PreCast(const LocalTensor<half> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<half> &offset, const LocalTensor<half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE;
    uint32_t repeatTimes = src.GetSize() % repeatEle == 0 ? src.GetSize() / repeatEle : src.GetSize() / repeatEle + 1;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, src.GetSize());
    if constexpr (IsSameType<InputDataType, int4b_t>::value) {
        Cast<half, int4b_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, s42f16unaryParams);
    } else {
        Cast<half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantNoTransposeImplScalar(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &offset,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const uint32_t N, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t groupCount = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t groupSize = K / groupCount;
    SetMaskCount();
    if constexpr (IsSameType<OutputDataType, half>::value && IsSameType<InputDataType, int8_t>::value) {
        SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
        UnaryRepeatParams unaryParams;
        unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

        Cast<half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();

        for (uint32_t j = 0; j < groupSize; j++) {
            AntiQuantInnerLoopF16(dst[j * N], dst[j * N], offset, scale, sharedTmpBuffer, N);
        }
        return;
    }
    for (uint32_t j = 0; j < groupSize; j++) {
        AntiQuantOuterLoop<InputDataType, OutputDataType, true>(dst[j * N], src[j * N], offset, scale, sharedTmpBuffer,
            N);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTranspose(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &offset,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    bool isPerformance = AntiQuantCheckPerformanceMode(scale, sharedTmpBuffer, K);
    if (isPerformance) {

        PreCast(dst, src, offset, scale, sharedTmpBuffer, K);
        uint32_t kTail = K % ANTIQUANT_MAX_K, loopK = K / ANTIQUANT_MAX_K, nTail;
        if constexpr (IsSameType<OutputDataType, half>::value) {
            nTail = N % ANTIQUANT_SINGLE_N_SIZE_FP16;
        } else {
            nTail = N % ANTIQUANT_SINGLE_N_SIZE_BF16;
        }
        uint32_t NAlign = N - nTail;
        for (int i = 0; i < K / ANTIQUANT_MAX_K; i++) {
            uint32_t offsetSrc = i * ANTIQUANT_MAX_K * N;
            AscendAntiQuantNoTransposePerformance(dst[offsetSrc], src[offsetSrc], offset, scale, sharedTmpBuffer,
                ANTIQUANT_MAX_K, N);
            if (nTail > 0) {
                AscendAntiQuantNoTransposePerformanceTail(dst[offsetSrc + NAlign], src[offsetSrc + NAlign],
                    offset[NAlign], scale[NAlign], sharedTmpBuffer, ANTIQUANT_MAX_K, N, nTail);
            }
        }
        if (kTail > 0) {
            uint32_t offsetSrc = K / ANTIQUANT_MAX_K * ANTIQUANT_MAX_K * N;
            AscendAntiQuantNoTransposePerformance(dst[offsetSrc], src[offsetSrc], offset, scale, sharedTmpBuffer,
                kTail, N);
            if (nTail > 0) {
                AscendAntiQuantNoTransposePerformanceTail(dst[offsetSrc + NAlign], src[offsetSrc + NAlign],
                    offset[NAlign], scale[NAlign], sharedTmpBuffer, kTail, N, nTail);
            }
        }
        return;
    }
    AntiQuantNoTransposeImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, N, shapeInfo);
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTranspose(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t groupCount = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t groupSize = K / groupCount;
    uint32_t N = (shapeInfo.scaleWidth == 0 ? scale.GetShapeInfo().shape[1] : shapeInfo.scaleWidth);

    SetMaskCount();
    for (uint32_t i = 0; i < groupCount; i++) {
        for (uint32_t j = 0; j < groupSize; j++) {

            AntiQuantOuterLoop<InputDataType, OutputDataType, false>(dst[(i * groupSize + j) * N],
                src[(i * groupSize + j) * N], scale, scale[i * N], sharedTmpBuffer, N);
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType, typename OutputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantNoTranspose(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const OutputDataType offset, const OutputDataType scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo)
{
    SetMaskCount();
    AntiQuantOuterLoop<InputDataType, OutputDataType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantImplScalar(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &offset,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    uint32_t groupSize = K / (shapeInfo.offsetWidth == 0 ? offset.GetShapeInfo().shape[1] : shapeInfo.offsetWidth);
    uint32_t offsetLength = K / groupSize;
    SetMaskCount();
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < offsetLength; j++) {
            auto offsetValue = offset.GetValue(i * offsetLength + j);
            auto scaleValue = scale.GetValue(i * offsetLength + j);
            AntiQuantOuterLoop<InputDataType, OutputDataType, true>(dst[i * K + j * groupSize],
                src[i * K + j * groupSize], offsetValue, scaleValue, sharedTmpBuffer, groupSize);
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType, typename OutputDataType>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantImplScalar(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    uint32_t groupSize = K / (shapeInfo.scaleWidth == 0 ? scale.GetShapeInfo().shape[1] : shapeInfo.scaleWidth);
    uint32_t scaleLength = K / groupSize;

    SetMaskCount();
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < scaleLength; j++) {
            auto scaleValue = scale.GetValue(i * scaleLength + j);
            AntiQuantOuterLoop<InputDataType, OutputDataType, false>(dst[i * K + j * groupSize],
                src[i * K + j * groupSize], scaleValue, scaleValue, sharedTmpBuffer, groupSize);
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename InputDataType, typename OutputDataType, bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantImplScalar(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const OutputDataType offset, const OutputDataType scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo)
{
    SetMaskCount();
    AntiQuantOuterLoop<InputDataType, OutputDataType, withOffset>(dst, src, offset, scale, sharedTmpBuffer,
        calCount);
    SetMaskNorm();
    ResetMask();
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantFp16TransposeMainImpl(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<half> &scale, const LocalTensor<half> &offset, const uint32_t srcN, const uint32_t K)
{
    SetMaskCount();



    uint32_t repStride = K * sizeof(half) / ONE_BLK_SIZE;
    BinaryRepeatParams binaryParams(1, 1, 0, repStride, repStride, 1);
    SetVectorMask<half, MaskMode::COUNTER>(0, srcN * B16_DATA_NUM_PER_REPEAT);
    const uint32_t loop = K / B16_DATA_NUM_PER_REPEAT;
    for (uint32_t i = 0; i < loop; ++i) {
        const uint32_t tmpOffset = i * B16_DATA_NUM_PER_REPEAT;
        if constexpr (withOffset) {
            Add<half, false>(dst[tmpOffset], src[tmpOffset], offset, MASK_PLACEHOLDER, srcN, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<half, false>(dst[tmpOffset], dst[tmpOffset], scale, MASK_PLACEHOLDER, srcN, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AntiQuantFp16TransposeTailImpl(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<half> &scale, const LocalTensor<half> &offset, const uint32_t srcN, const uint32_t K)
{
    SetMaskNorm();
    const uint32_t tailK = K % B16_DATA_NUM_PER_REPEAT;
    SetVectorMask<half, MaskMode::NORMAL>(tailK);



    const uint32_t repStride = K * sizeof(half) / ONE_BLK_SIZE;
    BinaryRepeatParams binaryParams(1, 1, 0, repStride, repStride, 1);
    const uint32_t loop = srcN / MAX_REPEAT_TIMES;
    for (uint32_t i = 0; i < loop; ++i) {
        const uint32_t srcOffset = MAX_REPEAT_TIMES * K * i;
        const uint32_t scaleOffset = MAX_REPEAT_TIMES * B16_DATA_NUM_PER_BLOCK * i;
        if constexpr (withOffset) {
            Add<half, false>(dst[srcOffset], dst[srcOffset], offset[scaleOffset], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<half, false>(dst[srcOffset], dst[srcOffset], scale[scaleOffset], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
            binaryParams);
        PipeBarrier<PIPE_V>();
    }
    const uint32_t tailN = srcN % MAX_REPEAT_TIMES;
    if (tailN != 0) {
        const uint32_t srcOffset = loop * MAX_REPEAT_TIMES * K;
        const uint32_t scaleOffset = loop * MAX_REPEAT_TIMES * B16_DATA_NUM_PER_BLOCK;
        if constexpr (withOffset) {
            Add<half, false>(
                dst[srcOffset], dst[srcOffset], offset[scaleOffset], MASK_PLACEHOLDER, tailN, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<half, false>(dst[srcOffset], dst[srcOffset], scale[scaleOffset], MASK_PLACEHOLDER, tailN, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool withOffset = true>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantFp16Transpose(const LocalTensor<half> &dst, const LocalTensor<int8_t> &src,
    LocalTensor<half> offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t scaleN = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t scaleBrcbSize = ONE_BLK_SIZE / sizeof(half) * scaleN;
    uint32_t stackBufferSize = sharedTmpBuffer.GetSize() / sizeof(half);
    constexpr uint32_t tmpBufferCoeff = withOffset ? ANTIQUANT_TWO : 1;
    if (stackBufferSize < scaleBrcbSize * tmpBufferCoeff || K >= MAX_K_FOR_FP16_BRCB) {
        return withOffset ? AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo) :
            AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, src.GetSize());
    UnaryRepeatParams unaryParams(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    Cast<half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LocalTensor<half> stackBuffer = sharedTmpBuffer.ReinterpretCast<half>();
    AntiquantParams<half> params;
    params.tempTensorScale = stackBuffer[0];
    if constexpr (withOffset) {
        params.tempTensorOffset = stackBuffer[B16_DATA_NUM_PER_BLOCK * scaleN];
    }
    AntiQuantFp16Brcb<withOffset>(scale, offset, params, scaleN);
    uint32_t srcN = src.GetSize() / K;
    if (K < B16_DATA_NUM_PER_REPEAT) {
        return AntiQuantFp16TransposeTailImpl<withOffset>(
            dst, dst, params.tempTensorScale, params.tempTensorOffset, srcN, K);
    }

    AntiQuantFp16TransposeMainImpl<withOffset>(dst, dst, params.tempTensorScale, params.tempTensorOffset, srcN, K);
    const uint32_t tailK = K % B16_DATA_NUM_PER_REPEAT;
    if (tailK != 0) {
        const uint32_t srcOffset = K - tailK;
        AntiQuantFp16TransposeTailImpl<withOffset>(dst[srcOffset], dst[srcOffset], params.tempTensorScale,
            params.tempTensorOffset, srcN, K);
    }
}


template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &offset,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t calCount = src.GetSize();
    if constexpr (!isTranspose) {
        return AscendAntiQuantNoTranspose(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    } else if constexpr (IsSameType<OutputDataType, half>::value && IsSameType<InputDataType, int8_t>::value) {
        return AscendAntiQuantFp16Transpose<true>(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }
    AscendAntiQuantTranspose(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}

template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t calCount = src.GetSize();
    if constexpr (!isTranspose) {
        return AscendAntiQuantNoTranspose(dst, src, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    } else if constexpr (IsSameType<OutputDataType, half>::value && IsSameType<InputDataType, int8_t>::value) {
        return AscendAntiQuantFp16Transpose<false>(dst, src, scale, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }
    AscendAntiQuantTranspose(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
}

template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const LocalTensor<OutputDataType> &offset,
    const LocalTensor<OutputDataType> &scale, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K);
}

template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const OutputDataType offset, const OutputDataType scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose<InputDataType, OutputDataType, true>(dst, src, offset, scale, sharedTmpBuffer,
            src.GetSize(), K, shapeInfo);
        return;
    }
    AntiQuantImplScalar<InputDataType, OutputDataType, true>(
        dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
}

template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose<InputDataType, OutputDataType, false>(dst, src, scale, scale, sharedTmpBuffer,
            src.GetSize(), K, shapeInfo);
        return;
    }
    AntiQuantImplScalar<InputDataType, OutputDataType, false>(
        dst, src, scale, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
}

template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuantImpl(const LocalTensor<OutputDataType> &dst,
    const LocalTensor<InputDataType> &src, const OutputDataType offset, const OutputDataType scale, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(
        dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
}
# 75 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, K, shapeInfo);
}
# 97 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(
        dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
}
# 140 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] __inline__ __attribute__((always_inline)) void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, K, shapeInfo);
}
#pragma end_pipe
}
# 62 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/logsoftmax_base_impl.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/logsoftmax_base_impl.h"
namespace AscendC {
constexpr float SCALAR_NATURE_LOG_10 = 0.4342944819;
 [aicore] __inline__ __attribute__((always_inline)) void GenericLogNZImpl(LocalTensor<float>& dst, const LocalTensor<float>& src,
     const uint32_t originalSrcM, const uint32_t srcK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Ln<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK], MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK],
                static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetMaskCount();
        SetVectorMask<float>(0, srcK);
        for (int j = 0; j < originalSrcM; j++) {
            Ln<float, false>(dst[j * srcK], src[j * srcK], MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * srcK], src[j * srcK], static_cast<float>(SCALAR_NATURE_LOG_10),
                MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZImpl(LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    const UnaryRepeatParams unaryParams;
    const uint64_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint64_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    GenericLogNZImpl(dst, dst, reduceParam.originalSrcM, tiling.srcK);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZReduceMaxImpl(const LocalTensor<float>& tmpBuffer0,
    const LocalTensor<float>& tmpBuffer1, const LocalTensor<half>& maxTensor, const uint32_t& offset2,
    const uint32_t& splitCount, uint64_t mask[2], const ReduceLastND& reduceParam)
{
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZSubImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2],
    const uint32_t& lastBlockMaskLen, const uint32_t& splitCount)
{
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZReduceSumImpl(const LocalTensor<float>& tmpBuffer0,
    const LocalTensor<float>& tmpBuffer1, const LocalTensor<half>& sumTensor, const uint32_t& offset2,
    const uint32_t& splitCount, uint64_t mask[2], const ReduceLastND& reduceParam)
{
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZDivImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2], const uint32_t& lastBlockMaskLen,
    const uint32_t& splitCount)
{
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Div<float>);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZLogImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<half>& dst, const uint32_t& splitOffset,
    const uint32_t& splitCount, const SoftMaxTiling& tiling, const uint32_t& offset1)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Ln<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Muls<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j],
            static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZExpImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2], const uint32_t& lastBlockMaskLen,
    const uint32_t& splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint64_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint64_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint64_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    LogSoftMaxGenericNZReduceMaxImpl(tmpBuffer0, tmpBuffer1, maxTensor, offset2, splitCount, mask, reduceParam);

    LogSoftMaxGenericNZSubImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZExpImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZReduceSumImpl(tmpBuffer0, tmpBuffer1, sumTensor, offset2, splitCount, mask, reduceParam);

    LogSoftMaxGenericNZDivImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZLogImpl(splitNZBlockCount, tmpBuffer0, dst, splitCount, splitOffset, tiling, offset1);
}

[aicore] __inline__ __attribute__((always_inline)) bool LogSoftMaxTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    LogSoftMaxTiling& softmaxTiling, const uint32_t dataTypeSize1, const uint32_t dataTypeSize2,
    bool isDataFormatNZ = false)
{

                                                                                                 ;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;
    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;
    if (isDataFormatNZ) {
        softmaxTiling.reduceM = workLocalSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT + ndinfo.k);
    } else {
        softmaxTiling.reduceM = CalculateNDSplitM(workLocalSize, dataTypeSize1, elementNumPerBlk, ndinfo);
    }

    if (softmaxTiling.reduceM < ndinfo.m && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;
    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                              ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] __inline__ __attribute__((always_inline)) void GenericLogNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t srcK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Ln<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK], MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK],
                static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetMaskCount();
        SetVectorMask<float>(0, srcK);
        for (int j = 0; j < originalSrcM; j++) {
            Ln<float, false>(dst[j * srcK], src[j * srcK], MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * srcK], src[j * srcK], static_cast<float>(SCALAR_NATURE_LOG_10), 1,
                MASK_PLACEHOLDER, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const LogSoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const UnaryRepeatParams unaryParams;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(dst[offset1], dst[offset1], sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(dst[offset1], dst[offset1], reduceParam.originalSrcM, tiling.srcK);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LogSoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitSize + tiling.reduceSize];
    const UnaryRepeatParams unaryParams;
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);

    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);
    PipeBarrier<PIPE_V>();

    Cast(sumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(tmpBuffer0, tmpBuffer0, reduceParam.originalSrcM, tiling.srcK);

    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LogSoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const UnaryRepeatParams unaryParams;

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(tmpBuffer0, tmpBuffer0, reduceParam.originalSrcM, tiling.srcK);

    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNZImpl(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2,
            splitSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNZImpl(LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const LogSoftMaxTiling& tiling)
{
    PipeBarrier<PIPE_V>();
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        LogSoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxNDImpl(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const LogSoftMaxTiling& tiling)
{
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        LogSoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMaxImpl(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LogSoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    SetMaskNorm();
    ResetMask();
    LocalTensor<float> tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tempBuffer.SetSize(sharedTmpBuffer.GetSize() / B32_BYTE_SIZE);
    ShapeInfo srcShape = src.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);




      ;
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    if constexpr (isDataFormatNZ) {
        if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
            SoftMaxTiling newTiling;
            SoftMaxTilingFunc(tempBuffer.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T), sizeof(T), isDataFormatNZ);
            LogSoftMaxNZImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape, newTiling);
        }
    } else {

        if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
            LogSoftMaxTiling newTiling = tiling;
            LogSoftMaxTilingFunc(tempBuffer.GetSize(), srcNDinfo, newTiling, sizeof(T), sizeof(T), isDataFormatNZ);
            LogSoftMaxNDImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape,
                newTiling);
        } else {
            LogSoftMaxNDImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape,
                tiling);
        }
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h" 2
#pragma begin_pipe(V)

namespace AscendC {
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/logsoftmax.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] __inline__ __attribute__((always_inline)) void LogSoftMax(LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LogSoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                    ;
    LogSoftMaxImpl<T, isReuseSource, isDataFormatNZ>(dst, sumTensor, maxTensor, src, sharedTmpBuffer,
        tiling, softmaxShapeInfo);
                                   ;
}
}

#pragma end_pipe
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/softmax/softmax_flash_base_impl.h"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashBasicBlock(const LocalTensor<T> &dst, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &src, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / B16_BYTE_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();

        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_BLK_NUM, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_BLK_NUM, offset, offset });
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * i], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1[halfSplitSize], tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        Brcb(tmpBuffer1[halfSplitSize + DEFAULT_BLK_NUM], tmpBuffer3, splitCeilM,
            { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i],
                tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM),
                { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }
        PipeBarrier<PIPE_V>();

        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * i], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();

        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 2 });
        }
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Cast<float, half, false>(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Cast<half, float, false>(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer1[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });

        Cast<float, half, false>(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_BLK_NUM, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        Cast<half, float, false>(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Mul<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashBasicBlockFloat(const LocalTensor<float> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<float> &src, const LocalTensor<float> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer1 = workLocal[0];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &inSumTmp = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfRepeatNum = DEFAULT_REPEAT_STRIDE / HALF_FACTOR;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        __attribute__((cce_unif_buff)) float *tmpBufferAddr0 = (__attribute__((cce_unif_buff)) float *)src[offset1].GetPhyAddr();
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            Copy<float, false>(tmpBuffer1, src[offset1], MASK_PLACEHOLDER, repeatTimes,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        } else {
            Max<float, false>(tmpBuffer1, src[offset1], src[offset1 + FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
            for (uint32_t j = 2; j < splitBlock; ++j) {
                PipeBarrier<PIPE_V>();
                Max<float, false>(tmpBuffer1, tmpBuffer1, src[offset1 + FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                    (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
            }
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer2[halfSplitSize], tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                tmpBuffer2[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(src[offset1], src[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            Copy<float, false>(tmpBuffer1, src[offset1], MASK_PLACEHOLDER, repeatTimes,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        } else {
            Add<float, false>(tmpBuffer1, src[offset1], src[offset1 + FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
            for (uint32_t j = 2; j < splitBlock; ++j) {
                PipeBarrier<PIPE_V>();
                Add<float, false>(tmpBuffer1, tmpBuffer1, src[offset1 + FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                    (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
            }
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer2[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Copy<float, false>(maxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer2[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Copy<float, false>(inSumTmp, inSumTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();


        Copy<float, false>(expMaxTensor[offset2], inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Copy<float, false>(sumTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();

        ResetMask();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Mul<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashNDImpl(const LocalTensor<T> &dst, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &src, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LastAxisShapeND &originalSrcShape,
    const SoftMaxTiling &tiling)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();

    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp = workLocal[0];

    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    BroadCastLastND brcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;


    if constexpr (isBasicBlock) {
        SoftmaxFlashBasicBlock<T>(dst, sumTensor, maxTensor, src, expMaxTensor, inSumTensor, inMaxTensor, workLocal,
            tiling);
    } else

    {
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset2 = i * tiling.reduceSize;
            offset1 = i * tiling.splitSize;
            Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceParam);
            PipeBarrier<PIPE_V>();

            Cast(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

            Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

            Cast(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Cast(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            Cast(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);

            Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.splitSize);
        }
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceParam);
        PipeBarrier<PIPE_V>();

        Cast(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        Cast(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Cast(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        Cast(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();
        Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.tailSplitSize);
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashNDImpl(const LocalTensor<float> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<float> &src, const LocalTensor<float> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp = workLocal[0];

    const ReduceLastND reduceMainParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                                           tiling.splitK, tiling.reduceM, tiling.reduceK };
    const ReduceLastND reduceTailParam = { tiling.tailM, originalSrcShape.k, tiling.tailM,
                                           tiling.splitK, tiling.tailM, tiling.reduceK };
    const BroadCastLastND mainBrcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    const BroadCastLastND tailBrcParam = { tiling.tailM, tiling.splitK, tiling.tailM, tiling.reduceK };

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, src[offset1], reduceSumBuffer, reduceMainParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, mainBrcParam);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer1, src[offset1], tmpBuffer1, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceMainParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(maxTensor[offset2], tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

        DataCopy(inSumTmp, inSumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Add(sumTensor[offset2], inMaxTmp, reduceSumBuffer, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Div(expMaxTensor[offset2], inMaxTmp, sumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(sumTensor[offset2], sumTensor[offset2], tiling.reduceSize);

        Div(tmpBuffer4, tmpBuffer4, sumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, mainBrcParam);
        PipeBarrier<PIPE_V>();
        Mul(dst[offset1], tmpBuffer1, tmpBuffer0, tiling.splitSize);
    }

    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, src[offset1], reduceSumBuffer, reduceTailParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, tailBrcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, src[offset1], tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceTailParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(maxTensor[offset2], tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        DataCopy(inSumTmp, inSumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(sumTensor[offset2], inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(expMaxTensor[offset2], inMaxTmp, sumTensor[offset2], tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, sumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, tailBrcParam);
        PipeBarrier<PIPE_V>();
        Mul(dst[offset1], tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashPostProcess(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling, bool isUpdate = false,
    const SoftMaxShapeInfo &softmaxShapeInfo = {})
{
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t workLocalSize = workLocal.GetSize();
    if constexpr (sizeof(T) == sizeof(half)) {
        if (!isUpdate) {
            SoftMaxNDImpl<T, T>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {
            SoftmaxFlashNDImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, originalSrcShape, tiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<T, T>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {

            if constexpr (isBasicBlock) {
                SoftmaxFlashBasicBlockFloat(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, workLocal, tiling);
            } else

            {
                SoftmaxFlashNDImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
                    workLocal, originalSrcShape, tiling);
            }
        }
    }
}

[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashBasicBlock(const LocalTensor<half> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &src, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfRepeatNum = DEFAULT_REPEAT_STRIDE / B16_BYTE_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / B16_BYTE_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, halfRepeatNum });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t j = 2; j < splitBlock; ++j) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1[halfSplitSize], tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t j = 2; j < splitBlock; ++j) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Copy<float, false>(maxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer1[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Copy<float, false>(inSumTmp, inSumTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();


        Copy<float, false>(tmpBuffer1, inMaxTmp, MASK_PLACEHOLDER, B16_BYTE_SIZE, { B16_BYTE_SIZE, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize * B16_BYTE_SIZE);
        Cast<half, float, false>(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer1, FLOAT2HALF_ROUND_MODE,
            MASK_PLACEHOLDER, reduceCeilValue, { 1, 1, halfRepeatNum, DEFAULT_REPEAT_STRIDE });
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(sumTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();

        ResetMask();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Mul<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, halfRepeatNum, DEFAULT_REPEAT_STRIDE });
    }
}

template <bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashNDImpl(const LocalTensor<half> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &src, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &inMaxTmp = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    BroadCastLastND brcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;


    if constexpr (isBasicBlock) {
        SoftmaxFlashBasicBlock(dst, sumTensor, maxTensor, src, expMaxTensor, inSumTensor, inMaxTensor, workLocal,
            tiling);
    } else

    {
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset2 = i * tiling.reduceSize;
            offset1 = i * tiling.splitSize;
            Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, inMaxTmp, reduceParam);
            PipeBarrier<PIPE_V>();

            DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Max(maxTensor[offset2], inMaxTmp, tmpBuffer4, tiling.reduceSize);
            PipeBarrier<PIPE_V>();

            Sub(tmpBuffer4, tmpBuffer4, maxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

            Sub(inMaxTmp, inMaxTmp, maxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

            DataCopy(inSumTmp, inSumTensor[offset2], tiling.reduceSize);

            PipeBarrier<PIPE_V>();
            Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            DataCopy(sumTensor[offset2], inSumTmp, tiling.reduceSize);


            BroadCastLastImpl(tmpBuffer0, inMaxTmp,
                { tiling.reduceM, HALF_NUM_PER_BLK, tiling.reduceM, tiling.reduceK });
            PipeBarrier<PIPE_V>();
            Cast(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer0, FLOAT2HALF_ROUND_MODE,
                tiling.reduceSize * HALF_FACTOR);

            Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.splitSize);
        }
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, inMaxTmp, reduceParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(maxTensor[offset2], inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer4, tmpBuffer4, maxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, maxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        DataCopy(inSumTmp, inSumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();


        BroadCastLastImpl(tmpBuffer0, inMaxTmp,
            { tiling.reduceM, FLOAT_NUM_PER_BLK * B16_BYTE_SIZE, tiling.reduceM, tiling.reduceK });
        PipeBarrier<PIPE_V>();
        Cast(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer0, FLOAT2HALF_ROUND_MODE,
            tiling.tailReduceSize * B16_BYTE_SIZE);
        DataCopy(sumTensor[offset2], inSumTmp, tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();
        Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.tailSplitSize);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling new_tiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, new_tiling, elementNumPerBlk, isUpdate, isBasicBlock);
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, workLocal, originalSrcShape, new_tiling, isUpdate);
    } else {
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, workLocal, originalSrcShape, tiling, isUpdate);
    }
}
template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<half> &dstTensor, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &srcTensor, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();

    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, FLOAT_NUM_PER_BLK, isUpdate, isBasicBlock);
        if (!isUpdate) {
            SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape,
                newTiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, workLocal, originalSrcShape, newTiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling);
        }
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SoftMaxTiling &tiling, bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    auto tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t workLocalSize = tempBuffer.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, elementNumPerBlk, isUpdate, isBasicBlock);
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, tempBuffer, originalSrcShape, newTiling, isUpdate);
    } else {
        SoftmaxFlashPostProcess<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
            inMaxTensor, tempBuffer, originalSrcShape, tiling, isUpdate);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlashImpl(const LocalTensor<half> &dstTensor, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &srcTensor, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SoftMaxTiling &tiling, bool isUpdate,
    const SoftMaxShapeInfo &softmaxShapeInfo)
{
    auto tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    uint32_t workLocalSize = tempBuffer.GetSize();
    if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, FLOAT_NUM_PER_BLK, isUpdate, isBasicBlock);
        if (!isUpdate) {
            SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, tempBuffer, originalSrcShape,
                newTiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, tempBuffer, originalSrcShape, newTiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, tempBuffer, originalSrcShape,
                tiling);
        } else {
            SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, tempBuffer, originalSrcShape, tiling);
        }
    }
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate = false, const SoftMaxShapeInfo &softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 89 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const SoftMaxTiling& tiling,
    bool isUpdate = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 130 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inSumTensor, const LocalTensor<T>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, bool isUpdate = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 170 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] __inline__ __attribute__((always_inline)) void SoftmaxFlash(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, bool isUpdate = false,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
}
#pragma end_pipe
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h" 2


# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_v220_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_impl.h" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_base_impl.h"
namespace AscendC {
const uint32_t CUBE_HALF_SIZE = CUBE_MAX_SIZE / 2;

template <typename T> struct ConfusionTranspose0213Params {
    [aicore] ConfusionTranspose0213Params(){};

    int32_t i;
    int32_t j;
    int32_t k;
    int32_t m;

    TransposeType transposeType;

    TransDataTo5HDParams mainTransDataParams;
    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams tailTransDataParams;
    TransDataTo5HDParams transDataParams2;


    uint64_t mainDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t mainSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t tailDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t tailSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose0213TransParams(ConfusionTranspose0213Tiling& tiling,
    ConfusionTranspose0213Params<T>& params, TransposeType transposeTypeIn)
{
    params.transposeType = transposeTypeIn;

    params.mainTransDataParams.repeatTimes = tiling.newPopSize / CUBE_MAX_SIZE;
    params.mainTransDataParams.dstRepStride = BLOCK_CUBE / tiling.blockSize;
    params.mainTransDataParams.srcRepStride = CUBE_MAX_SIZE / tiling.blockSize;
    if (params.mainTransDataParams.repeatTimes == 1) {
        params.mainTransDataParams.dstRepStride = 0;
        params.mainTransDataParams.srcRepStride = 0;
    }

    params.transDataParams1.repeatTimes = tiling.newPopSize / CUBE_MAX_SIZE;
    params.transDataParams1.dstRepStride = tiling.alignA3MulA1 * BLOCK_CUBE / tiling.blockSize;
    params.transDataParams1.srcRepStride = BLOCK_CUBE / tiling.blockSize;
    if (params.transDataParams1.repeatTimes == 1) {
        params.transDataParams1.dstRepStride = 0;
        params.transDataParams1.srcRepStride = 0;
    }

    params.tailTransDataParams.repeatTimes = tiling.tailSize / CUBE_MAX_SIZE;
    params.tailTransDataParams.dstRepStride = BLOCK_CUBE / tiling.blockSize;
    params.tailTransDataParams.srcRepStride = CUBE_MAX_SIZE / tiling.blockSize;
    if (params.tailTransDataParams.repeatTimes == 1) {
        params.tailTransDataParams.dstRepStride = 0;
        params.tailTransDataParams.srcRepStride = 0;
    }

    params.transDataParams2.repeatTimes = tiling.tailSize / CUBE_MAX_SIZE;
    params.transDataParams2.dstRepStride = tiling.alignA3MulA1 * BLOCK_CUBE / tiling.blockSize;
    params.transDataParams2.srcRepStride = BLOCK_CUBE / tiling.blockSize;
    if (params.transDataParams2.repeatTimes == 1) {
        params.transDataParams2.dstRepStride = 0;
        params.transDataParams2.srcRepStride = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213MainHalf(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{

    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.mainDstLocalList[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.mainSrcLocalList[n] = (uint64_t)srcTensor[(tiling.newPopSize * params.m + params.j * tiling.needSize +
            params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)]
            .GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.mainDstLocalList, params.mainSrcLocalList, params.mainTransDataParams);

    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] =
                (uint64_t)dstTensor[(tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                params.i * tiling.alignA3 + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] =
                (uint64_t)
                    dstTensor[(tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                params.i * BLOCK_CUBE + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList1[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213MainFloat(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{
    for (int16_t p = 0; p < 2; p++) {

        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
            params.mainDstLocalList[n] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2))].GetPhyAddr();
            params.mainDstLocalList[n + 1] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2)) + tiling.blockSize]
                .GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.mainSrcLocalList[n] = (uint64_t)
                srcTensor[(p * tiling.blockSize + tiling.newPopSize * params.m + params.j * tiling.needSize +
                params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.mainDstLocalList, params.mainSrcLocalList, params.mainTransDataParams);
    }
    for (int16_t p = 0; p < 2; p++) {

        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList1[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                    params.i * tiling.alignA3 + tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)]
                    .GetPhyAddr();
                params.dstLocalList1[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                    params.i * tiling.alignA3 + tiling.alignA3MulA1 * (n / 2) + tiling.blockSize +
                    params.k * tiling.batchOffset)].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList1[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                    params.i * BLOCK_CUBE + tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)]
                    .GetPhyAddr();
                params.dstLocalList1[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                    params.i * BLOCK_CUBE + tiling.alignA3MulA1 * (n / 2) + tiling.blockSize +
                    params.k * tiling.batchOffset)].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList1[n] = (uint64_t)tmp1[(p * tiling.blockSize + tiling.newPopH * n)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213TailHalf(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{

    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.tailDstLocalList[n] = (uint64_t)tmp1[tiling.newPopH * n].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.tailSrcLocalList[n] =
            (uint64_t)srcTensor[(tiling.newPopSize * tiling.mainBlocks + params.j * tiling.needSize +
            params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)]
            .GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.tailDstLocalList, params.tailSrcLocalList, params.tailTransDataParams);

    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[(tiling.mainOffset + params.j * BLOCK_CUBE +
                params.i * tiling.alignA3 + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[(tiling.mainOffset + params.j * tiling.shapeA1BlockCube +
                params.i * BLOCK_CUBE + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213TailFloat(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{
    for (int16_t p = 0; p < 2; p++) {

        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
            params.tailDstLocalList[n] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2))].GetPhyAddr();
            params.tailDstLocalList[n + 1] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2)) + tiling.blockSize]
                .GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.tailSrcLocalList[n] = (uint64_t)srcTensor[(p * tiling.blockSize +
                tiling.newPopSize * tiling.mainBlocks + params.j * tiling.needSize +
                params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.tailDstLocalList, params.tailSrcLocalList, params.tailTransDataParams);
    }
    for (int16_t p = 0; p < 2; p++) {

        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * BLOCK_CUBE + params.i * tiling.alignA3 +
                    tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * BLOCK_CUBE + params.i * tiling.alignA3 +
                    tiling.alignA3MulA1 * (n / 2) + tiling.blockSize + params.k * tiling.batchOffset)].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * tiling.shapeA1BlockCube + params.i * BLOCK_CUBE +
                    tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * tiling.shapeA1BlockCube + params.i * BLOCK_CUBE +
                    tiling.alignA3MulA1 * (n / 2) + tiling.blockSize + params.k * tiling.batchOffset)].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp1[(p * tiling.blockSize + tiling.newPopH * n)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T> struct ConfusionTranspose2NZ012NParams {
    [aicore] ConfusionTranspose2NZ012NParams(){};

    int32_t i;
    int32_t j;
    int32_t k;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2Count;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t dstPrehnCount;
    uint32_t dstAllCount;

    TransposeType transposeType;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];

    uint64_t dstLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList3[NCHW_CONV_ADDR_LIST_SIZE];

    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams transDataParams2;
    TransDataTo5HDParams transDataParams3;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose2NZ012N(ConfusionTranspose2NZ012NParams<T> &params,
    ConfusionTranspose2NZ012NTiling &tiling)
{

    if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }

    params.tmp1RemainRowCount = 0;
    params.tmp2Count = 0;
    params.tmp2NeedRowCount = BLOCK_CUBE;
    params.transdataRepeat = 0;
    params.dstPrehnCount = 0;
    params.dstAllCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;

    params.transDataParams3.repeatTimes = 1;
    params.transDataParams3.dstRepStride = 0;
    params.transDataParams3.srcRepStride = 0;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp1RemainRowCountZero(const LocalTensor<T>& srcTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                    .GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList1[n] =
                        (uint64_t)srcTensor[params.i * tiling.alignsBlockCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                        .GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
            }
        }

        params.tmp1RemainRowCount = BLOCK_CUBE;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2RemainRowCountFirst(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{


    if (((params.i * BLOCK_CUBE) <= tiling.hnDiv) && (((params.i + 1) * BLOCK_CUBE) > tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE - tiling.gap;
    }

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams1.dstStride = 0;
            dataCopyParams1.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);

            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2RemainRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] =
                    (uint64_t)
                        dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList2[n] =
                        (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                        tiling.alignsBlockCube +
                        params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                        params.k * tiling.dstBatchOffset]
                        .GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
            }
        }
        params.transdataRepeat += 1;



        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {


            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NCalcTmp2NeedRowCount(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params)
{

    if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE;
    } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
        params.tmp2NeedRowCount =
            (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ? BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
    } else if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2NeedRowCount(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{

    if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count = params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;


            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    } else {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NTmp2NeedRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList3[n] =
                    (uint64_t)
                        dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList3[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList3[n] =
                        (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                        tiling.alignsBlockCube +
                        params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                        params.k * tiling.dstBatchOffset].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList3[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
            }
        }
        params.transdataRepeat += 1;
        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {
            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;

        if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
            params.tmp2NeedRowCount = (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ?
                BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
        } else if (tiling.hnDiv < BLOCK_CUBE) {
            params.tmp2NeedRowCount = tiling.hnDiv;
        }
    }
}

template <typename T> struct ConfusionTranspose2ND012NParams {
    [aicore] ConfusionTranspose2ND012NParams(){};

    int32_t i;
    int32_t j;
    int32_t k;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t tmp2Count;
    uint32_t dstPrehnCount;
    uint32_t dstAllCount;
    uint32_t dstPrehnCountBefore;
    uint32_t PrehnCount;

    TransposeType transposeType;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams1;


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams2;


    uint64_t dstLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams3;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose2ND012N(ConfusionTranspose2ND012NParams<T> &params,
    ConfusionTranspose2ND012NTiling &tiling)
{
    params.tmp1RemainRowCount = 0;
    params.tmp2NeedRowCount = BLOCK_CUBE;
    params.transdataRepeat = 0;
    params.tmp2Count = 0;
    params.dstPrehnCount = 0;

    params.dstAllCount = 0;
    params.dstPrehnCountBefore = 0;
    params.PrehnCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;

    params.transDataParams3.repeatTimes = 1;
    params.transDataParams3.dstRepStride = 0;
    params.transDataParams3.srcRepStride = 0;

    if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp1RemainRowCount(const LocalTensor<T>& srcTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                    .GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList1[n] =
                        (uint64_t)srcTensor[params.i * tiling.alignsCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                        .GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
            }
        }

        params.tmp1RemainRowCount = BLOCK_CUBE;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp1RemainRowCountFirst(ConfusionTranspose2ND012NTiling& tiling,
    ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{


    if (((params.i * BLOCK_CUBE) <= tiling.hnDiv) && (((params.i + 1) * BLOCK_CUBE) > tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE - tiling.gap;
    }

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams1.dstStride = 0;
            dataCopyParams1.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);

            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCountHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    if (tiling.hnDiv <= BLOCK_CUBE) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] =
                (uint64_t)
                    dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsMulAlignHnDiv +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    } else {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] =
                (uint64_t)
                    dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                tiling.alignsMulAlignHnDiv +
                ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCountFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (tiling.hnDiv <= BLOCK_CUBE) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] =
                    (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    params.j * tiling.alignHnDivCube + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        } else {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
                params.dstLocalList2[n + 1] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCount(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            ConfusionTranspose2ND012NTmp2NeedRowCountHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose2ND012NTmp2NeedRowCountFloat(dstTensor, tiling, params, tmp2);
        }
        params.transdataRepeat += 1;



        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {


            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;
        if (params.dstPrehnCount == tiling.hnDiv) {
            params.dstPrehnCount = 0;
        }
    }

    if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE;
    } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
        params.tmp2NeedRowCount =
            (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ? BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
    } else if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NTmp2NeedRowCountNotZero(ConfusionTranspose2ND012NTiling& tiling,
    ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount != 0) {
        if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count = params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        } else {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012Ntmp2NeedRowCountZeroHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    if (tiling.hnDiv <= BLOCK_CUBE) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList3[n] =
                (uint64_t)
                    dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsMulAlignHnDiv +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    } else {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList3[n] =
                (uint64_t)
                    dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                tiling.alignsMulAlignHnDiv +
                ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList3[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012Ntmp2NeedRowCountZeroFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (tiling.hnDiv <= BLOCK_CUBE) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList3[n] =
                    (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    params.j * tiling.alignHnDivCube + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        } else {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList3[n] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
                params.dstLocalList3[n + 1] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList3[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012Ntmp2NeedRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            ConfusionTranspose2ND012Ntmp2NeedRowCountZeroHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose2ND012Ntmp2NeedRowCountZeroFloat(dstTensor, tiling, params, tmp2);
        }
        params.transdataRepeat += 1;
        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {
            params.tmp1RemainRowCount = 0;
        }
        params.tmp2Count = 0;
        if (params.dstPrehnCount == tiling.hnDiv) {
            params.dstPrehnCount = 0;
        }

        if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
            params.tmp2NeedRowCount = (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ?
                BLOCK_CUBE :
                (tiling.hnDiv - params.dstPrehnCount);
        } else if (tiling.hnDiv < BLOCK_CUBE) {
            params.tmp2NeedRowCount = tiling.hnDiv;
        }
    }
}

template <typename T> struct ConfusionTranspose012Params {
    [aicore] ConfusionTranspose012Params(){};

    int32_t i;
    int32_t j;
    int32_t k;

    TransposeType transposeType;
    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams transDataParams2;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t tmp2Count;
    uint32_t tmp1Count;
    uint32_t dstAllCount;
    uint32_t dstPreHnCount;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConfusionTranspose012TransParams(ConfusionTranspose012Tiling& tiling,
    ConfusionTranspose012Params<T>& params, TransposeType transposeTypeIn)
{
    if (tiling.shapeH < 16) {
        params.tmp2NeedRowCount = tiling.shapeH;
    }
    params.transposeType = transposeTypeIn;

    params.tmp1RemainRowCount = 0;
    params.tmp2NeedRowCount = 16;
    params.transdataRepeat = 0;
    params.tmp2Count = 0;
    params.tmp1Count = 0;
    params.dstAllCount = 0;
    params.dstPreHnCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp1RemainRowCountZero(const LocalTensor<T>& srcTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount != 0) {
        return;
    }
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsCube +
                params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
    } else if constexpr (sizeof(T) == sizeof(float)) {
        for (uint16_t m = 0; m < 2; m++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] =
                    (uint64_t)srcTensor[params.i * tiling.alignsCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        }
    }

    if (tiling.hnDiv > BLOCK_CUBE) {
        if ((params.dstPreHnCount < tiling.hnDiv) && (params.dstPreHnCount + BLOCK_CUBE) > tiling.hnDiv) {
            params.tmp1RemainRowCount = tiling.hnDiv - params.dstPreHnCount;
        } else {
            params.tmp1RemainRowCount = BLOCK_CUBE;
        }
    } else if (tiling.hnDiv <= BLOCK_CUBE) {
        params.tmp1RemainRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp1ToTmp2(const LocalTensor<T>& tmp2, ConfusionTranspose012Tiling& tiling,
    ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);
            params.tmp1Count += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.dstPreHnCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    } else if (params.tmp2NeedRowCount > params.tmp1RemainRowCount) {
        if (params.tmp1RemainRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);
            params.tmp1Count += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.dstPreHnCount += params.tmp1RemainRowCount;
            params.tmp2Count += params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp2ToDstHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                params.j * tiling.alignhBlockCube + tiling.alignH * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * tiling.alignsCube +
                params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp2ToDstFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                    params.j * tiling.alignhBlockCube + m * tiling.blockSizeMulAlignH +
                    tiling.alignH * (n / 2) + params.k * tiling.dstBatchOffset].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                    params.j * tiling.alignhBlockCube + m * tiling.blockSizeMulAlignH + tiling.alignH * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * tiling.alignsCube +
                    params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp2ToDst(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            ConfusionTranspose012Tmp2ToDstHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose012Tmp2ToDstFloat(dstTensor, tiling, params, tmp2);
        }

        params.tmp2Count = 0;
        params.transdataRepeat += 1;
        if (params.dstAllCount == tiling.shapeH) {
            params.tmp1RemainRowCount = 0;
            params.dstAllCount = 0;
        }

        if ((params.transdataRepeat + 1) != tiling.hBlockNum) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else {
            params.tmp2NeedRowCount = tiling.shapeH - params.transdataRepeat * BLOCK_CUBE;
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Tmp1ToTmp2Remain(const LocalTensor<T>& tmp2,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1[(params.tmp1Count * BLOCK_CUBE)], dataCopyParams2);
            params.dstAllCount += params.tmp2NeedRowCount;
            params.dstPreHnCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    } else {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1[(params.tmp1Count * BLOCK_CUBE)], dataCopyParams2);
            params.dstAllCount += params.tmp1RemainRowCount;
            params.dstPreHnCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213Compute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose0213Tiling &tiling)
{
    ConfusionTranspose0213Params<T> params;
    InitConfusionTranspose0213TransParams<T>(tiling, params, transposeTypeIn);

    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        for (params.i = 0; params.i < tiling.shapeA1; params.i++) {
            for (params.j = 0; params.j < tiling.widthTiling; params.j++) {
                for (params.m = 0; params.m < tiling.mainBlocks; params.m++) {

                    if constexpr (sizeof(T) == sizeof(half)) {
                        ConfusionTranspose0213MainHalf(dstTensor, srcTensor, params, tiling, tmp1);
                    } else if constexpr (sizeof(T) == sizeof(float)) {
                        ConfusionTranspose0213MainFloat(dstTensor, srcTensor, params, tiling, tmp1);
                    } else {
                                     ;
                    }
                }
                if (tiling.tailSize) {
                    if constexpr (sizeof(T) == sizeof(half)) {
                        ConfusionTranspose0213TailHalf(dstTensor, srcTensor, params, tiling, tmp1);
                    } else if constexpr (sizeof(T) == sizeof(float)) {
                        ConfusionTranspose0213TailFloat(dstTensor, srcTensor, params, tiling, tmp1);
                    } else {
                                     ;
                    }
                }
            }
        }
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012NCompute(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    ConfusionTranspose2NZ012NTiling &tiling)
{
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    ConfusionTranspose2NZ012NParams<T> params;
    InitConfusionTranspose2NZ012N(params, tiling);

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            for (params.i = 0; params.i < tiling.hBlockNum; params.i++) {
                ConfusionTranspose2NZ012NTmp1RemainRowCountZero(srcTensor, tiling, params, tmp1);
                ConfusionTranspose2NZ012NTmp2RemainRowCountFirst(tiling, params, tmp1, tmp2);
                ConfusionTranspose2NZ012NTmp2RemainRowCountZero(dstTensor, tiling, params, tmp2);
                ConfusionTranspose2NZ012NCalcTmp2NeedRowCount(tiling, params);


                while (params.tmp1RemainRowCount) {
                    ConfusionTranspose2NZ012NTmp2NeedRowCount(tiling, params, tmp1, tmp2);
                    ConfusionTranspose2NZ012NTmp2NeedRowCountZero(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012NCompute(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    ConfusionTranspose2ND012NTiling &tiling)
{
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    ConfusionTranspose2ND012NParams<T> params;
    InitConfusionTranspose2ND012N(params, tiling);

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            for (params.i = 0; params.i < tiling.hBlockNum; params.i++) {
                ConfusionTranspose2ND012NTmp1RemainRowCount(srcTensor, tiling, params, tmp1);
                ConfusionTranspose2ND012NTmp1RemainRowCountFirst(tiling, params, tmp1, tmp2);
                ConfusionTranspose2ND012NTmp2NeedRowCount(dstTensor, tiling, params, tmp2);


                while (params.tmp1RemainRowCount) {
                    ConfusionTranspose2ND012NTmp2NeedRowCountNotZero(tiling, params, tmp1, tmp2);
                    ConfusionTranspose2ND012Ntmp2NeedRowCountZero(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012Compute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose012Tiling &tiling)
{
    ConfusionTranspose012Params<T> params;
    InitConfusionTranspose012TransParams<T>(tiling, params, transposeTypeIn);
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            params.transdataRepeat = 0;
            for (params.i = 0; params.i < (tiling.hnDivBlockNum * tiling.shapeN); params.i++) {

                params.tmp1Count = 0;
                ConfusionTranspose012Tmp1RemainRowCountZero(srcTensor, tiling, params, tmp1);

                if (params.dstPreHnCount == tiling.hnDiv) {
                    params.dstPreHnCount = 0;
                }
                ConfusionTranspose012Tmp1ToTmp2(tmp2, tiling, params, tmp1);
                ConfusionTranspose012Tmp2ToDst(dstTensor, tiling, params, tmp2);


                while (params.tmp1RemainRowCount) {

                    ConfusionTranspose012Tmp1ToTmp2Remain(tmp2, tiling, params, tmp1);

                    ConfusionTranspose012Tmp2ToDst(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTransposeOnlyCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    ConfusionTransposeOnlyTiling &tiling)
{
    uint64_t dstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = tiling.repeat;
    transDataParams.dstRepStride = transDataParams.repeatTimes > 1 ? tiling.stride : 0;
    transDataParams.srcRepStride = transDataParams.repeatTimes > 1 ? 1 : 0;
    for (int32_t i = 0; i < tiling.highBlock; i++) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
                dstLocalList[m] = (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * m].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                srcLocalList[n] =
                    (uint64_t)srcTensor[i * tiling.width * BLOCK_CUBE + tiling.width * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(dstLocalList, srcLocalList, transDataParams);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + 2) {
                dstLocalList[m] = (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * (m / 2)].GetPhyAddr();
                dstLocalList[m + 1] =
                    (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * (m / 2) + tiling.blockSize].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                srcLocalList[n] =
                    (uint64_t)srcTensor[i * tiling.width * BLOCK_CUBE + tiling.width * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(dstLocalList, srcLocalList, transDataParams);
        }
    }
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_v220_impl.h" 2

namespace AscendC {





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose0213(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose0213Tiling& tiling)
{
    ConfusionTranspose0213Compute(dstTensor, srcTensor, sharedTmpBuffer, transposeTypeIn, tiling);
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2NZ012N(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, ConfusionTranspose2NZ012NTiling& tiling)
{
    ConfusionTranspose2NZ012NCompute(dstTensor, srcTensor, sharedTmpBuffer, tiling);
}





template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose2ND012N(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, ConfusionTranspose2ND012NTiling& tiling)
{
    ConfusionTranspose2ND012NCompute(dstTensor, srcTensor, sharedTmpBuffer, tiling);
}






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose012(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose012Tiling& tiling)
{
    ConfusionTranspose012Compute(dstTensor, srcTensor, sharedTmpBuffer, transposeTypeIn, tiling);
}




template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTransposeOnly(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTransposeOnlyTiling& tiling)
{
    ConfusionTransposeOnlyCompute(dstTensor, srcTensor, tiling);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/../../impl/transpose/confusion_transpose/confusion_transpose_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTransposeImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{





    if (transposeType == TransposeType::TRANSPOSE_NZ2ND_0213 || transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        ConfusionTranspose0213(dstTensor, srcTensor, sharedTmpBuffer, transposeType,
            reinterpret_cast<ConfusionTranspose0213Tiling&>(tiling));
    }




    else if (transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITH_N) {
        ConfusionTranspose2NZ012N(dstTensor, srcTensor, sharedTmpBuffer,
            reinterpret_cast<ConfusionTranspose2NZ012NTiling &>(tiling));
    }




    else if (transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITH_N) {
        ConfusionTranspose2ND012N(dstTensor, srcTensor, sharedTmpBuffer,
            reinterpret_cast<ConfusionTranspose2ND012NTiling &>(tiling));
    }





    else if (transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N ||
        transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
        ConfusionTranspose012(dstTensor, srcTensor, sharedTmpBuffer, transposeType,
            reinterpret_cast<ConfusionTranspose012Tiling &>(tiling));
    }



    else if (transposeType == TransposeType::TRANSPOSE_ND2ND_ONLY) {
        ConfusionTransposeOnly(dstTensor, srcTensor, reinterpret_cast<ConfusionTransposeOnlyTiling &>(tiling));
    }
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    ConfusionTransposeImpl<T>(dstTensor, srcTensor, sharedTmpBuffer, transposeType, tiling);
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/transpose/confusion_transpose.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ConfusionTranspose(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    ConfusionTransposeImpl<T>(dstTensor, srcTensor, tmpBuffer, transposeType, tiling);
}
#pragma end_pipe
}
# 65 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h" 1
# 15 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_common_impl.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_common_impl.h"
namespace AscendC {

template <typename T> [aicore] __inline__ __attribute__((always_inline)) void InitScalarSelectMask(const LocalTensor<T> &tmpMask, T scalar)
{
    SetVectorMask<half, MaskMode::COUNTER>(0, ONE_REPEAT_BYTE_SIZE / sizeof(T));
    Duplicate<T, false>(tmpMask, static_cast<T>(scalar), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask(tmpMask);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void CastMaskToHalfImpl(const LocalTensor<half> &localMaskTmp, const LocalTensor<U> &mask)
{
    if constexpr (sizeof(U) == 4) {
        LocalTensor<float> tmpTensor = mask.template ReinterpretCast<float>();
        Cast<half, float, false>(localMaskTmp, tmpTensor, RoundMode::CAST_ODD, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    } else if constexpr (sizeof(U) == 2) {
        LocalTensor<int16_t> tmpTensor = mask.template ReinterpretCast<int16_t>();
        Cast<half, int16_t, false>(localMaskTmp, tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else if constexpr (sizeof(U) == 1) {
        LocalTensor<uint8_t> tmpTensor = mask.template ReinterpretCast<uint8_t>();
        Cast<half, uint8_t, false>(localMaskTmp, tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    }
}

template <typename T, typename U, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMaskPerAxisImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t srcAxisLen,
    const uint32_t bucketSize)
{
    const auto paddingLen = AlignUp(bucketSize / ONE_BYTE_BIT_SIZE, ONE_BLK_SIZE);
    const auto localMaskTmpOffset = paddingLen;
    const auto localScalarOffset = sizeof(half) * bucketSize;
    LocalTensor<uint8_t> localMask = sharedTmpBuffer;
    LocalTensor<half> localMaskTmp = sharedTmpBuffer.ReinterpretCast<half>();
    SetVectorMask<half, MaskMode::COUNTER>(0, srcAxisLen);
    CastMaskToHalfImpl<U>(localMaskTmp, mask);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;
    constexpr auto loopSize = ONE_REPEAT_BYTE_SIZE / sizeof(half);
    const auto repeatTime = DivCeil(srcAxisLen, loopSize);

    if constexpr (!reverse) {
        CompareScalar<half, uint8_t, false>(localMask, localMaskTmp, static_cast<half>(0), CMPMODE::EQ,
            MASK_PLACEHOLDER, repeatTime, unaryParams);
    } else {
        CompareScalar<half, uint8_t, false>(localMask, localMaskTmp, static_cast<half>(0), CMPMODE::NE,
            MASK_PLACEHOLDER, repeatTime, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    Select(dst, localMask, src0, 1, binaryParams);
}


template <typename U>
[aicore] __inline__ __attribute__((always_inline)) void RemoveRedundantMask(const LocalTensor<U> &dst, const LocalTensor<U> &mask,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    LocalTensor<uint16_t> tmpDst = dst.template ReinterpretCast<uint16_t>();
    LocalTensor<uint16_t> tmpMask = mask.template ReinterpretCast<uint16_t>();

    uint64_t rsvdCnt;

    GatherMask<uint16_t>(tmpDst, tmpMask, REDUCEV2_MODE_SEVEN, true, info.srcLastAxis * sizeof(U) / sizeof(uint16_t),
        { DEFAULT_BLK_STRIDE, static_cast<uint16_t>(info.firstAxis),
        static_cast<uint16_t>(info.maskLastAxis * sizeof(U) / ONE_BLK_SIZE), 0 },
        rsvdCnt);
    SetMaskCount();
}

template <typename T, typename U, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMaskLoopImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t loopSize,
    const uint32_t totalLen, const uint32_t srcOriginOffset, const uint32_t maskOriginOffset)
{
    for (uint32_t offset = 0; offset < totalLen; offset += loopSize) {
        auto calSize = offset + loopSize > totalLen ? totalLen - offset : loopSize;
        SelectWithBytesMaskPerAxisImpl<T, U, reverse>(dst[srcOriginOffset + offset], src0[srcOriginOffset + offset],
            src1, mask[maskOriginOffset + offset], sharedTmpBuffer, calSize, loopSize);
        PipeBarrier<PIPE_V>();
    }
}
}
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_v220_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) uint32_t ComputeMaskExtraBufSize(const uint32_t srcSize, const uint32_t typeSize)
{
    return AlignUp(srcSize * typeSize, ONE_BLK_SIZE);
}

template <typename T, typename U, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMaskProcess(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<U> &tmpMask, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SelectWithBytesMaskShapeInfo &info, const uint32_t tmpBufferOffset, const uint32_t loopSize)
{
    if (info.srcLastAxis != info.maskLastAxis) {
        RemoveRedundantMask(tmpMask, mask, sharedTmpBuffer, info);
        PipeBarrier<PIPE_V>();
    }

    SelectWithBytesMaskLoopImpl<T, U, reverse>(dst, src0, src1, tmpMask, sharedTmpBuffer[tmpBufferOffset], loopSize,
        src0.GetSize(), 0, 0);
}
}
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/../../impl/select/selectwithbytesmask/selectwithbytesmask_impl.h" 2




namespace AscendC {


template <typename T, typename U, bool isReuseMask, bool reverse = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void SelectWithBytesMaskImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    T src1, const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SelectWithBytesMaskShapeInfo &info)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    PipeBarrier<PIPE_V>();
    constexpr uint32_t MIN_REQUIRED_BUFFER = 1024;
    constexpr uint32_t RESERVED_BUFFER = 256;
    constexpr uint32_t MAX_CALC_BYTE_PER_LOOP = 255 * ONE_REPEAT_BYTE_SIZE;
    const uint32_t firstAxis = info.firstAxis;
    const uint32_t srcLastAxis = info.srcLastAxis;
    const uint32_t maskLastAxis = info.maskLastAxis;
    const uint32_t srcSize = src0.GetSize();


                                                                                                  ;

                                                                                                   ;

                                                                                                               ;
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
                                                                                                                   ;
    LocalTensor<U> tmpMask = mask;
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t tmpBufferOffset = 0;
    if constexpr (!isReuseMask) {
        if (srcLastAxis != maskLastAxis) {
            const uint32_t tmpMaskRequiredBuffer = ComputeMaskExtraBufSize(srcSize, sizeof(U));



              ;
            tmpMask = sharedTmpBuffer.template ReinterpretCast<U>();
            tmpMask.SetSize(tmpMaskRequiredBuffer / sizeof(U));
            bufferSize -= tmpMaskRequiredBuffer;
            tmpBufferOffset = tmpMaskRequiredBuffer;
        }
    }

    bufferSize -= RESERVED_BUFFER;
    uint32_t loopSize = bufferSize / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    if (loopSize > MAX_CALC_BYTE_PER_LOOP / sizeof(half)) {
        loopSize = MAX_CALC_BYTE_PER_LOOP / sizeof(half);
    }

    SetMaskCount();
    InitScalarSelectMask(tmpTensor, src1);

    SelectWithBytesMaskProcess<T, U, reverse>(dst, src0, src1, mask, tmpMask, sharedTmpBuffer, info, tmpBufferOffset,
        loopSize);
    SetMaskNorm();
    ResetMask();
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h"
template <typename T, typename U, bool isReuseMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMask(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, false>(dst, src0, src1, mask, sharedTmpBuffer, info);
}
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/select/selectwithbytesmask.h"
template <typename T, typename U, bool isReuseMask = true>
[aicore] __inline__ __attribute__((always_inline)) void SelectWithBytesMask(const LocalTensor<T> &dst, T src0, const LocalTensor<T> &src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, true>(dst, src1, src0, mask, sharedTmpBuffer, info);
}
#pragma end_pipe
}
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sinh/sinh_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sinh/sinh_common_impl.h"
namespace AscendC {
constexpr float SINH_NEG_LN_TWO = -0.69314718055994530941723212145818;
constexpr float SINH_NEG_ONE = -1.0;
constexpr float SINH_POINT_FIVE = 0.5;
constexpr uint32_t SINH_HALF_CALC_PROC = 4;
constexpr uint32_t SINH_FLOAT_CALC_PROC = 1;


template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void SinhCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t offset)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<T>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<T>();


    Muls<T, false>(tmpFloatBuffer1, src, static_cast<T>(SINH_NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<T>(SINH_NEG_LN_TWO), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, src, static_cast<T>(SINH_NEG_LN_TWO), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<T, false>(dst, dst, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}




template<>
[aicore] __inline__ __attribute__((always_inline)) void SinhCompute(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t offset)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<float>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[offset];

    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, SINH_NEG_LN_TWO, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, SINH_NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, SINH_NEG_LN_TWO, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 123 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sinh/sinh_common_impl.h"
    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(T);

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / SINH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / SINH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                                       ;

    const uint32_t loopCount = calCount / splitCount;
    const uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        SinhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        SinhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer, splitCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    SinhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SinhImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sinh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SinhImpl(dstTensor, srcTensor, calCount);
}
# 98 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sinh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 67 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swiglu/swiglu_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swiglu/swiglu_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swiglu/swiglu_common_impl.h" 2


namespace AscendC {
constexpr float NUMBER_ONE = 1.0;
constexpr uint32_t REPEAT_TIME_SWIGLU = 1;
constexpr uint32_t SWIGLU_HALF_BUFFER_SIZE = 3;
constexpr uint32_t SWIGLU_FLOAT_TMP_BUFFER_SIZE = 0;
constexpr uint32_t SWIGLU_STRIDE_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const float &scalarValue, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const float &scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, LocalTensor<T> &srcTensor0, LocalTensor<T> &srcTensor1,
                              const float &scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
                              const LocalTensor<T> &srcTensor1, const float &scalarValue,
                              const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }




                                                                            ;

                                                                                             ;

                                                                                                                    ;

                                                                                                            ;

                                                                                        ;

                                                                     ;

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));
    uint32_t stackSize = calCount;

    if (sizeof(T) == sizeof(half)) {


        stackSize = sharedTmpBuffer.GetSize() / sizeof(float) / SWIGLU_HALF_BUFFER_SIZE;
    }

    stackSize = ((stackSize * sizeof(T)) / ONE_BLK_SIZE * ONE_BLK_SIZE) / sizeof(T);

    if (stackSize <= 0) {
        stackSize = ONE_BLK_SIZE / sizeof(T);
    }

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T>(0, stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        SwiGLUImpl(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], scalarValue, tmpBuffer, stackSize);
        offset = offset + stackSize;
    }
    if (tail != 0) {

        bool isTail32BAligned = (tail * sizeof(T) % ONE_BLK_SIZE == 0);
        auto tail32BAligned = (tail * sizeof(T) / ONE_BLK_SIZE + (isTail32BAligned ? 0 : 1)) *
                              ONE_BLK_SIZE / sizeof(T);
        SetVectorMask<T>(0, tail);
        SwiGLUImpl(dstTensor[offset], srcTensor0[offset], srcTensor1[offset],
                   scalarValue, tmpBuffer, tail32BAligned);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SwishCalcSimplified(
   const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const float &scalarValue)
{


    const UnaryRepeatParams unaryParams;

    Muls<float, false>(dstTensor, srcTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Div<float, false>(dstTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, const LocalTensor<T> &src1,
                                  const float &beta, const LocalTensor<float> &sharedTmpBuffer, uint32_t calCount)
{

    float scalar = static_cast<float>(static_cast<float>(-1.0) * static_cast<float>(beta));
    SwishCalcSimplified(dst, src1, scalar);

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(dst, src0, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLUImpl<half>(const LocalTensor<half> &dst, const LocalTensor<half> &src0,
                                        const LocalTensor<half> &src1, const float &beta,
                                        const LocalTensor<float> &sharedTmpBuffer, uint32_t calCount)
{
    LocalTensor<float> tmpSrc1FloatBuffer1 = sharedTmpBuffer;
    LocalTensor<float> tmpSrc1FloatBuffer2 = sharedTmpBuffer[calCount];
    LocalTensor<float> tmpSrc0FloatBuffer = sharedTmpBuffer[2 * calCount];


    Cast<float, half, false>(tmpSrc1FloatBuffer1, src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();


    float scalar = static_cast<float>(static_cast<float>(-1.0) * static_cast<float>(beta));
    SwishCalcSimplified(tmpSrc1FloatBuffer2, tmpSrc1FloatBuffer1, scalar);


    Cast<float, half, false>(tmpSrc0FloatBuffer, src0, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(tmpSrc1FloatBuffer2, tmpSrc0FloatBuffer, tmpSrc1FloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Cast<half, float, false>(dst, tmpSrc1FloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, LocalTensor<T>& srcTensor0, LocalTensor<T>& srcTensor1,
                              const float& scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, srcTensor0.GetSize());
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const float& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    SwiGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const float& scalarValue, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, calCount);
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
                              const LocalTensor<T>& srcTensor1, const float& scalarValue,
                              const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, calCount);
}
#pragma end_pipe
}
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_v220_impl.h"
namespace AscendC {
template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReGluCast(const LocalTensor<T> &dstTensor, const LocalTensor<float> &srcTensor)
{
    if constexpr (IsSameType<T, half>::value) {
        Cast<T, float, false>(dstTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else {
        Cast<T, float, false>(dstTensor, srcTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/reglu/reglu_common_impl.h" 2




namespace AscendC {
const uint8_t REGLU_HALF_CALC_PROCEDURE = 3;
const uint32_t REGLU_TEMP_BUFFER_OFFSET = 2U;

[aicore] __inline__ __attribute__((always_inline)) void Compute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Maxs<float, false>(dstTensor, srcTensor1, static_cast<float>(0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, srcTensor0, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template<typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReGluCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const LocalTensor<float>& x0CastBuffer = tmpTensor;
    const LocalTensor<float>& x1CastBuffer = tmpTensor[splitSize];
    const LocalTensor<float>& yCastBuffer = tmpTensor[splitSize * REGLU_TEMP_BUFFER_OFFSET];

    Cast<float, T, false>(x0CastBuffer, srcTensor0, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    Cast<float, T, false>(x1CastBuffer, srcTensor1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Compute(yCastBuffer, x0CastBuffer, x1CastBuffer);
    ReGluCast(dstTensor, yCastBuffer);
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGluImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

                                                                                ;
                                                                                                                  ;




                                                                                        ;

    SetMaskCount();
    if constexpr (IsSameType<T, float>::value) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
        Compute(dstTensor, srcTensor0, srcTensor1);
    } else {
        uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);
                                                                                                     ;
        LocalTensor<float> tmpBuffer;
        tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        uint32_t stackSize = 0;

        stackSize = tmpBufferSize / REGLU_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                             ;

        const uint32_t round = calCount / stackSize;
        const uint32_t tail = calCount % stackSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);
        uint32_t offset = 0;

        for (uint32_t i = 0; i < round; i++) {
            ReGluCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, stackSize);
            offset = offset + stackSize;
        }

        if (tail != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, tail);
            ReGluCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, stackSize);
        }
    }
    SetMaskNorm();
    ResetMask();
}

template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGluImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool hasStackBuffer = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                          ;
    ReGluImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/reglu.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGlu(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (g_coreType == AIC) {
        return;
    }
    ReGluImpl<T, false>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}







template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReGlu(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const uint32_t calCount)
{
    if (g_coreType == AIC) {
        return;
    }
    ReGluImpl<T, false>(dstTensor, srcTensor0, srcTensor1, calCount);
}

#pragma end_pipe
}
# 69 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h" 1
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TanCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h" 2




namespace AscendC {
constexpr uint32_t TAN_HALF_CALC_PROCEDURE = 10;
constexpr uint32_t TAN_FLOAT_CALC_PROCEDURE = 4;

constexpr float PI_FOR_X_TODIV = 0.3183098733425140380859375;
constexpr float KPI_FIRS_PI_MULS = 0.0009670257568359375;

constexpr float PI_V2 = 3.140625;

constexpr float PI_DOWN = 1.57079637050628662109375;
constexpr float PI_DOWN_NEG = -1.57079637050628662109375;

constexpr float KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float PI_RESDOWN_ADDS = 0.00000004371139000189375;
constexpr float PI_RESDOWN_ADDS_NEG = -0.00000004371139000189375;

constexpr float KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;

constexpr float KPI_FOR_PI_MULS = -1.0291767438275201129727065563201904296875e-13;

constexpr float TAN_RES_MULIT_SCA = 0.0698520831551998762793;
constexpr float TAN_RES_ADDICT_UP = -6.8711573651634203789;
constexpr float TAN_2ADDS = 61.20362572811089435388;
constexpr float TAN_3ADDS = -24.8048928861126769186219;

[aicore] __inline__ __attribute__((always_inline)) void KPI_0(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, PI_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_1(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor1, srcTensor, PI_DOWN, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor2, srcTensor, PI_DOWN_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_2(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor1, resTensor1, PI_RESDOWN_ADDS_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor2, resTensor2, PI_RESDOWN_ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_3(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void KPI_4(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_FOR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void TanRound(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
# 169 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h"
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(roundTensor, srcTensor, PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    TanCast(roundTensor, roundTensor, RoundMode::CAST_RINT);

    KPI_0(dstTensor, srcTensor, roundTensor);
    KPI_1(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_2(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_3(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_4(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
}

[aicore] __inline__ __attribute__((always_inline)) void TanPolynomialApproximation(const LocalTensor<float>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<float>& roundTensor,
    const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
# 197 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/tan/tan_common_impl.h"
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;


    Mul<float, false>(roundTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, TAN_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, TAN_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, TAN_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, dstTensor, srcTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(roundTensor, roundTensor, TAN_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, resTensor1, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, resTensor2, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TanCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const LocalTensor<T>& tmpTensor1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<T>& tmpTensor2 = tmpTensor1[calSize];
    const LocalTensor<T>& tmpTensor3 = tmpTensor2[calSize];
    const LocalTensor<T>& tmpTensor4 = tmpTensor3[calSize];

    Adds<T, false>(tmpTensor4, srcTensor, static_cast<float>(0.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    TanRound(dstTensor, tmpTensor4, tmpTensor1, tmpTensor2, tmpTensor3);
    TanPolynomialApproximation(dstTensor, tmpTensor4, tmpTensor1, tmpTensor2, tmpTensor3);
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void TanCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t calSize)
{
    const LocalTensor<float>& tempTensorConv = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpTensor1 = tempTensorConv[calSize];
    const LocalTensor<float>& tmpTensor2 = tmpTensor1[calSize];
    const LocalTensor<float>& tmpTensor3 = tmpTensor2[calSize];
    const LocalTensor<float>& tmpTensor4 = tmpTensor3[calSize];

    Cast<float, half, false>(tmpTensor1, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    TanRound(tempTensorConv, tmpTensor1, tmpTensor2, tmpTensor3, tmpTensor4);
    TanPolynomialApproximation(tempTensorConv, tmpTensor1, tmpTensor2, tmpTensor3, tmpTensor4);

    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(T);
    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / TAN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / TAN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
                                                                                           ;

    const uint32_t loopCount = calCount / splitCount;
    const uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < loopCount; ++i) {
        TanCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer, splitCount);
    }

    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        TanCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer, tailCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
 template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Tan<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TanImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 111 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    TanImpl(dstTensor, srcTensor, calCount);
}
# 128 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Tan<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 70 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_v220_impl.h"
namespace AscendC {
constexpr uint32_t STRIDE_OF_DIFFERENT_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void RoundComputeCount(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{}

template <>
[aicore] __inline__ __attribute__((always_inline)) void RoundComputeCount<float, false>(const LocalTensor<float> &dstTensor,
    const LocalTensor<float> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void RoundComputeCount<half, false>(const LocalTensor<half> &dstTensor,
    const LocalTensor<half> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(float);
    splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    const LocalTensor<float> &tmpTensor = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<half, MaskMode::COUNTER>(0, splitCount);


    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(tmpTensor, srcTensor[i * splitCount], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS });
        PipeBarrier<PIPE_V>();

        Cast<float, float, false>(tmpTensor, tmpTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dstTensor[i * splitCount], tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, calcTail);
        Cast<float, half, false>(tmpTensor, srcTensor[loopCount * splitCount], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS });
        PipeBarrier<PIPE_V>();
        Cast<float, float, false>(tmpTensor, tmpTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dstTensor[loopCount * splitCount], tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
}
}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/round/round_common_impl.h" 2





namespace AscendC {

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void RoundImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SetMaskCount();
    if constexpr (sizeof(T) == sizeof(half)) {
        RoundComputeCount<half, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
    } else {
        RoundComputeCount<float, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void RoundImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    RoundImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Round(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    RoundImpl<T, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/round.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Round(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    RoundImpl<T, false>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 71 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_common_impl.h" 1
# 16 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_v220_impl.h" 1
# 14 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TruncCastForTrunc(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 17 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/trunc/trunc_common_impl.h" 2


namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void TruncCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();

    Cast<float, half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    TruncCastForTrunc(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void TruncCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{

    TruncCastForTrunc(dstTensor, srcTensor, tmpTensor);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TruncImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(T);
    constexpr uint32_t TRUNC_HALF_CALC_PROCEDURE = 2;
    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / TRUNC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        TruncCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        TruncCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TruncImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TruncImpl(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TruncImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 56 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    TruncImpl(dstTensor, srcTensor, calCount);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Trunc<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Trunc<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 72 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SwishCalcSimplified(
    const LocalTensor<T> &dstAddr, const LocalTensor<T> &srcAddr, T &scalarValue, uint32_t repeatTimes)
{


    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstAddr, srcAddr, scalarValue, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(dstAddr, dstAddr, static_cast<T>(1), MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Div<T, false>(dstAddr, srcAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void SwishCompute(
    const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, uint32_t dataSize, const T scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 64 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h"
    T scalar = static_cast<T>(static_cast<float>(-1) * static_cast<float>(scalarValue));

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, dataSize);
    SwishCalcSimplified(dstLocal, srcLocal, scalar, 1);
    SetMaskNorm();
# 106 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/swish/swish_common_impl.h"
    ResetMask();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h" 2

namespace AscendC {
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/swish.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Swish(
    const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, uint32_t dataSize, const T scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SwishCompute<T, false>(dstLocal, srcLocal, dataSize, scalarValue);
}
}
# 73 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_utils.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_utils.h"
namespace {
constexpr uint16_t MIN_SORT32_SIZE = 32;
constexpr uint16_t MIN_RPSORT16_SIZE = 16;
constexpr uint32_t BUF_LIST_SIZE = 2;
constexpr uint32_t MRG_MAX_ARRAY_SIZE = 15;
constexpr uint32_t MRGSORT_VALID_QUEUE = 4;
constexpr uint32_t MRGSORT_VALID_TWO = 2;
constexpr uint32_t MRGSORT_VALID_TWO_OFFSET = 769;
constexpr uint32_t TWO = 2;
constexpr uint32_t THREE = 3;
constexpr uint32_t FOUR = 4;
constexpr uint32_t FIVE = 5;
constexpr uint32_t SIX = 6;
constexpr uint32_t SEVEN = 7;
constexpr uint32_t EIGHT = 8;
constexpr uint32_t NINE = 9;
constexpr uint32_t TWELVE = 12;
constexpr uint32_t SIXTEEN = 16;
constexpr uint32_t THIRTY_TWO = 32;
constexpr uint32_t FORTYEIGHT = 48;
constexpr uint32_t VREDUCEV2_HALF_MASK = 128;
constexpr uint32_t VREDUCEV2_FOUR_BYTE_MASK = 64;
constexpr uint32_t SRC1_STACK_TENSORSIZE = 10;
constexpr uint32_t SRC1_STACK_VAL_OFFSET = 16;
constexpr uint32_t TOPK_INNER_ALIGN_LEN = 32;
constexpr uint32_t TOPK_NORMAL_INNER_MAX_HALF_LEN = 2048;
constexpr uint32_t TOPK_NSMALL_INNER_LEN = 32;
constexpr uint32_t TOPK_NORMAL_INNER_MAX_LEN = 4096;
}

namespace AscendC {
struct TopKInfo {
    int32_t outter = 1;
    int32_t inner;
    int32_t n;
};

enum class TopKMode {
    TOPK_NORMAL,
    TOPK_NSMALL,
};
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h"
namespace AscendC {
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgFourQueueSort(const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo, uint16_t &z, int32_t &mrgFourQueueCount, int32_t &dstIdx)
{
    uint16_t innerU16Type = static_cast<uint16_t>(topKInfo.inner);
    for (; z * MRGSORT_VALID_QUEUE <= innerU16Type; z *= MRGSORT_VALID_QUEUE) {
        auto src = (mrgFourQueueCount % TWO == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
        dstIdx = (mrgFourQueueCount + 1) % TWO;
        auto dst = (dstIdx == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
        mrgFourQueueCount += 1;
        uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = {z, z, z, z};
        struct MrgSort4Info srcInfo(elementLengths, false, 0b1111, tilling.mrgSortRepeat / z);
        struct MrgSortSrcList<T> srcList(src,
            src[z * tilling.mrgSortSrc1offset],
            src[z * tilling.mrgSortSrc2offset],
            src[z * tilling.mrgSortSrc3offset]);
        MrgSort<T>(dst, srcList, srcInfo);
        PipeBarrier<PIPE_V>();
        const DataCopyParams intriParams = {static_cast<uint16_t>(tilling.copyUbToUbBlockCount), 1, 0, 0};
        DataCopy(src, dst, intriParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgTwoQueueSort(const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo, const uint16_t z, const int32_t mrgFourQueueCount, int32_t &dstIdx, const int32_t k)
{
    int32_t arrayCount = 0;
    if (z < topKInfo.inner) {

        int32_t mrgArray[MRG_MAX_ARRAY_SIZE] = {0};
        int32_t tmpInner = topKInfo.inner;
        for (int32_t i = z; i >= MIN_SORT32_SIZE; i /= MRGSORT_VALID_QUEUE) {
            int32_t count;
            for (count = 0; count < tmpInner / i; ++count) {
                mrgArray[arrayCount++] = i;
            }
            tmpInner -= count * i;
        }
        uint16_t mrgSortedLen = 0;
        for (int32_t i = 0; i < arrayCount - 1; ++i) {
            auto src = ((mrgFourQueueCount + i) % TWO == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
            dstIdx = (mrgFourQueueCount + 1 + i) % TWO;
            auto dst = (dstIdx == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
            mrgSortedLen += static_cast<uint16_t>(mrgArray[i]);
            uint64_t tmpMrgSortedLen = mrgSortedLen;
            uint64_t tmpMrgArray = mrgArray[i + 1];
            if (mrgSortedLen > k) {
                tmpMrgSortedLen = k;
            }
            if (mrgArray[i + 1] > k) {
                tmpMrgArray = k;
            }
            uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = {
                static_cast<uint16_t>(tmpMrgSortedLen), static_cast<uint16_t>(tmpMrgArray), 0, 0};
            struct MrgSort4Info srcInfo(elementLengths, false, 0b0011, 1);
            struct MrgSortSrcList<T> srcList(src, src[mrgSortedLen * tilling.mrgSortTwoQueueSrc1Offset], src, src);
            MrgSort<T>(dst, srcList, srcInfo);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GatherDstValAndDstIdx(const LocalTensor<T> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const int32_t dstIdx, const int32_t dstOffsetFourBytes, const int outterIdx)
{
    uint64_t rsvdCnt = 0;
    int32_t tmpLocalDstOffset = tilling.innerDataSize * dstIdx;
    if constexpr (sizeof(T) == sizeof(float)) {

        struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
        GatherMask<T>(dstValueLocal[dstOffsetFourBytes], tmpLocal[tmpLocalDstOffset], REDUCEV2_MODE_ONE,
                      true, tilling.maskVreducev2FourBytes, reducev2Params, rsvdCnt);
    } else {
        int32_t dstOffsetTwoBytes = outterIdx * tilling.kAlignTwoBytes;

        struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
        GatherMask<T>(dstValueLocal[dstOffsetTwoBytes], tmpLocal[tmpLocalDstOffset], REDUCEV2_MODE_THREE,
                      true, tilling.maskVreducev2TwoBytes, reducev2Params, rsvdCnt);
    }
    PipeBarrier<PIPE_V>();

    LocalTensor<float> tempBuffer = dstIndexLocal[dstOffsetFourBytes].template ReinterpretCast<float>();
    LocalTensor<float> tempBufferLocal = tmpLocal[tmpLocalDstOffset].template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
    GatherMask<float>(tempBuffer, tempBufferLocal, REDUCEV2_MODE_TWO, true,
                      tilling.maskVreducev2FourBytes, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
}

template <typename T, bool isInitIndex>
[aicore] __inline__ __attribute__((always_inline)) void TmpLocalSort32(const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal,
    const LocalTensor<T> &tmpLocal, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest,
    const int outterIdx, const UnaryRepeatParams unaryParams)
{


    int offset = outterIdx * topKInfo.inner;
    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, topKInfo.inner);
        Muls<T, false>(tmpLocal[tilling.innerDataSize], srcLocal[offset], T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        if constexpr (!isInitIndex) {
            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.srcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, tilling.sortRepeat);
        } else {
            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, tilling.sortRepeat);
        }
    } else {
        if constexpr (!isInitIndex) {
            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.srcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal[offset], tempBufferUint32, tilling.sortRepeat);
        } else {
            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal[offset], tempBufferUint32, tilling.sortRepeat);
        }
    }
    PipeBarrier<PIPE_V>();
    const DataCopyParams intriParams = {static_cast<uint16_t>(tilling.copyUbToUbBlockCount), 1, 0, 0};
    DataCopy(tmpLocal[tilling.innerDataSize], tmpLocal, intriParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isInitIndex, bool isHasfinish>
[aicore] __inline__ __attribute__((always_inline)) void TopKCompute(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<T> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest)
{
    const UnaryRepeatParams unaryParams;
    for (int j = 0; j < topKInfo.outter; ++j) {
        int32_t dstOffsetFourBytes = j * tilling.kAlignFourBytes;
        TmpLocalSort32<T, isInitIndex>(srcLocal, srcIndexLocal, tmpLocal, tilling, topKInfo, isLargest, j, unaryParams);

        int32_t mrgFourQueueCount = 0;
        uint16_t z = MIN_SORT32_SIZE;
        int32_t dstIdx = 0;
        MrgFourQueueSort<T>(tmpLocal, tilling, topKInfo, z, mrgFourQueueCount, dstIdx);
        MrgTwoQueueSort<T>(tmpLocal, tilling, topKInfo, z, mrgFourQueueCount, dstIdx, k);
        GatherDstValAndDstIdx(dstValueLocal, dstIndexLocal, tmpLocal, tilling, dstIdx, dstOffsetFourBytes, j);

        if constexpr (isHasfinish) {
            bool finishValue = finishLocal.GetValue(j);
            auto eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
            SetFlag<HardEvent::S_V>(eventID);
            WaitFlag<HardEvent::S_V>(eventID);

            if (finishValue) {
                SetVectorMask<T, MaskMode::COUNTER>(0, k);
                Duplicate<int32_t, false>(dstIndexLocal[dstOffsetFourBytes],
                    static_cast<int32_t>(topKInfo.n),
                    MASK_PLACEHOLDER,
                    1,
                    1,
                    DEFAULT_REPEAT_STRIDE);
            }
        }
        PipeBarrier<PIPE_V>();
    }
}

[aicore] __inline__ __attribute__((always_inline)) void TopKNSmallGetFloatTopKValue(const LocalTensor<float> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<float> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo)
{
    LocalTensor<uint32_t> src1stackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset].template
                                            ReinterpretCast<uint32_t>();
    auto eventID = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventID);
    WaitFlag<HardEvent::V_S>(eventID);
    src1stackTensor.SetSize(SRC1_STACK_TENSORSIZE);

    src1stackTensor.SetValue(0, tilling.vreduceValMask0);

    src1stackTensor.SetValue(1, tilling.vreduceValMask1);

    src1stackTensor.SetValue(EIGHT, tilling.vreduceIdxMask0);
    src1stackTensor.SetValue(NINE, tilling.vreduceIdxMask1);

    eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventID);
    WaitFlag<HardEvent::S_V>(eventID);

    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    uint64_t rsvdCnt = 0;
    GatherMask<float, uint32_t>(dstValueLocal, tmpLocal, src1stackTensor,
                                true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params, rsvdCnt);


    LocalTensor<float> tempBuffer = dstIndexLocal.template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params2(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    GatherMask<float, uint32_t>(tempBuffer, tmpLocal,
                                tmpLocal[tilling.topkMrgSrc1MaskSizeOffset + EIGHT].ReinterpretCast<uint32_t>(),
                                true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params2, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
}

[aicore] __inline__ __attribute__((always_inline)) void TopKNSmallGetHalfTopKValue(const LocalTensor<half> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<half> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo)
{
    LocalTensor<uint16_t> src1stackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset].template
                                            ReinterpretCast<uint16_t>();
    auto eventID = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventID);
    WaitFlag<HardEvent::V_S>(eventID);
    src1stackTensor.SetSize(EIGHT);
    src1stackTensor.SetValue(0, tilling.vreducehalfValMask0);
    src1stackTensor.SetValue(1, tilling.vreducehalfValMask1);
    src1stackTensor.SetValue(TWO, tilling.vreducehalfValMask2);
    src1stackTensor.SetValue(THREE, tilling.vreducehalfValMask3);
    src1stackTensor.SetValue(FOUR, tilling.vreducehalfValMask4);
    src1stackTensor.SetValue(FIVE, tilling.vreducehalfValMask5);
    src1stackTensor.SetValue(SIX, tilling.vreducehalfValMask6);
    src1stackTensor.SetValue(SEVEN, tilling.vreducehalfValMask7);
    LocalTensor<uint32_t> indexstackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset + SRC1_STACK_VAL_OFFSET].
                                             template ReinterpretCast<uint32_t>();;
    indexstackTensor.SetSize(TWO);
    indexstackTensor.SetValue(0, tilling.vreduceIdxMask0);
    indexstackTensor.SetValue(1, tilling.vreduceIdxMask1);
    eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventID);
    WaitFlag<HardEvent::S_V>(eventID);

    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    uint64_t rsvdCnt = 0;
    GatherMask<half, uint16_t>(dstValueLocal, tmpLocal, src1stackTensor,
                               true, VREDUCEV2_HALF_MASK, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();

    LocalTensor<float> tempBufferIndex = dstIndexLocal.template ReinterpretCast<float>();
    LocalTensor<float> tempBufferLocal = tmpLocal.template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params2(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    GatherMask<float, uint32_t>(tempBufferIndex, tempBufferLocal,
        tempBufferLocal[(tilling.topkMrgSrc1MaskSizeOffset + SRC1_STACK_VAL_OFFSET) / TWO].ReinterpretCast<uint32_t>(),
        true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params2, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
}

template <typename T, bool isInitIndex, bool isHasfinish>
[aicore] __inline__ __attribute__((always_inline)) void TopKNSmallCompute(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<T> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest)
{
    if (!isLargest) {
        if (!isInitIndex) {

            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.topkNSmallSrcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, topKInfo.outter);
        } else {

            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, topKInfo.outter);
        }
    } else {
        if (!isInitIndex) {

            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.topkNSmallSrcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal, tempBufferUint32, topKInfo.outter);
        } else {

            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal, tempBufferUint32, topKInfo.outter);
        }
    }
    PipeBarrier<PIPE_V>();

    if constexpr (sizeof(T) == sizeof(float)) {
        TopKNSmallGetFloatTopKValue(dstValueLocal, dstIndexLocal, tmpLocal, tilling, topKInfo);
        return;
    } else {
        TopKNSmallGetHalfTopKValue(dstValueLocal, dstIndexLocal, tmpLocal, tilling, topKInfo);
        return;
    }
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNSmall(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{
    LocalTensor<T> tempBuffer = tmpLocal.template ReinterpretCast<T>();

    if constexpr (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = tempBuffer[tilling.topkNSmallSrcIndexOffset].template
                                             ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
        if (topKInfo.outter > 1) {
            Copy(indexLocalTmp[topKInfo.inner], indexLocalTmp, THIRTY_TWO, topKInfo.outter - 1, {1, 1, FOUR, 0});
            PipeBarrier<PIPE_V>();
        }
    }

    SetMaskCount();
    const UnaryRepeatParams unaryParams;

    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.allDataSize);
        Muls<T, false>(tempBuffer[tilling.innerDataSize], srcLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
    TopKNSmallCompute<T, isInitIndex, isHasfinish>(dstValueLocal,
        dstIndexLocal,
        srcLocal,
        srcIndexLocal,
        finishLocal,
        tempBuffer,
        k,
        tilling,
        topKInfo,
        isLargest);

    if (!isLargest) {
        PipeBarrier<PIPE_V>();
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNSmall(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{
    LocalTensor<T> stackTensor;
    PopStackBuffer<T, TPosition::LCM>(stackTensor);
# 418 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_v220_impl.h"
    stackTensor.SetSize(tilling.tmpLocalSize);


    if constexpr (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = stackTensor[tilling.topkNSmallSrcIndexOffset].template
                                             ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
        if (topKInfo.outter > 1) {
            Copy(indexLocalTmp[topKInfo.inner], indexLocalTmp, THIRTY_TWO, topKInfo.outter - 1, {1, 1, FOUR, 0});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskCount();
    const UnaryRepeatParams unaryParams;

    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.allDataSize);
        Muls<T, false>(stackTensor[tilling.innerDataSize], srcLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    TopKNSmallCompute<T, isInitIndex, isHasfinish>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
                                                   finishLocal, stackTensor, k, tilling, topKInfo, isLargest);
    if (!isLargest) {
        PipeBarrier<PIPE_V>();

        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h" 2





namespace AscendC {
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNormal(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{
    LocalTensor<T> tempBuffer = tmpLocal.template ReinterpretCast<T>();

    if (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = tempBuffer[tilling.srcIndexOffset].template ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
    TopKCompute<T, isInitIndex, isHasfinish>(dstValueLocal,
        dstIndexLocal,
        srcLocal,
        srcIndexLocal,
        finishLocal,
        tempBuffer,
        k,
        tilling,
        topKInfo,
        isLargest);

    if (!isLargest) {
        const UnaryRepeatParams unaryParams;
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] __inline__ __attribute__((always_inline)) void TopKNormal(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{
    LocalTensor<T> stackTensor;
    PopStackBuffer<T, TPosition::LCM>(stackTensor);
# 91 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/../../impl/sort/topk/topk_common_impl.h"
    stackTensor.SetSize(tilling.tmpLocalSize);

    if (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = stackTensor[tilling.srcIndexOffset].template ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
    TopKCompute<T, isInitIndex, isHasfinish>(dstValueLocal,
        dstIndexLocal,
        srcLocal,
        srcIndexLocal,
        finishLocal,
        stackTensor,
        k,
        tilling,
        topKInfo,
        isLargest);

    if (!isLargest) {
        const UnaryRepeatParams unaryParams;
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h" 2







namespace AscendC {
#pragma begin_pipe(V)
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h"
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false,
    enum TopKMode topkMode = TopKMode::TOPK_NORMAL>
[aicore] __inline__ __attribute__((always_inline)) void TopK(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    if constexpr (topkMode == TopKMode::TOPK_NORMAL) {
        TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal,
            dstIndexLocal,
            srcLocal,
            srcIndexLocal,
            finishLocal,
            tmpLocal,
            k,
            tilling,
            topKInfo,
            isLargest);
    }
    if constexpr (topkMode == TopKMode::TOPK_NSMALL) {
        TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal,
            dstIndexLocal,
            srcLocal,
            srcIndexLocal,
            finishLocal,
            tmpLocal,
            k,
            tilling,
            topKInfo,
            isLargest);
    }
}
# 129 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/sort/topk.h"
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false,
    enum TopKMode topkMode = TopKMode::TOPK_NORMAL>
[aicore] __inline__ __attribute__((always_inline)) void TopK(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    if constexpr (topkMode == TopKMode::TOPK_NORMAL) {
        TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(
            dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal, k, tilling, topKInfo, isLargest);
    }
    if constexpr (topkMode == TopKMode::TOPK_NSMALL) {
        TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(
            dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal, k, tilling, topKInfo, isLargest);
    }
}

#pragma end_pipe
}
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/geglu/geglu_common_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/geglu/geglu_common_impl.h"
namespace AscendC {
constexpr float COEFF0 = -0.0713548162726;
constexpr float COEFF1 = 2.2363860002236e1;
constexpr uint32_t GEGLU_HALF_BUFFER_SIZE = 8;
constexpr uint32_t GEGLU_FLOAT_BUFFER_SIZE = 0;
constexpr uint32_t GEGLU_STRIDE_DIGITS = 2;
constexpr uint32_t GEGLU_ALGINED = 31;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }
    LocalTensor<uint8_t> tmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, tmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/../../impl/activation/geglu/geglu_common_impl.h"
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));
    SetMaskCount();

    if (sizeof(T) == sizeof(half)) {
        auto tmpBufCount = sharedTmpBuffer.GetSize() / GEGLU_HALF_BUFFER_SIZE;
        tmpBufCount = tmpBufCount * sizeof(T) / ONE_BLK_SIZE * ONE_BLK_SIZE / sizeof(T);
        for (uint32_t offset = 0; offset < calCount; offset += tmpBufCount) {
            auto splitSize = (calCount - offset) > tmpBufCount ? tmpBufCount : (calCount - offset);
            SetVectorMask<T>(0, splitSize);
            splitSize = (splitSize * sizeof(T) + GEGLU_ALGINED) / ONE_BLK_SIZE * ONE_BLK_SIZE / sizeof(T);
            GeGLUCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, splitSize);
        }
    } else {
        SetVectorMask<T>(0, calCount);
        GeGLUCompute(dstTensor, srcTensor0, srcTensor1, tmpBuffer, calCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src0, const LocalTensor<T> &src1,
    const LocalTensor<float> &tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;


    Mul<T, false>(dst, src1, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, dst, static_cast<T>(COEFF1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(dst, dst, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<T, false>(dst, dst, static_cast<T>(COEFF0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, dst, static_cast<T>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<T, false>(dst, src1, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(dst, src0, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



template <>
[aicore] __inline__ __attribute__((always_inline)) void GeGLUCompute(const LocalTensor<half> &dst, const LocalTensor<half> &src0,
    const LocalTensor<half> &src1, const LocalTensor<float> &tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpBuffer[calSize];

    Cast<float, half, false>(tmpFloatBuffer1, src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS});
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, COEFF1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, COEFF0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer1, src0, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS});
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer, uint32_t calCount)
{
    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }

    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, calCount);
}
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    GeGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, srcTensor0.GetSize());
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1)
{
    GeGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, srcTensor0.GetSize());
}
#pragma end_pipe
}
# 75 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void LGammaFloor(const LocalTensor<float> &dst, const LocalTensor<float> &src)
{
    Cast<float, float, false>(
        dst, src, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/lgamma/lgamma_common_impl.h" 2





namespace AscendC {
namespace {
constexpr float f05 = 0.5;
constexpr float f07 = 0.7;
constexpr float f15 = 1.5;
constexpr float f58 = 5.8;
constexpr float f1 = 1.0;
constexpr float f2 = 2.0;
constexpr float f3 = 3.0;
constexpr float fn05 = -0.5;
constexpr float fn1 = -1.0;
constexpr float fn2 = -2.0;
constexpr float fn3 = -3.0;
constexpr float fPi = 3.1415927410125732421875;
constexpr uint32_t FLOAT_NOREUSE_CALC_PROC = 8;
constexpr uint32_t FLOAT_REUSE_CALC_PROC = 7;
constexpr uint32_t LGAMMA_HALF_CALC_PROCEDURE = 13;
constexpr uint32_t i2 = 2;
constexpr uint32_t i4 = 4;
constexpr uint32_t i6 = 6;
constexpr uint32_t i7 = 7;
constexpr uint32_t i16 = 16;
constexpr float PI = 3.14159265358979323846264338327950288;
constexpr float t0 = 0.0f;
constexpr float t4 = 4.0f;
constexpr float t5 = 5.0f;
constexpr float t01 = 0.1f;
constexpr float t12 = 12.0f;
constexpr float N01 = -0.1f;

constexpr size_t params007Len = 7U;
constexpr float params007[params007Len] = {0.00358751555905,
    -0.00547128543258,
    -0.0446271263063,
    0.167317703366,
    -0.0421359799802,
    -0.655867278576,
    0.577215373516};

constexpr size_t params0715Len = 11U;
constexpr float params0715[params0715Len] = {0.0458826646209,
    0.103739671409,
    0.122803635895,
    0.127524212003,
    0.143216684461,
    0.169343575835,
    0.207407936454,
    0.27058750391,
    0.400685429573,
    0.82246696949,
    0.577215671539};

constexpr size_t params153Len = 10U;
constexpr float params153[params153Len] = {4.95984932058e-05,
    -0.000220894842641,
    0.000541314249858,
    -0.00120451697148,
    0.00288425176404,
    -0.00738275796175,
    0.0205813199282,
    -0.067352488637,
    0.322467029095,
    0.422784328461};

constexpr size_t params378X1Len = 4U;
constexpr size_t params378X2Len = 3U;
constexpr float params378X1[params378X1Len] = {-748.890319824, -12349.7421875, -41061.375, -48310.6640625};
constexpr float params378X2[params378X2Len] = {-259.250976562, -10777.1796875, -92685.046875};

constexpr size_t params58Len = 2U;
constexpr float params58[params58Len] = {0.000777830660809, -0.00277765537612};

constexpr size_t negParamsOddLen = 4U;
constexpr float negParamsOdd[negParamsOddLen] = {0.00002427957952022552490234375,
    -0.001388786011375486850738525390625,
    0.0416667275130748748779296875,
    -0.4999999701976776123046875};
constexpr size_t negParamsEvenLen = 3U;
constexpr float negParamsEven[negParamsEvenLen] = {
    -0.000195746586541645228862762451171875, 0.0083327032625675201416015625, -0.16666662693023681640625};
}

struct LGammaFParams {
    [aicore] LGammaFParams()
    {}
    LocalTensor<float> tmp1;
    LocalTensor<float> tmp2;
    LocalTensor<float> tmp3;
    LocalTensor<float> tmp4;
    LocalTensor<float> tmp5;
    LocalTensor<float> tmp6;
    LocalTensor<float> tmpScalar;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> tmpMask1;
    LocalTensor<uint8_t> tmpMask2;
    LocalTensor<uint8_t> tmpMask3;
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    uint32_t splitSize;
};

[aicore] __inline__ __attribute__((always_inline)) void LGammaCalcMulAdd(const LocalTensor<float> &tmp, const LocalTensor<float> &src,
    const UnaryRepeatParams &unaryParams, const BinaryRepeatParams binaryParams, const float params[],
    const size_t paramLen)
{

    Muls<float, false>(tmp, src, params[0], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmp, tmp, params[1], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();



    for (size_t i = 2U; i < paramLen && i < params0715Len - 1U; ++i) {

        Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        Adds<float, false>(tmp, tmp, params[i], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    if (paramLen == params0715Len) {
        Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        Adds<float, false>(tmp, tmp, params[params0715Len - 1U], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma007(const LocalTensor<float> &src, const LGammaFParams &params)
{

    LGammaCalcMulAdd(params.tmp1, src, params.unaryParams, params.binaryParams, params007, params007Len);


    Mul<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.tmp1, params.tmp1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma0715(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Muls<float, false>(params.tmp1, src, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmp1, params.tmp1, f1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params0715, params0715Len);
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma153(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Adds<float, false>(params.tmp1, src, fn2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params153, params153Len);
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma358(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Adds<float, false>(params.tmp1, src, fn3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params378X1, params378X1Len);

    constexpr float ftmp2 = -143033.40625;
    Adds<float, false>(params.tmp2, params.tmp2, ftmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp3, params.tmp1, params.unaryParams, params.binaryParams, params378X2, params378X2Len);

    constexpr float ftmp3 = -206353.578125;
    Adds<float, false>(params.tmp3, params.tmp3, ftmp3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(params.tmp3, params.tmp2, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp3, params.tmp3, params.tmp1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGamma58(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Ln<float, false>(params.tmp1, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tmp2, src, fn05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(params.tmp2, params.tmp2, params.tmp1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Reciprocal<float, false>(params.tmp3, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmp4, params.tmp3, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp1, params.tmp4, params.unaryParams, params.binaryParams, params58, params58Len);
    constexpr float ftmp1 = 0.0833332762122;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(params.tmp1, params.tmp1, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    constexpr float ftmp2 = 0.91893851757;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenLTMask(
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src, const LGammaFParams &params, const float scalar)
{
    Duplicate<float, false>(params.tmp1, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmp1, CMPMODE::LT, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenGEMask(
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src, const LGammaFParams &params, const float scalar)
{
    Duplicate<float, false>(params.tmp1, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmp1, CMPMODE::GE, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenRangeMask(
    const LocalTensor<float> &src, const LGammaFParams &params, const float min, const float max)
{
    LGammaGenLTMask(params.mask, src, params, max);
    LGammaGenGEMask(params.tmpMask1, src, params, min);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    SetVectorMask<float>(0, params.splitSize);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaSelect(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const LocalTensor<uint8_t> &mask, const LGammaFParams &params)
{
    SetCmpMask<float>(params.tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(params.tmp1, mask, src, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(dst, params.tmp1, dst, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaPositive(const LGammaFParams &params)
{
    Duplicate<float, false>(params.tmp5, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LGammaGenLTMask(params.mask, params.tmp6, params, f07);
    LGamma007(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp1, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f07, f15);
    LGamma0715(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp2, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f15, f3);
    LGamma153(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp2, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f3, f58);
    LGamma358(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp3, params.mask, params);


    NotNumUnion notNum;
    notNum.i = F32_INF;
    LGammaGenRangeMask(params.tmp6, params, f58, notNum.f);
    LGamma58(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp1, params.mask, params);


    LGammaGenGEMask(params.mask, params.tmp6, params, notNum.f);
    LGammaSelect(params.tmp5, params.tmp6, params.mask, params);
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalNegTmp1(const LGammaFParams &params)
{

    Add<float, false>(params.tmp2, params.tmp6, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmp2, params.tmp2, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaFloor(params.tmp2, params.tmp2);


    Muls<float, false>(params.tmp3, params.tmp2, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaFloor(params.tmp3, params.tmp3);
    Muls<float, false>(params.tmp3, params.tmp3, f2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmp3, params.tmp2, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaGenGEMask(params.mask, params.tmp3, params, f05);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));

    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    SetVectorMask<float>(0, params.splitSize);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmp2, params.tmp2, fn05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp2, params.tmp2, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp2, params.tmp2, fPi, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalNegTmp2(const LGammaFParams &params)
{

    Mul<float, false>(params.tmp3, params.tmp2, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(
        params.tmp1, params.tmp3, params.unaryParams, params.binaryParams, negParamsEven, negParamsEvenLen);
    Mul<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmp2, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp2, params.tmp1, params.tmpMask1, params);


    LGammaCalcMulAdd(params.tmp1, params.tmp3, params.unaryParams, params.binaryParams, negParamsOdd, negParamsOddLen);
    Adds<float, false>(params.tmp1, params.tmp1, f1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp2, params.tmp1, params.mask, params);
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalNegTmp3(const LGammaFParams &params)
{

    Abs<float, false>(params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(params.tmp3, params.tmp1, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.tmp1, params.tmp3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    constexpr float ftmp = 1.1447298526763916015625;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp2, params.tmp5, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp2, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaCalMinNeg(const LocalTensor<float> &src, const LGammaFParams &params)
{

    Ln<float, false>(params.tmp1, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaNegative(const LGammaFParams &params)
{
    Duplicate<float, false>(params.tmp4, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    constexpr float minf = 9.99999968266e-20;
    LGammaGenLTMask(params.mask, params.tmp6, params, minf);
    LGammaCalMinNeg(params.tmp6, params);
    LGammaSelect(params.tmp4, params.tmp1, params.mask, params);


    LGammaCalNegTmp1(params);

    LGammaCalNegTmp2(params);

    LGammaCalNegTmp3(params);


    LGammaFloor(params.tmp1, params.tmp6);


    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(
        params.mask, params.tmp1, params.tmp6, CMPMODE::NE, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();


    NotNumUnion notNum;
    notNum.i = F32_INF;
    Duplicate<float, false>(params.tmp1, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Compare<float, uint8_t, false>(
        params.tmpMask1, params.tmp3, params.tmp1, CMPMODE::LT, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    LGammaGenGEMask(params.tmpMask1, params.tmp6, params, minf);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelect(params.tmp4, params.tmp2, params.mask, params);


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(), params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    LGammaGenGEMask(params.mask, params.tmp6, params, minf);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);

    Duplicate<float, false>(params.tmp2, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp4, params.tmp2, params.tmpMask1, params);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LGammaInitFParams(
    const LocalTensor<float> &tmp, const uint32_t splitSize, const LocalTensor<float> &src, LGammaFParams &params)
{
    params.tmp1 = tmp;
    params.tmp2 = tmp[splitSize];
    params.tmp3 = params.tmp2[splitSize];
    params.tmp4 = params.tmp3[splitSize];
    params.tmp5 = params.tmp4[splitSize];
    params.tmp6 = params.tmp5[splitSize];
    if constexpr (isReuseSource) {
        params.mask = params.tmp6[splitSize].ReinterpretCast<uint8_t>();
        params.tmpScalar = src.ReinterpretCast<float>();
    } else {
        params.tmpScalar = params.tmp6[splitSize];
        params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    }
    params.tmpMask1 = params.mask[splitSize];
    params.tmpMask2 = params.tmpMask1[splitSize];
    params.tmpMask3 = params.tmpMask2[splitSize];

    params.tmp1.SetSize(splitSize);
    params.tmp2.SetSize(splitSize);
    params.tmp3.SetSize(splitSize);
    params.tmp4.SetSize(splitSize);
    params.tmp5.SetSize(splitSize);
    params.tmp6.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.tmpMask1.SetSize(splitSize);
    params.tmpMask2.SetSize(splitSize);
    params.tmpMask3.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);

    params.splitSize = splitSize;
}

[aicore] __inline__ __attribute__((always_inline)) void Lgamma1Compute(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmp1Tensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmp2Tensor[splitSize];
    LocalTensor<float> tmp4Tensor = tmp3Tensor[splitSize];
    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);

    Adds<float, false>(tmp1Tensor, srcTensor, t4, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dstTensor, f1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dstTensor, dstTensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp2Tensor, dstTensor, PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmp2Tensor, tmp2Tensor, f2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmp2Tensor, tmp2Tensor, f05, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();



    Muls<float, false>(tmp3Tensor, dstTensor, N01, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp4Tensor, tmp1Tensor, t12, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmp4Tensor, tmp4Tensor, tmp3Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(dstTensor, f1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmp4Tensor, dstTensor, tmp4Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tmp4Tensor, tmp4Tensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmp4Tensor, tmp4Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmp4Tensor, tmp4Tensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmp4Tensor, tmp4Tensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(dstTensor, tmp4Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputePosHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmpTensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmpTensor[splitSize * 2];
    LocalTensor<float> tmp4Tensor = tmpTensor[splitSize * 3];

    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);


    Lgamma1Compute(dstTensor, srcTensor, tmpTensor, splitSize);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmp3Tensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f3, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dstTensor, dstTensor, tmp3Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputeNegHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmp1Tensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmp2Tensor[splitSize];
    LocalTensor<float> tmp4Tensor = tmp3Tensor[splitSize];
    LocalTensor<float> tmp5Tensor = tmp4Tensor[splitSize];
    LocalTensor<float> tmp6Tensor = tmp5Tensor[splitSize];
    LocalTensor<float> tmp7Tensor = tmpTensor[splitSize * i2];
    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);
    tmp5Tensor.SetSize(splitSize);
    tmp6Tensor.SetSize(splitSize);
    tmp7Tensor.SetSize(splitSize * i4);


    Muls<float, false>(tmp1Tensor, srcTensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmp1Tensor, tmp1Tensor, f1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LgammaComputePosHalf(dstTensor, tmp1Tensor, tmp7Tensor, splitSize);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, dstTensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaFloor(tmp1Tensor, srcTensor);


    Sub<float, false>(tmp1Tensor, srcTensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp1Tensor, tmp1Tensor, PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    SinCompute(tmp2Tensor, tmp1Tensor, tmp7Tensor, splitSize, false);
    PipeBarrier<PIPE_V>();

    Abs<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(tmp3Tensor, PI, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    Div<float, false>(tmp2Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenLTMaskHalf(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    const LocalTensor<float> &tmptensor, const float scalar, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Duplicate<float, false>(tmptensor, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, tmptensor, CMPMODE::LT, MASK_PLACEHOLDER, repeat, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void LGammaGenGEMaskHalf(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    const LocalTensor<float> &tmptensor, const float scalar, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Duplicate<float, false>(tmptensor, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, tmptensor, CMPMODE::GE, MASK_PLACEHOLDER, repeat, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LGammaSelectHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmpTensor, const LocalTensor<float> &tmpScalar)
{
    const BinaryRepeatParams binParams;
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(tmpTensor, mask, srcTensor, 1, binParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(dstTensor, tmpTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LGammaSelectINF(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmpTensor, const LocalTensor<float> &tmpScalar)
{
    const BinaryRepeatParams binParams;
    Duplicate<float, false>(tmpScalar, 655040.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(dstTensor, mask, srcTensor, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputeImpl(const LocalTensor<half> &dstTensor, const LocalTensor<half> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t &splitSize, bool isReuseSource)
{
    (void)isReuseSource;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;


    LocalTensor<float> restmpBuffer = tmpTensor;
    LocalTensor<float> srctmpBuffer = restmpBuffer[splitSize];
    Duplicate<float, false>(restmpBuffer, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(srctmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    LocalTensor<float> TensorPosRes = srctmpBuffer[splitSize];

    LocalTensor<float> TensorNegRes = TensorPosRes[splitSize];
    LocalTensor<float> tmp1Tensor = TensorNegRes[splitSize];

    LocalTensor<float> tmpScalar = tmp1Tensor[splitSize];
    Duplicate<float, false>(tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LocalTensor<uint8_t> MaskPos = tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    LocalTensor<uint8_t> MaskNeg = MaskPos[splitSize];
    LocalTensor<uint8_t> tmpMask1 = MaskNeg[splitSize];
    LocalTensor<uint8_t> tmpMask2 = tmpMask1[splitSize];
    LocalTensor<float> stackTensor = tmpScalar[splitSize*i2];

    restmpBuffer.SetSize(splitSize);
    srctmpBuffer.SetSize(splitSize);
    TensorPosRes.SetSize(splitSize);
    TensorNegRes.SetSize(splitSize);
    tmp1Tensor.SetSize(splitSize);
    tmpScalar.SetSize(splitSize);
    MaskNeg.SetSize(splitSize);
    MaskPos.SetSize(splitSize);
    tmpMask1.SetSize(splitSize);
    tmpMask2.SetSize(splitSize);
    stackTensor.SetSize(splitSize * i6);


    LgammaComputePosHalf(TensorPosRes, srctmpBuffer, stackTensor, splitSize);
    PipeBarrier<PIPE_V>();

    LGammaGenGEMaskHalf(MaskPos, srctmpBuffer, tmp1Tensor, 0.0f, splitSize);
    PipeBarrier<PIPE_V>();
    LGammaSelectHalf(restmpBuffer, TensorPosRes, MaskPos, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    LgammaComputeNegHalf(TensorNegRes, srctmpBuffer, stackTensor, splitSize);
    PipeBarrier<PIPE_V>();

    LGammaGenLTMaskHalf(MaskNeg, srctmpBuffer, tmp1Tensor, 0.0f, splitSize);
    PipeBarrier<PIPE_V>();
    LGammaSelectHalf(restmpBuffer, TensorNegRes, MaskNeg, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float>(0, ConstCeil(splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(tmpMask1.ReinterpretCast<uint16_t>(), MaskPos.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, unaryParams);
    Not<uint16_t, false>(tmpMask2.ReinterpretCast<uint16_t>(), MaskNeg.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(tmpMask1.ReinterpretCast<uint16_t>(), tmpMask1.ReinterpretCast<uint16_t>(),
        tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binParams);

    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, splitSize);
    LGammaSelectHalf(restmpBuffer, srctmpBuffer, tmpMask1, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    Abs<float, false>(srctmpBuffer, srctmpBuffer, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LGammaGenGEMaskHalf(tmpMask1, srctmpBuffer, tmp1Tensor, 65504.0f, splitSize);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(tmpMask2.ReinterpretCast<uint16_t>(), tmpMask1.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, splitSize);
    LGammaSelectINF(restmpBuffer, restmpBuffer, tmpMask2, tmp1Tensor, tmpScalar);
    PipeBarrier<PIPE_V>();


    Cast<half, float, false>(dstTensor, restmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
}

[aicore] __inline__ __attribute__((always_inline)) void LgammaComputeImpl(
    const LocalTensor<float> &dst, const LocalTensor<float> &src, LGammaFParams &params)
{

    LGammaGenGEMask(params.tmpMask2, src, params, 0.0f);
    LGammaGenLTMask(params.tmpMask3, src, params, 0.0f);



    Abs<float, false>(params.tmp6, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LGammaPositive(params);
    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(dst, params.tmp5, params.tmpMask2, params);


    LGammaNegative(params);
    LGammaSelect(dst, params.tmp4, params.tmpMask3, params);


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.tmpMask3.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelect(dst, params.tmp6, params.mask, params);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LgammaCompute(const LocalTensor<half> &dstTensor, const LocalTensor<half> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                                 ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);
                                                                                                 ;

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;

                                                                                                        ;
    stackSize = tmpBufferSize / LGAMMA_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                         ;

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;
    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, stackSize);
    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        LgammaComputeImpl(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
        offset = offset + stackSize;
    }

    if (tail > 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        LgammaComputeImpl(
            dstTensor[round * stackSize], srcTensor[round * stackSize], tmpBuffer, stackSize, isReuseSource);
    }
    SetMaskNorm();
    AscendCUtils::ResetMask();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void LgammaCompute(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{



      ;

    LocalTensor<float> tmpBuffer = tmp.ReinterpretCast<float>();
    uint32_t splitSize = tmpBuffer.GetSize();
    if constexpr (isReuseSource) {
        splitSize = splitSize / FLOAT_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitSize = splitSize / FLOAT_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
                                                                                                        ;


    LGammaFParams params;
    LGammaInitFParams<isReuseSource>(tmpBuffer, splitSize, src, params);

    const uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<float>(0, splitSize);
    for (uint32_t i = 0U; i < loopCount; ++i) {
        LgammaComputeImpl(dst[i * splitSize], src[i * splitSize], params);
    }
    if (calcTail > 0) {
        calcTail = (calcTail + ONE_BYTE_BIT_SIZE - 1U) / ONE_BYTE_BIT_SIZE * ONE_BYTE_BIT_SIZE;
        SetVectorMask<float>(0, calcTail);
        params.splitSize = calcTail;
        LgammaComputeImpl(dst[loopCount * splitSize], src[loopCount * splitSize], params);
    }
    SetMaskNorm();
    ResetMask();
}
}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Lgamma(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LgammaCompute<isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/lgamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Lgamma(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;
    LgammaCompute<isReuseSource>(dstTensor, srcTensor, tmp, calCount);
}
#pragma end_pipe
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h" 1
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_v220_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void DigammaCast(const LocalTensor<float> &dst, const LocalTensor<float> &src, RoundMode castType)
{
    Cast<float, float, false>(dst, src, castType, MASK_PLACEHOLDER, 1,
                              {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h" 2






namespace AscendC {
namespace {
constexpr float MIN_NEG_WITH_FLOAT = -8388608.0;
constexpr float DIGAMMA_PI = 3.141592653589793238f;
constexpr float DIGAMMA_NEG_PI = -3.141592653589793238f;
constexpr uint32_t DIGAMMA_FLOAT_NOREUSE_CALC_PROC = 7;
constexpr uint32_t DIGAMMA_FLOAT_REUSE_CALC_PROC = 6;
constexpr uint32_t DIGAMMA_HALF_CALC_PROC = 8;
constexpr size_t DIGAMMA_MAX_LOOP = 5;

constexpr float posCalcConst[] = {2.10927960927960927961e-2, 7.57575757575757575758e-3, 4.16666666666666666667e-3,
                                  3.96825396825396825397e-3, 8.33333333333333333333e-3, 8.33333333333333333333e-2};
constexpr float tmp1CalcConst[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};
constexpr float tmp1HalfCalcConst[] = {1.0, 2.0};
constexpr float picotCalcConst[] = {0.00326538085938f, 0.0242919921875f, 0.053466796875f,
                                    0.13337790966f, 0.333332300186f};
}

struct DigammaParams {
    [aicore] DigammaParams() {}
    LocalTensor<float> result;
    LocalTensor<float> tmpCal1;
    LocalTensor<float> tmpCal2;
    LocalTensor<float> tmpCal3;
    LocalTensor<float> tmpCal4;
    LocalTensor<float> tmpCal5;
    LocalTensor<float> tmpScalar;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> mask1;
    LocalTensor<uint8_t> mask2;
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    uint32_t splitSize;
};

#pragma begin_pipe(V)


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenCompareMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                             DigammaParams &params, const float scalar, CMPMODE cmpMode)
{
    Duplicate<float, false>(params.tmpScalar, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmpScalar, cmpMode,
                                   MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenNegIntMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                            DigammaParams &params, const float scalar)
{

    DigammaGenCompareMask(params.mask1, src, params, 0.0f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, MIN_NEG_WITH_FLOAT, CMPMODE::GT);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask1.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);

    DigammaCast(params.tmpCal1, src, RoundMode::CAST_ROUND);
    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(params.mask2, src, params.tmpCal1, CMPMODE::EQ,
                                   MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenRangeMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                           DigammaParams &params, const float min, const float max)
{

    DigammaGenCompareMask(params.mask1, src, params, max, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, min, CMPMODE::GE);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaGenNanMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                         DigammaParams &params)
{
    DigammaGenCompareMask(params.mask1, src, params, 0.0f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, 0.0f, CMPMODE::GE);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.mask1.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    Not<uint16_t, false>(params.mask2.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaSelect(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                     const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmp,
                                     DigammaParams &params)
{
    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetCmpMask<float>(params.tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(tmp, mask, src, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dst, tmp, dst, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaPositiveHalf(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Adds<float, false>(params.tmpCal1, src, 3.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmpScalar, params.tmpCal1, 0.5f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal1, 0.0833333333333333f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal2, 0.0083333333333333f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal2, params.tmpCal2, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal2, 0.003968253968254f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);

    constexpr size_t calcSize = 2;
    for (size_t i = 0U; i < calcSize; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1HalfCalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Sub<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaNegativeHalf(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmpCal5, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    DigammaPositiveHalf(dst, params.tmpCal5, params);


    Adds<float, false>(params.tmpCal2, dst, 0.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, dst, params.tmpCal3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaNegativeRange(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                            DigammaParams &params)
{

    DigammaCast(params.tmpScalar, src, RoundMode::CAST_FLOOR);
    Sub<float, false>(params.tmpScalar, src, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpScalar, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    CosCompute<float>(params.tmpCal3, params.tmpScalar, params.result, params.splitSize, true);


    Muls<float, false>(src, src, DIGAMMA_NEG_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SinCompute<float>(params.tmpScalar, src, params.result, params.splitSize, true);


    Muls<float, false>(params.tmpCal3, params.tmpCal3, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal3, params.tmpCal3, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, params.tmpCal2, params.tmpCal3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DigammaComputeImpl(const LocalTensor<half> &dst, const LocalTensor<half> &src,
                                          DigammaParams &params)
{

    Cast<float, half, false>(params.tmpCal5, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();


    DigammaCast(params.tmpCal4, params.tmpCal5, RoundMode::CAST_FLOOR);
    Sub<float, false>(params.tmpCal4, params.tmpCal5, params.tmpCal4, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpCal4, params.tmpCal4, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    TanCompute<float>(params.tmpScalar, params.tmpCal4, params.result.ReinterpretCast<uint8_t>(), params.splitSize);

    Duplicate<float, false>(params.tmpCal1, DIGAMMA_PI, MASK_PLACEHOLDER, 1,
                            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal3, params.tmpCal1, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(params.tmpCal4, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    NotNumUnion notNum;
    notNum.i = F32_NAN;
    Duplicate<float, false>(params.result, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, MIN_NEG_WITH_FLOAT, CMPMODE::LE);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);


    DigammaGenNegIntMask(params.mask1, params.tmpCal5, params, MIN_NEG_WITH_FLOAT);
    DigammaSelect(params.tmpCal4, params.result, params.mask1, params.tmpCal1, params);


    DigammaGenNanMask(params.mask, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.tmpCal5, params.mask, params.tmpCal1, params);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, 0.0f, CMPMODE::GE);
    DigammaPositiveHalf(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, -0.0001f, CMPMODE::LT);
    DigammaNegativeHalf(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);

    Cast<float, half, false>(params.tmpCal5, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    DigammaGenRangeMask(params.mask, params.tmpCal5, params, -0.0001f, 0.0f);
    DigammaNegativeRange(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);

    Cast<half, float, false>(dst, params.tmpCal4, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void DigammaInitParams(const LocalTensor<float> &tmp, const uint32_t &splitSize,
                                         const LocalTensor<half> &src, DigammaParams &params)
{
    params.result = tmp;
    params.tmpCal1 = params.result[splitSize];
    params.tmpCal2 = params.tmpCal1[splitSize];
    params.tmpCal3 = params.tmpCal2[splitSize];
    params.tmpCal4 = params.tmpCal3[splitSize];
    params.tmpCal5 = params.tmpCal4[splitSize];
    params.tmpScalar = params.tmpCal5[splitSize];
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.mask1 = params.mask[splitSize];
    params.mask2 = params.mask1[splitSize];


    params.result.SetSize(splitSize * 4);
    params.tmpCal1.SetSize(splitSize);
    params.tmpCal2.SetSize(splitSize);
    params.tmpCal3.SetSize(splitSize);
    params.tmpCal4.SetSize(splitSize);
    params.tmpCal5.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.mask1.SetSize(splitSize);
    params.mask2.SetSize(splitSize);

    params.splitSize = splitSize;
}
# 404 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/digamma/digamma_common_impl.h"
[aicore] __inline__ __attribute__((always_inline)) void DigammaPositiveTmp0(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Adds<float, false>(params.tmpCal1, src, 10.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmpCal2, params.tmpCal1, 0.5f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);


    Duplicate<float, false>(params.tmpCal2, 8.33333333333333333333e-2, MASK_PLACEHOLDER, 1,
                            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    for (size_t i = 0U; i < DIGAMMA_MAX_LOOP; ++i) {
        Duplicate<float, false>(params.tmpScalar, posCalcConst[i], MASK_PLACEHOLDER, 1,
                                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


        Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Sub<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    constexpr size_t calcSize = 6;
    for (size_t i = DIGAMMA_MAX_LOOP; i < calcSize; ++i) {
        Duplicate<float, false>(params.tmpScalar, posCalcConst[i], MASK_PLACEHOLDER, 1,
                                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


        Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
        Sub<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}




[aicore] __inline__ __attribute__((always_inline)) void DigammaPositiveTmp1(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dst, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);

    for (size_t i = 0U; i < DIGAMMA_MAX_LOOP; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1CalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    constexpr size_t calcSize = 9;
    for (size_t i = DIGAMMA_MAX_LOOP; i < calcSize; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1CalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaPositive(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                       DigammaParams &params)
{

    DigammaPositiveTmp0(dst, src, params);


    DigammaPositiveTmp1(params.tmpCal1, src, params);


    Sub<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DigammaNegPicotPix(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                          DigammaParams &params)
{

    Add<float, false>(params.tmpCal1, src, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    DigammaCast(params.tmpCal2, params.tmpCal1, RoundMode::CAST_ROUND);


    Sub<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpCal1, params.tmpCal1, 1.5707963267948966f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<int32_t, float, false>(params.tmpCal2.ReinterpretCast<int32_t>(), params.tmpCal2, RoundMode::CAST_ROUND,
                                MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<int32_t, false>(params.tmpCal3.ReinterpretCast<int32_t>(), 1, MASK_PLACEHOLDER, 1,
                              DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize * (sizeof(float) / sizeof(uint16_t)));
    And<uint16_t, false>(params.tmpCal2.ReinterpretCast<uint16_t>(), params.tmpCal2.ReinterpretCast<uint16_t>(),
            params.tmpCal3.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    Cast<float, int32_t, false>(params.tmpCal2, params.tmpCal2.ReinterpretCast<int32_t>(), RoundMode::CAST_NONE,
                                MASK_PLACEHOLDER, 1, params.unaryParams);
    DigammaGenCompareMask(params.mask1, params.tmpCal2, params, 0.5f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, params.tmpCal2, params, 0.5f, CMPMODE::GE);


    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dst, 0.0093383789065f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    constexpr size_t calcSize = 5;
    for (size_t i = 0U; i < calcSize; ++i) {
        Mul<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Adds<float, false>(dst, dst, picotCalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();
    }

    Mul<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmpCal1, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    DigammaSelect(dst, params.tmpCal1, params.mask2, params.tmpCal3, params);


    Duplicate<float, false>(params.tmpScalar, -1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    DigammaSelect(dst, params.tmpCal1, params.mask1, params.tmpCal3, params);


    Muls<float, false>(dst, dst, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] __inline__ __attribute__((always_inline)) void DigammaNegative(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                       DigammaParams &params)
{

    Muls<float, false>(params.tmpCal3, src, -1.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmpCal3, params.tmpCal3, 1.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    DigammaPositive(dst, params.tmpCal3, params);


    DigammaNegPicotPix(params.tmpCal4, src, params);


    Add<float, false>(dst, dst, params.tmpCal4, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void DigammaComputeImpl(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                          DigammaParams &params)
{
    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    NotNumUnion notNum;
    notNum.i = F32_NAN;
    Duplicate<float, false>(params.result, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    DigammaGenCompareMask(params.mask, src, params, MIN_NEG_WITH_FLOAT, CMPMODE::LE);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);


    DigammaGenNegIntMask(params.mask1, src, params, MIN_NEG_WITH_FLOAT);
    DigammaSelect(dst, params.result, params.mask1, params.tmpCal3, params);


    DigammaGenNanMask(params.mask, src, params);
    DigammaSelect(dst, src, params.mask, params.tmpCal3, params);


    DigammaGenCompareMask(params.mask, src, params, 0.0f, CMPMODE::GE);
    DigammaPositive(params.result, src, params);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);


    DigammaGenCompareMask(params.mask, src, params, 0.0f, CMPMODE::LT);
    DigammaNegative(params.result, src, params);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);
}

template <bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void DigammaInitParams(const LocalTensor<float> &tmp, const uint32_t &splitSize,
                                         const LocalTensor<float> &src, DigammaParams &params)
{
    params.result = tmp;
    params.tmpCal1 = tmp[splitSize];
    params.tmpCal2 = params.tmpCal1[splitSize];
    params.tmpCal3 = params.tmpCal2[splitSize];
    if constexpr (isReuseSource) {
        params.tmpCal4 = src;
        params.tmpScalar = params.tmpCal3[splitSize];
    } else {
        params.tmpCal4 = params.tmpCal3[splitSize];
        params.tmpScalar = params.tmpCal4[splitSize];
    }
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.mask1 = params.mask[splitSize];
    params.mask2 = params.mask1[splitSize];

    params.result.SetSize(splitSize);
    params.tmpCal1.SetSize(splitSize);
    params.tmpCal2.SetSize(splitSize);
    params.tmpCal3.SetSize(splitSize);
    params.tmpCal4.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.mask1.SetSize(splitSize);
    params.mask2.SetSize(splitSize);

    params.splitSize = splitSize;
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void DigammaCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
                                      const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{



      ;

    LocalTensor<float> tmpBuffer = tmp.ReinterpretCast<float>();
    uint32_t splitSize = tmpBuffer.GetSize();

    if (sizeof(T) == sizeof(float)) {
        if constexpr (isReuseSource) {
            splitSize = splitSize / DIGAMMA_FLOAT_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            splitSize = splitSize / DIGAMMA_FLOAT_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    } else {
        splitSize = splitSize / DIGAMMA_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

                                                                                                        ;


    DigammaParams params;
    DigammaInitParams<isReuseSource>(tmpBuffer, splitSize, src, params);

    const uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<T>(0, splitSize);
    uint32_t offset = 0;
    for (uint32_t i = 0U; i < loopCount; ++i) {
        DigammaComputeImpl(dst[offset], src[offset], params);
        offset += splitSize;
    }

    if (calcTail > 0) {
        calcTail = (calcTail + ONE_BYTE_BIT_SIZE - 1U) / ONE_BYTE_BIT_SIZE * ONE_BYTE_BIT_SIZE;
        SetVectorMask<T>(0, calcTail);
        params.splitSize = calcTail;
        DigammaComputeImpl(dst[offset], src[offset], params);
    }
    SetMaskNorm();
    ResetMask();
}

#pragma end_pipe
}
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Digamma(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                               LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DigammaCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/digamma.h"
template<typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Digamma(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;
    DigammaCompute<T, isReuseSource>(dstTensor, srcTensor, tmp, calCount);
}
#pragma end_pipe
}
# 77 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h" 2

#pragma begin_pipe(V)
namespace AscendC {
constexpr uint32_t SIGN_CALC_PROC = 3;
constexpr uint32_t SIGN_BIT = 8;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SignComputeImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &tmpBuffer1, const LocalTensor<uint8_t> &tmpBuffer2, const LocalTensor<T> &tmpBuffer3,
    const LocalTensor<T> &tmpBuffer4, uint32_t calCount, uint32_t repeatTimes)
{
    BinaryRepeatParams binaryParams;

    Duplicate<T, false>(dstTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();





    Compare<T, uint8_t, false>(
        tmpBuffer1, srcTensor, dstTensor, CMPMODE::LT, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    Compare<T, uint8_t, false>(
        tmpBuffer2, srcTensor, dstTensor, CMPMODE::GT, MASK_PLACEHOLDER, repeatTimes, binaryParams);





    Duplicate<T, false>(tmpBuffer3, static_cast<T>(1), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask<T>(tmpBuffer3);
    Select<T, uint8_t>(tmpBuffer4, tmpBuffer1, dstTensor, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<T, false>(tmpBuffer3, static_cast<T>(-1), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask<T>(tmpBuffer3);
    Select<T, uint8_t>(dstTensor, tmpBuffer2, dstTensor, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(dstTensor, tmpBuffer4, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void SignCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(T);
    splitCount = splitCount / SIGN_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/sign/sign_common_impl.h"
    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    __attribute__((cce_unif_buff)) uint8_t *tmpBuffer1 = (__attribute__((cce_unif_buff)) uint8_t *)sharedTmpBuffer.GetPhyAddr();
    uint32_t tmpLen = AlignUp(splitCount / SIGN_BIT, ONE_BLK_SIZE);
    __attribute__((cce_unif_buff)) uint8_t *tmpBuffer2 = tmpBuffer1 + tmpLen;
    LocalTensor<T> stackTensor = sharedTmpBuffer[tmpLen * 2].ReinterpretCast<T>();
    __attribute__((cce_unif_buff)) T *tmpBuffer3 = (__attribute__((cce_unif_buff)) T *)stackTensor.GetPhyAddr();
    __attribute__((cce_unif_buff)) T *tmpBuffer4 = tmpBuffer3 + splitCount;

    uint32_t offset = 0;
    uint32_t repeatTimes = (splitCount * sizeof(T) + ONE_REPEAT_BYTE_SIZE - 1) / ONE_REPEAT_BYTE_SIZE;
    for (uint32_t i = 0; i < loopCount; ++i) {
        SignComputeImpl(dstTensor[offset], srcTensor[offset],
            sharedTmpBuffer, sharedTmpBuffer[tmpLen], stackTensor, stackTensor[splitCount], splitCount, repeatTimes);
        offset = offset + splitCount;
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        repeatTimes = (calcTail * sizeof(T) + ONE_REPEAT_BYTE_SIZE - 1) / ONE_REPEAT_BYTE_SIZE;
        SignComputeImpl(dstTensor[offset], srcTensor[offset],
            sharedTmpBuffer, sharedTmpBuffer[tmpLen], stackTensor, stackTensor[splitCount], calcTail, repeatTimes);
    }
    SetMaskNorm();
    ResetMask();
}
}
#pragma end_pipe
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h" 2



#pragma begin_pipe(V)
namespace AscendC {
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SignCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sign<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Sign<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    Sign<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
}
#pragma end_pipe
# 78 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h" 2





namespace AscendC {
constexpr uint32_t HALF_NUM_PER = 128;
constexpr uint32_t FLOAT_NUM_PER = 64;
struct MeanParams {
    uint32_t outter = 1;
    uint32_t inner;
    uint32_t n;
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CheckParamsIsValid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams, uint32_t tmpBufferSize)
{
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/mean/mean_common_impl.h"
}

[aicore] __inline__ __attribute__((always_inline)) void MeanCast(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams)
{
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
    uint32_t finalWorkSize =
        meanParams.inner * sizeof(float) + (repeateTimes + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE * ONE_BLK_SIZE;



    const UnaryRepeatParams unaryParams;
    float scalarValue = static_cast<float>(1) / static_cast<float>(static_cast<int32_t>(meanParams.n));
    LocalTensor<float> TmpTensor = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<half> castTensor = sharedTmpBuffer.ReinterpretCast<half>();
    SetMaskCount();
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        SetVectorMask<half>(0, meanParams.n);
        Cast<float, half, false>(TmpTensor, srcTensor[row * meanParams.inner], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                                 1, {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        RepeatReduceSum<float, false>(TmpTensor[meanParams.inner], TmpTensor, 1,
                                      MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                      DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        uint32_t reduceNums = repeateTimes;
        while (reduceNums > 1) {
            SetVectorMask<half>(0, reduceNums);
            reduceNums = (reduceNums + elementNumPerRep - 1) / elementNumPerRep;
            RepeatReduceSum<float, false>(TmpTensor[meanParams.inner], TmpTensor[meanParams.inner], 1,
                                      MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                      DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);

            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<half>(0, 1);
        Muls<float, false>(TmpTensor[meanParams.inner], TmpTensor[meanParams.inner],
                       scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(castTensor, TmpTensor[meanParams.inner], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                                 1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        RepeatReduceSum<half, false>(dstTensor[row], castTensor, 1, MASK_PLACEHOLDER,
                                     DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MeanForOneRepeatTime(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                            const MeanParams& meanParams, T scalarValue)
{
    SetVectorMask<T>(0, meanParams.n);
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        RepeatReduceSum<T, false>(dstTensor[row], srcTensor[row * meanParams.inner], 1, MASK_PLACEHOLDER,
                                  DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<T>(0, meanParams.outter);
    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstTensor, dstTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MeanCommon(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams)
{
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    if constexpr (sizeof(T) == sizeof(half)) {
        elementNumPerRep = HALF_NUM_PER;
    }
    uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
    uint32_t finalWorkSize = (repeateTimes + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE * ONE_BLK_SIZE;



    T scalarValue = static_cast<T>(static_cast<float>(1) / static_cast<float>(static_cast<int32_t>(meanParams.n)));
    SetMaskCount();
    if (repeateTimes == 1) {
        return MeanForOneRepeatTime(dstTensor, srcTensor, meanParams, scalarValue);
    }
    const UnaryRepeatParams unaryParams;
    LocalTensor<T> TmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        uint32_t reduceNums = repeateTimes;
        SetVectorMask<T>(0, meanParams.n);
        RepeatReduceSum<T, false>(TmpTensor,
            srcTensor[row * meanParams.inner],
            1,
            MASK_PLACEHOLDER,
            DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        while (reduceNums > 1) {
            SetVectorMask<T>(0, reduceNums);
            reduceNums = (reduceNums + elementNumPerRep - 1) / elementNumPerRep;
            if (reduceNums == 1) {
                RepeatReduceSum<T, false>(dstTensor[row], TmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                          DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            } else {
                RepeatReduceSum<T, false>(TmpTensor, TmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                          DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();
        }
    }
    SetVectorMask<T>(0, meanParams.outter);
    Muls<T, false>(dstTensor, dstTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
}

#pragma end_pipe
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h"
template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] __inline__ __attribute__((always_inline)) void Mean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                            const LocalTensor<uint8_t> &sharedTmpBuffer, const MeanParams &meanParams)
{
    if constexpr(g_coreType == AscendC::AIC)
    {
        return;
    }





                                                                                      ;
    if constexpr (sizeof(T) == sizeof(half) && sizeof(accType) == sizeof(float))
    {
        MeanCast(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
    }
    else
    {
        MeanCommon(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
    }
}
# 77 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/mean.h"
template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] __inline__ __attribute__((always_inline)) void Mean(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const MeanParams &meanParams)
{
    if constexpr(g_coreType == AscendC::AIC)
    {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Mean<T, accType>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
}
#pragma end_pipe
}
# 79 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h" 1
# 18 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/axpy/axpy_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/axpy/axpy_common_impl.h"
namespace AscendC {
 template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor,
    const U& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
                                                                                               ;
}





 template <>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const half& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
    LocalTensor<float> tmpSrc = stackBuffer[0];
    LocalTensor<float> tmpDst = stackBuffer[stackSize];

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Cast<float, half, false>(tmpSrc, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpDst, dstTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpSrc, tmpSrc, (float)scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tmpDst, tmpSrc, tmpDst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dstTensor, tmpDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}




 template <>
[aicore] __inline__ __attribute__((always_inline)) void AxpyIntrinsicsImpl(const LocalTensor<float>& dstTensor, const LocalTensor<half>& srcTensor,
    const half& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Cast<float, half, false>(stackBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<float, false>(stackBuffer, stackBuffer, (float)scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, stackBuffer, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t axpyTmpCalc(uint32_t tmpBufferSize)
{
    uint32_t stackSize = tmpBufferSize;
    if constexpr (sizeof(T) == sizeof(half)) {
        stackSize = tmpBufferSize / 2 / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        stackSize = tmpBufferSize / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }


      ;
    return stackSize;
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void AxpySub(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U& scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{


                                                                                                        ;
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();

    uint32_t stackSize = axpyTmpCalc<T>(tmpBufferSize);

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AxpyIntrinsicsImpl(dstTensor[offset], srcTensor[offset], scalarValue, tmpBuffer, stackSize);
        offset = offset + stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AxpyIntrinsicsImpl(dstTensor[offset], srcTensor[offset], scalarValue, tmpBuffer, stackSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, typename U, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void AxpyImpl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
# 146 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/axpy/axpy_common_impl.h"
    if constexpr (sizeof(U) == sizeof(float)) {
        Axpy<T, U>(dstTensor, srcTensor, scalarValue, calCount);
    } else {
        AxpySub<T, U, isReuseSource>(dstTensor, srcTensor, scalarValue, sharedTmpBuffer, calCount);
    }
}

}
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/axpy.h"
template <typename T, typename U, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AxpyImpl<T, U, isReuseSource>(dstTensor, srcTensor, scalarValue, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 80 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_common_impl.h" 1
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_v220_impl.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_v220_impl.h"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void CeilProcess(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &tmpTensor)
{
    (void)tmpTensor;
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_CEIL, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] __inline__ __attribute__((always_inline)) void CeilProcess(const LocalTensor<half> &dstTensor, const LocalTensor<half> &srcTensor,
    const LocalTensor<uint8_t> &tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();



    Cast<float, half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    CeilProcess(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/ceil/ceil_common_impl.h" 2





namespace AscendC {

constexpr uint32_t CEIL_HALF_CALC_PROCEDURE = 2;
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CeilImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    uint32_t splitCount = sharedTmpBuffer.GetSize() / sizeof(T);
    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / CEIL_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
                                                                                           ;

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        CeilProcess(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T>(0, calcTail);
        CeilProcess(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void CeilImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CeilImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Ceil(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CeilImpl(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 59 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/ceil.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void Ceil(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CeilImpl(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_utils.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_utils.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_utils.h" 2

namespace AscendC {
constexpr uint32_t ONE_VOR_BLOCK_DIM = 8;
constexpr uint32_t ELEMENT_NUM_FOR_UINT16 = 16;
constexpr int32_t FLOAT_ELEMENT_NUM = 2;
constexpr uint32_t REPEAT_STRIDE_NUM = 8;
constexpr uint32_t MAX_REPEAT_NUM = 255;

template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastDimAlign(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &zeroTemp, const uint32_t firstDim, const uint32_t blockDim)
{
    int32_t dtypeCount = 1;
    if constexpr (sizeof(T) == sizeof(float)) {
        dtypeCount = FLOAT_ELEMENT_NUM;
    }
    uint32_t orCounts = firstDim / ONE_VOR_BLOCK_DIM;
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    uint8_t repeateTimes = blockDim / oneBlockElementNum;
    SetMaskNorm();
    SetVectorMask<uint16_t, MaskMode::NORMAL>(ONE_VOR_BLOCK_DIM * ELEMENT_NUM_FOR_UINT16);
    uint8_t dstBlkStride = blockDim * dtypeCount / ELEMENT_NUM_FOR_UINT16;
    BinaryRepeatParams binaryParams(dstBlkStride, 0, 0, 1, 1, 0);
    uint32_t transTmpBufferOffset = 0;
    for (uint32_t i = 0; i < orCounts; i++) {
        Or<uint16_t, false>(dstLocal[transTmpBufferOffset].template ReinterpretCast<uint16_t>(),
            srcLocal.template ReinterpretCast<uint16_t>(),
            zeroTemp.template ReinterpretCast<uint16_t>(),
            MASK_PLACEHOLDER,
            repeateTimes,
            binaryParams);
        transTmpBufferOffset += ONE_VOR_BLOCK_DIM * blockDim;
    }
    uint32_t orCountsTail = firstDim - orCounts * ONE_VOR_BLOCK_DIM;
    if (orCountsTail > 0) {
        SetMaskNorm();
        SetVectorMask<uint16_t, MaskMode::NORMAL>(orCountsTail * ELEMENT_NUM_FOR_UINT16);
        Or<uint16_t, false>(dstLocal[transTmpBufferOffset].template ReinterpretCast<uint16_t>(),
                            srcLocal.template ReinterpretCast<uint16_t>(),
                            zeroTemp.template ReinterpretCast<uint16_t>(),
                            MASK_PLACEHOLDER,
                            repeateTimes,
                            binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoopBroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &zeroTemp, const uint32_t firstDim, const uint32_t blockDim)
{
    int32_t dtypeCount = 1;
    if constexpr (sizeof(T) == sizeof(float)) {
        dtypeCount = FLOAT_ELEMENT_NUM;
    }
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(firstDim * dtypeCount);
    BinaryRepeatParams binaryParams(1, 1, 0, REPEAT_STRIDE_NUM, REPEAT_STRIDE_NUM, 0);
    uint32_t temBufferOffset = 0;
    for (uint32_t i = 0; i < blockDim; i++) {
        Or<uint16_t, false>(dstLocal[temBufferOffset].template ReinterpretCast<uint16_t>(),
            srcLocal.template ReinterpretCast<uint16_t>(),
            zeroTemp.template ReinterpretCast<uint16_t>(),
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        temBufferOffset += firstDim;
    }
    PipeBarrier<PIPE_V>();
}

}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_v220_impl.h" 2

namespace AscendC {
constexpr uint32_t BRCB_ONE_SIZE = 8;
constexpr uint32_t BRCB_HALF_MAX_REPEATE_TIMES = 254;
constexpr uint32_t BRCB_FLOAT_MAX_REPEATE_TIMES = 255;
constexpr uint8_t GATHER_MASK_PATTERN = 7;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BrcbToOneBlock(const LocalTensor<T> &srcLocal, const uint32_t firstDim,
    uint32_t oneBlockElementNum, LocalTensor<T> &brcbOneBlockTempBuffer)
{
    const uint32_t brcbRepeatTime = (firstDim + BRCB_ONE_SIZE - 1) / BRCB_ONE_SIZE;
    uint32_t brcbMaxRepeateTimes = BRCB_HALF_MAX_REPEATE_TIMES;
    if constexpr (sizeof(T) == sizeof(float)) {
        brcbMaxRepeateTimes = BRCB_FLOAT_MAX_REPEATE_TIMES;
    }
    const uint32_t brcbCount = brcbRepeatTime / brcbMaxRepeateTimes;
    const uint32_t tailBrcbRepeateTime = brcbRepeatTime % brcbMaxRepeateTimes;
    uint32_t brcbSrcOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    for (uint32_t i = 0; i < brcbCount; i++) {
        Brcb(brcbOneBlockTempBuffer[brcbOneBlockTempBufferOffset],
            srcLocal[brcbSrcOffset],
            brcbMaxRepeateTimes,
            {1, DEFAULT_REPEAT_STRIDE});
        brcbOneBlockTempBufferOffset += brcbMaxRepeateTimes * oneBlockElementNum * BRCB_ONE_SIZE;
        brcbSrcOffset += brcbMaxRepeateTimes * BRCB_ONE_SIZE;
    }
    if (tailBrcbRepeateTime != 0) {
        Brcb(brcbOneBlockTempBuffer[brcbOneBlockTempBufferOffset],
            srcLocal[brcbSrcOffset],
            tailBrcbRepeateTime,
            {1, DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastLastDimAlign220(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    LocalTensor<T> &tmpBuffer, const uint32_t firstDim, const uint32_t blockDim)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    BrcbToOneBlock(srcLocal, firstDim, oneBlockElementNum, tmpBuffer);
    SetVectorMask<T, MaskMode::COUNTER>(blockDim);
    const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)(blockDim / oneBlockElementNum), 1};
    uint32_t CopyCounts = firstDim / MAX_REPEAT_TIMES;
    uint32_t dstOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    for (uint32_t i = 0; i < CopyCounts; i++) {
        Copy<T, false>(dstLocal[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            MAX_REPEAT_TIMES,
            copyRepeatParams);
        dstOffset += MAX_REPEAT_TIMES * blockDim;
        brcbOneBlockTempBufferOffset += MAX_REPEAT_TIMES * oneBlockElementNum;
    }
    uint32_t tailsCopyRepeateTimes = firstDim % MAX_REPEAT_TIMES;
    if (tailsCopyRepeateTimes != 0) {
        Copy<T, false>(dstLocal[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            tailsCopyRepeateTimes,
            copyRepeatParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastLastDimNotAlign220(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    LocalTensor<T> &tmpBuffer, const uint32_t firstDim, const uint32_t blockDim)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    BrcbToOneBlock(srcLocal, firstDim, oneBlockElementNum, tmpBuffer);
    const uint32_t blockDimAlignBlockNum = (blockDim + oneBlockElementNum - 1) / oneBlockElementNum;
    const uint32_t blockDimAlign = blockDimAlignBlockNum * oneBlockElementNum;
    SetVectorMask<T, MaskMode::COUNTER>(blockDimAlign);
    const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)blockDimAlignBlockNum, 1};
    uint32_t CopyCounts = firstDim / MAX_REPEAT_TIMES;
    uint32_t dstOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    auto copyTempBuffer = tmpBuffer[firstDim * oneBlockElementNum];
    for (uint32_t i = 0; i < CopyCounts; i++) {
        Copy<T, false>(copyTempBuffer[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            MAX_REPEAT_TIMES,
            copyRepeatParams);
        dstOffset += MAX_REPEAT_TIMES * blockDimAlign;
        brcbOneBlockTempBufferOffset += MAX_REPEAT_TIMES * oneBlockElementNum;
    }
    uint32_t tailsCopyRepeateTimes = firstDim % MAX_REPEAT_TIMES;
    if (tailsCopyRepeateTimes != 0) {
        Copy<T, false>(copyTempBuffer[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            tailsCopyRepeateTimes,
            copyRepeatParams);
    }
    PipeBarrier<PIPE_V>();
    const GatherMaskParams gatherMaskParams = {
        1, (uint16_t)firstDim, (uint16_t)blockDimAlignBlockNum, 0};
    uint64_t rsvdCnt = 0;
    GatherMask(dstLocal, copyTempBuffer, GATHER_MASK_PATTERN, true, blockDim, gatherMaskParams, rsvdCnt);
    SetMaskCount();
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetAlignLoopNumbers(const uint32_t firstDim, const uint32_t blockDim,
    const uint32_t tmpBufferSize, uint32_t &oneRepeateSize, uint32_t &rangeM, uint32_t &tailM)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t minBrcbTempBufferSize = oneBlockElementNum * oneBlockElementNum;
    constexpr uint32_t minTmpBufferSize = minBrcbTempBufferSize;





      ;
    oneRepeateSize = tmpBufferSize / minTmpBufferSize * oneBlockElementNum;
    rangeM = firstDim / oneRepeateSize;
    tailM = firstDim - oneRepeateSize * rangeM;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetNotAlignLoopNumbers(const uint32_t firstDim, const uint32_t blockDim,
    const uint32_t tmpBufferSize, uint32_t &oneRepeateSize, uint32_t &rangeM, uint32_t &tailM)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t minBrcbTempBufferSize = oneBlockElementNum * oneBlockElementNum;
    const uint32_t blockDimAlignBlockNum = (blockDim + oneBlockElementNum - 1) / oneBlockElementNum;
    const uint32_t blockDimAlign = blockDimAlignBlockNum * oneBlockElementNum;
    const uint32_t minCopyTempBufferSize = oneBlockElementNum * blockDimAlign;
    const uint32_t minTmpBufferSize = minBrcbTempBufferSize + minCopyTempBufferSize;





      ;
    oneRepeateSize = tmpBufferSize / minTmpBufferSize * oneBlockElementNum;
    rangeM = firstDim / oneRepeateSize;
    tailM = firstDim - oneRepeateSize * rangeM;
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastLastDim(LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    const auto firstDim = dstShape[0];
    const auto blockDim = dstShape[axis];
    uint32_t oneRepeateSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t dstLocalOffset = 0;
    uint32_t srcLocalOffset = 0;
    if (blockDim * sizeof(T) % ONE_BLK_SIZE == 0) {
        GetAlignLoopNumbers<T>(firstDim, blockDim, tmpBuffer.GetSize(), oneRepeateSize, rangeM, tailM);
        for (uint32_t i = 0; i < rangeM; i++) {
            TwoDimBroadCastLastDimAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, oneRepeateSize, blockDim);
            dstLocalOffset += oneRepeateSize * blockDim;
            srcLocalOffset += oneRepeateSize;
        }

        if (tailM != 0) {
            TwoDimBroadCastLastDimAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, tailM, blockDim);
        }
    } else {
        GetNotAlignLoopNumbers<T>(firstDim, blockDim, tmpBuffer.GetSize(), oneRepeateSize, rangeM, tailM);
        for (uint32_t i = 0; i < rangeM; i++) {
            TwoDimBroadCastLastDimNotAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, oneRepeateSize, blockDim);
            dstLocalOffset += oneRepeateSize * blockDim;
            srcLocalOffset += oneRepeateSize;
        }
        if (tailM != 0) {
            TwoDimBroadCastLastDimNotAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, tailM, blockDim);
        }
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void NoBroad(LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t size)
{
    SetVectorMask<T, MaskMode::COUNTER>(size);
    Copy<T, false>(dstLocal, srcLocal, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

}
# 23 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/../../impl/pad/broadcast/broadcast_common_impl.h" 2





namespace AscendC {
constexpr uint32_t TWO_DIM = 2;

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void TwoDimBroadCastFirstDim(LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    const uint32_t firstDim = dstShape[0];
    const uint32_t blockDim = dstShape[1];

                                                                                                                     ;

    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t FIRST_DIM_LOOP_LIMITE = MAX_REPEAT_NUM * oneBlockElementNum;

    auto zeroTemp = tmpBuffer;
    Duplicate(zeroTemp.template ReinterpretCast<uint16_t>(), (uint16_t)0, ONE_BLK_SIZE / sizeof(uint16_t));
    PipeBarrier<PIPE_V>();

    if (blockDim >= FIRST_DIM_LOOP_LIMITE) {
        LoopBroadCast<T>(dstLocal, srcLocal, zeroTemp, blockDim, firstDim);
        return;
    }

    TwoDimBroadCastDimAlign<T, isReuseSource>(dstLocal, srcLocal, zeroTemp, firstDim, blockDim);
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void BroadCastCompute(LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    uint32_t srcSize = 1;
    uint32_t dstSize = 1;
    for (uint32_t i = 0; i < dim; i++) {
        srcSize *= srcShape[i];
        dstSize *= dstShape[i];
    }

    if (srcSize == dstSize) {
        NoBroad(dstLocal, srcLocal, dstSize);
    } else if (srcSize == 1) {
        TEventID event1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(event1);
        WaitFlag<HardEvent::V_S>(event1);
        auto scalar = srcLocal.GetValue(0);
        TEventID event2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::S_V>(event2);
        WaitFlag<HardEvent::S_V>(event2);
        Duplicate(dstLocal, scalar, dstSize);
        PipeBarrier<PIPE_V>();
    } else {
        if constexpr (dim == TWO_DIM) {
            if constexpr (axis == 1) {
                TwoDimBroadCastLastDim<T, dim, axis, false>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
            } else {
                TwoDimBroadCastFirstDim<T, dim, axis, false>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
            }
        } else {
                                                                                                   ;
        }
    }
    SetMaskCount();
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h" 2



namespace AscendC {
#pragma begin_pipe(V)
constexpr uint32_t HALF_ONE_BLK_SIZE = 16;
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h"
template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void BroadCast(LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t dstShape[dim],
    const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                   ;
    if constexpr (sizeof(T) == 1) {
        LocalTensor<half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<half>();
        uint32_t srcSize = 1;
        uint32_t dstSize = 1;
        for (uint32_t i = 0; i < dim; i++) {
            srcSize *= srcShape[i];
            dstSize *= dstShape[i];
        }
        auto srcTempBuffer = tmpBuffer;
        const uint32_t alignSrcSize = ((srcSize + HALF_ONE_BLK_SIZE - 1) / HALF_ONE_BLK_SIZE) * HALF_ONE_BLK_SIZE;
        const uint32_t alignDstSize = ((dstSize + HALF_ONE_BLK_SIZE - 1) / HALF_ONE_BLK_SIZE) * HALF_ONE_BLK_SIZE;
        auto dstTempBuffer = tmpBuffer[alignSrcSize];
        auto tempTempBuffer = dstTempBuffer[alignDstSize];
        SetMaskCount();
        SetVectorMask<T, MaskMode::COUNTER>(srcSize);
        Cast<half, T, false>(srcTempBuffer,
            srcLocal,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

        BroadCastCompute<half, dim, axis, isReuseSource>(
            dstTempBuffer, srcTempBuffer, dstShape, srcShape, tempTempBuffer);
        SetVectorMask<T, MaskMode::COUNTER>(dstSize);
        Cast<T, half, false>(dstLocal,
            dstTempBuffer,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    } else {
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        SetMaskCount();
        BroadCastCompute<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
        SetMaskNorm();
        ResetMask();
    }
                                  ;
}
# 97 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/pad/broadcast.h"
template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void BroadCast(LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t dstShape[dim],
    const uint32_t srcShape[dim])
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, sharedTmpBuffer);
}

#pragma end_pipe
}
# 82 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h" 2

# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 2



namespace AscendC {
namespace {
constexpr uint32_t REDUCE_XOR_SUM_REUSE_CALC_PROC = 2U;
constexpr uint32_t REDUCE_XOR_SUM_NOREUSE_CALC_PROC = 3U;
const uint8_t REDUCE_XOR_SUM_INT16_REPEAT_STRIDE = 4;
}
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/../../impl/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h"
#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceXorSumCompute(LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, LocalTensor<uint8_t>& tmp, const uint32_t calCount)
{




    LocalTensor<T> xorTensor = tmp.ReinterpretCast<T>();
    uint32_t splitSize = xorTensor.GetSize();

    if constexpr (isReuseSource) {
        splitSize = splitSize / REDUCE_XOR_SUM_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitSize = splitSize / REDUCE_XOR_SUM_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }



      ;

    LocalTensor<T> tmpXor = xorTensor[splitSize];
    xorTensor.SetSize(splitSize);
    tmpXor.SetSize(splitSize);

    Xor<T, false>(xorTensor, src0, src1, tmpXor.template ReinterpretCast<uint8_t>(), calCount);

    LocalTensor<float> tmpWork = xorTensor.template ReinterpretCast<float>();
    LocalTensor<float> reduceTensor = tmpXor.template ReinterpretCast<float>();

    SetMaskCount();
    SetVectorMask<T>(0, calCount);

    if constexpr (isReuseSource) {
        Copy<T, false>(src0, xorTensor, MASK_PLACEHOLDER, 1,
            {1, 1, REDUCE_XOR_SUM_INT16_REPEAT_STRIDE, REDUCE_XOR_SUM_INT16_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        reduceTensor = xorTensor.template ReinterpretCast<float>();
        xorTensor = src0;
        tmpWork = src1.template ReinterpretCast<float>();
    }

    Cast<float, T, false>(reduceTensor, xorTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                          {1, 1, DEFAULT_REPEAT_STRIDE, REDUCE_XOR_SUM_INT16_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();

    ReduceSum<float>(reduceTensor, reduceTensor, tmpWork, calCount);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<T>(0, 1);
    Cast<T, float, false>(dst, reduceTensor, RoundMode::CAST_ROUND, MASK_PLACEHOLDER, 1,
                          {1, 1, REDUCE_XOR_SUM_INT16_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}
#pragma end_pipe
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h" 2







namespace AscendC {
#pragma begin_pipe(V)
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceXorSum(LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    ReduceXorSumCompute<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 68 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/reduce/reduce_xor_sum.h"
template <typename T, bool isReuseSource = false>
[aicore] __inline__ __attribute__((always_inline)) void ReduceXorSum(LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
                                    const LocalTensor<T>&src1Tensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;

    ReduceXorSumCompute<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, tmp, calCount);
}
#pragma end_pipe
}
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h" 2


namespace AscendC {
struct CumSumInfo {
    uint32_t outter{0};
    uint32_t inner{0};
};

struct CumSumConfig {
    bool isLastAxis{true};
    bool isReuseSource{false};
    bool outputLastRow{false};
};

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CumSumLastDim(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<T> tempBuffer, const CumSumInfo &cumSumInfo)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    uint16_t alignOutter =
        (cumSumInfo.outter + NCHW_CONV_ADDR_LIST_SIZE - 1) / NCHW_CONV_ADDR_LIST_SIZE * NCHW_CONV_ADDR_LIST_SIZE;
    uint64_t transDataTo5HDDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t transDataTo5HDSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
    if (cumSumInfo.outter == alignOutter && alignOutter > cumSumInfo.inner) {

        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {

            dstRepStride = 1;
            srcRepStride = cumSumInfo.inner;
        }

        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);

        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + alignOutter * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    } else {

        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {
            dstRepStride = alignOutter;
            srcRepStride = 1;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);

        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {

                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[((i * NCHW_CONV_ADDR_LIST_SIZE +
                                             n % (cumSumInfo.outter - i * NCHW_CONV_ADDR_LIST_SIZE)) *
                                            cumSumInfo.inner)]
                        .GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + alignOutter * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(alignOutter * cumSumInfo.inner);
    LocalTensor<float> floatTempBuffer = tempBuffer[alignOutter * cumSumInfo.inner].template ReinterpretCast<float>();
    Cast<float, T, false>(floatTempBuffer,
        tempBuffer,
        RoundMode::CAST_NONE,
        MASK_PLACEHOLDER,
        1,
        {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, alignOutter);
    const BinaryRepeatParams binaryParams;

    for (uint32_t row = 1; row < cumSumInfo.inner; ++row) {
        Add<float, false>(floatTempBuffer[row * alignOutter],
            floatTempBuffer[(row - 1) * alignOutter],
            floatTempBuffer[row * alignOutter],
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<T, MaskMode::COUNTER>(alignOutter * cumSumInfo.inner);
    Cast<T, float, false>(tempBuffer,
        floatTempBuffer,
        RoundMode::CAST_NONE,
        MASK_PLACEHOLDER,
        1,
        {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    auto tempBuffer2 = tempBuffer[alignOutter * cumSumInfo.inner];
    if (alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / oneBlockElementNum;
        if (repeatTimes > 1) {
            dstRepStride = cumSumInfo.inner;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * alignOutter].GetPhyAddr();

                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer2[i * NCHW_CONV_ADDR_LIST_SIZE + n * cumSumInfo.inner].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {
            dstRepStride = alignOutter;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, srcRepStride, dstRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + alignOutter * n].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * cumSumInfo.inner].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<T>(0, cumSumInfo.outter * cumSumInfo.inner);

    Adds<T, false>(
        dstTensor, tempBuffer2, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <>
[aicore] __inline__ __attribute__((always_inline)) void CumSumLastDim(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    LocalTensor<float> tempBuffer, const CumSumInfo &cumSumInfo)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(float);

    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
    uint16_t alignOutter =
        (cumSumInfo.outter + NCHW_CONV_ADDR_LIST_SIZE - 1) / NCHW_CONV_ADDR_LIST_SIZE * NCHW_CONV_ADDR_LIST_SIZE;
    uint64_t transDataTo5HDDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t transDataTo5HDSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    if (cumSumInfo.outter == alignOutter && alignOutter > cumSumInfo.inner) {

        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {

            dstRepStride = 2;
            srcRepStride = cumSumInfo.inner * 2;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {

                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer[(i * oneBlockElementNum + n) * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer[(i * oneBlockElementNum + n) * alignOutter + oneBlockElementNum].GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {

            dstRepStride = alignOutter;
            srcRepStride = 1;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[((i * NCHW_CONV_ADDR_LIST_SIZE +
                                             n % (cumSumInfo.outter - i * NCHW_CONV_ADDR_LIST_SIZE)) *
                                            cumSumInfo.inner)]
                        .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float>(0, alignOutter);
    const BinaryRepeatParams binaryParams;

    uint32_t addOffset = alignOutter;
    for (uint32_t row = 1; row < cumSumInfo.inner; ++row) {
        Add<float, false>(tempBuffer[addOffset],
            tempBuffer[addOffset - alignOutter],
            tempBuffer[addOffset],
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        addOffset += alignOutter;
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();

    auto tempBuffer2 = tempBuffer[alignOutter * cumSumInfo.inner];
    if (alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {

            dstRepStride = cumSumInfo.inner * 2;
            srcRepStride = 2;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, dstRepStride, srcRepStride);

        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + n * alignOutter].GetPhyAddr();
                transDataTo5HDSrcLocalList[n + NCHW_CONV_ADDR_LIST_SIZE / 2] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer2[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer2[i * oneBlockElementNum + (n + oneBlockElementNum) * cumSumInfo.inner]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }

    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {

            dstRepStride = alignOutter;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, srcRepStride, dstRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2;
                 n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter].GetPhyAddr();
                transDataTo5HDSrcLocalList[n + NCHW_CONV_ADDR_LIST_SIZE / 2] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2;
                 n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + (n + NCHW_CONV_ADDR_LIST_SIZE / 2)) *
                                          cumSumInfo.inner]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float>(0, cumSumInfo.outter * cumSumInfo.inner);
    Adds<float, false>(
        dstTensor, tempBuffer2, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CumSumFirstDim(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    if constexpr (sizeof(T) == 2) {
        const uint32_t minTmpBufferSize = cumSumInfo.outter * cumSumInfo.inner * sizeof(float);
        const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
# 344 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/../../impl/math/cumsum/cumsum_common_impl.h"
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(cumSumInfo.outter * cumSumInfo.inner);
        LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        Cast<float, T, false>(tmpBuffer,
            srcTensor,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

        SetVectorMask<T>(0, cumSumInfo.inner);
        const BinaryRepeatParams binaryParams;
        for (uint32_t row = 1; row < cumSumInfo.outter; ++row) {
            Add<float, false>(tmpBuffer[row * cumSumInfo.inner],
                tmpBuffer[(row - 1) * cumSumInfo.inner],
                tmpBuffer[row * cumSumInfo.inner],
                MASK_PLACEHOLDER,
                1,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }

        SetVectorMask<T, MaskMode::COUNTER>(cumSumInfo.outter * cumSumInfo.inner);
        Cast<T, float, false>(dstTensor,
            tmpBuffer,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

    } else {
        SetMaskCount();
        SetVectorMask<T>(0, cumSumInfo.inner);
        Adds<T, false>(
            dstTensor, srcTensor, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        const BinaryRepeatParams binaryParams;

        for (uint32_t row = 1; row < cumSumInfo.outter; ++row) {
            Add<T, false>(dstTensor[row * cumSumInfo.inner],
                dstTensor[(row - 1) * cumSumInfo.inner],
                srcTensor[row * cumSumInfo.inner],
                MASK_PLACEHOLDER,
                1,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskNorm();
        ResetMask();
    }
}
}
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h" 2





namespace AscendC {
#pragma begin_pipe(V)

constexpr CumSumConfig defaultCumSumConfig = {true, false, true};
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
template <typename T, const CumSumConfig &config = defaultCumSumConfig>
[aicore] __inline__ __attribute__((always_inline)) void CumSum(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
    if constexpr (config.isLastAxis) {
        uint32_t minCastTempBufferSize = 0;
        if constexpr (sizeof(T) == 2) {
            minCastTempBufferSize = cumSumInfo.inner * NCHW_CONV_ADDR_LIST_SIZE * sizeof(half);
        }
        const uint32_t minTmpBufferSize = minCastTempBufferSize + NCHW_CONV_ADDR_LIST_SIZE * cumSumInfo.inner *
                                                                      sizeof(T) * 2;
        const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
# 83 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
        const uint32_t oneRepeateSize = tmpBufferSize / minTmpBufferSize * NCHW_CONV_ADDR_LIST_SIZE;
        const uint32_t rangeM = cumSumInfo.outter / oneRepeateSize;
        const uint32_t tailM = cumSumInfo.outter - oneRepeateSize * rangeM;
        uint32_t dstLocalOffset = 0;
        uint32_t srcLocalOffset = 0;
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        for (uint32_t i = 0; i < rangeM; i++) {
            CumSumLastDim<T>(
                dstTensor[dstLocalOffset], srcTensor[srcLocalOffset], tmpBuffer, {oneRepeateSize, cumSumInfo.inner});
            dstLocalOffset += cumSumInfo.inner * oneRepeateSize;
            srcLocalOffset += cumSumInfo.inner * oneRepeateSize;
        }

        if (tailM != 0) {
            CumSumLastDim<T>(
                dstTensor[dstLocalOffset], srcTensor[srcLocalOffset], tmpBuffer, {tailM, cumSumInfo.inner});
        }
    } else {
        CumSumFirstDim<T>(dstTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
    }

    if constexpr (config.outputLastRow) {
        SetMaskCount();
        SetVectorMask<T>(0, cumSumInfo.inner);
        Adds<T, false>(lastRowTensor,
            dstTensor[(cumSumInfo.outter - 1) * cumSumInfo.inner],
            0,
            MASK_PLACEHOLDER,
            1,
            {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    }
}
# 132 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/math/cumsum.h"
template <typename T, const CumSumConfig &config = defaultCumSumConfig>
[aicore] __inline__ __attribute__((always_inline)) void CumSum(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor, const LocalTensor<T> &srcTensor,
    const CumSumInfo &cumSumInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CumSum<T, config>(dstTensor, lastRowTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
}

#pragma end_pipe
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h" 1
# 21 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/../../impl/utils/init_global_memory/init_global_memory_v220_impl.h" 1
# 19 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/../../impl/utils/init_global_memory/init_global_memory_v220_impl.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 1
# 20 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/../../impl/utils/init_global_memory/init_global_memory_v220_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitGlobalMemoryImpl(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<T> popBuffer;
    constexpr uint32_t MAX_REPEAT_LEN = 256;
    bool ret = PopStackBuffer<T, TPosition::LCM>(popBuffer);
                                                                                                     ;
    constexpr uint32_t maxBurstSize = (MAX_REPEAT_TIMES * MAX_REPEAT_LEN) / sizeof(T);
    const uint32_t popSize = popBuffer.GetSize() >= maxBurstSize ? maxBurstSize : popBuffer.GetSize();
    const uint32_t round = size / popSize;
    const uint32_t tail = size % popSize;
    const uint32_t roundSize = round != 0 ? popSize : 0;
    Duplicate<T>(popBuffer, value, popSize);
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    struct DataCopyExtParams repeatParams;
    repeatParams.blockCount = 1;
    uint32_t comOffset = 0;

    repeatParams.blockLen = static_cast<uint32_t>(roundSize * sizeof(T));
    for (uint32_t index = 0; index < round; ++index) {
        DataCopyPad(gmWorkspaceAddr[comOffset], popBuffer, repeatParams);
        comOffset += roundSize;
    }

    repeatParams.blockLen = static_cast<uint32_t>(tail * sizeof(T));
    if (tail != 0) {
        comOffset = round * roundSize;
        DataCopyPad(gmWorkspaceAddr[comOffset], popBuffer, repeatParams);
    }
    PipeBarrier<PIPE_MTE3>();
}
}
# 22 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h" 2


namespace AscendC {
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/utils/init_global_memory.h"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitGlobalMemory(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    InitGlobalMemoryImpl<T>(gmWorkspaceAddr, size, value);
}

}
# 85 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/lib/kernel_api.h" 2
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h" 2
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 1
# 24 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
namespace AscendC {
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void __attribute__((inout_pipe("MTE2"))) DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const DataCopyParams& repeatParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());





    if (dstHWPos == Hardware::UB) {

        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            repeatParams);
    } else if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            repeatParams);
    } else {

                                                                            ;
    }
}

[aicore] __inline__ __attribute__((always_inline)) void CheckNd2NzParams(Nd2NzParams params, const __attribute__((cce_global)) char *msg)
{
    constexpr uint16_t ND2NZ_LIMIT = 16384;
                                                                        ;
                                                                           ;
                                                                                         ;
                                                                                       ;
}
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const Nd2NzParams& intriParams)
{
    CheckNd2NzParams(intriParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1ND2NZImpl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else if (dstHWPos == Hardware::UB) {
        DataCopyGM2UBND2NZImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else {

                                                                         ;
    }
}
# 126 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const Nd2NzParams &intriParams)
{
    CheckNd2NzParams(intriParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECCALC / VECOUT",
        "DataCopy from LocalTensor to LocalTensor with Nd2NzParams");
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "TSCM",
        "DataCopy from LocalTensor to LocalTensor with Nd2NzParams");
    DataCopyUB2L1ND2NZImpl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        intriParams);
}
# 150 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyParams& repeatParams)
{
    using PrimType = PrimT<T>;
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());
# 166 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
    if (srcHWPos == Hardware::UB) {

        DataCopyUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
            repeatParams);
    } else if (srcHWPos == Hardware::L1) {

        DataCopyL12GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)srcLocal.GetPhyAddr(),
            repeatParams);
    } else {





                                                                            ;

    }







}
# 202 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &repeatParams)
{
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());

    if (srcHWPos == Hardware::UB) {
        if (dstHWPos == Hardware::UB) {






            DataCopyUB2UBIntf(dstLocal, srcLocal, repeatParams);
        } else if (dstHWPos == Hardware::L1) {

            DataCopyUB2L1Intf(dstLocal, srcLocal, repeatParams);
        } else {


                                                                                                         ;




        }
    } else if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL12UBIntf(dstLocal, srcLocal, repeatParams);
        } else if (dstHWPos == Hardware::BIAS) {
            CheckTensorAlign<T>(dstLocal, 64, "dstLocal", "DataCopy from C1 to C2");
            CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "DataCopy from C1 to C2");
            DataCopyL12BTIntf(dstLocal, srcLocal, repeatParams);

        } else if (dstHWPos == Hardware::FIXBUF) {
            CheckTensorAlign<T>(dstLocal, 128, "dstLocal", "DataCopy from A1 / B1 / C1 to C2PIPE2GM");
            CheckTensorAlign<T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "DataCopy from C1 to C2");
            DataCopyL12FBIntf(dstLocal, srcLocal, repeatParams);

        } else {

                                                                                             ;
        }
    } else {

                                                                           ;
    }
}
# 264 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename dst_T, typename src_T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<dst_T> &dstLocal, const LocalTensor<src_T> &srcLocal,
    const DataCopyParams &repeatParams)
{
    using PrimDstType = PrimT<dst_T>;
    using PrimSrcType = PrimT<src_T>;
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());

    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::BIAS) {

            CheckTensorAlign<dst_T>(dstLocal, 64, "dstLocal", "DataCopy from C1 to C2");
            CheckTensorAlign<src_T>(srcLocal, ONE_BLK_SIZE, "srcLocal", "DataCopy from C1 to C2");
            if constexpr (IsSameType<PrimDstType, PrimSrcType>::value) {
                DataCopyL12BTImpl((uint64_t)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimSrcType*)srcLocal.GetPhyAddr(),
                    (uint16_t)0, repeatParams);
            } else if constexpr (IsSameType<PrimDstType, float>::value && IsSameType<PrimSrcType, half>::value) {
                DataCopyL12BTImpl((uint64_t)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) half *)srcLocal.GetPhyAddr(), (uint16_t)1,
                    repeatParams);
            } else {

                                                                                                          ;
            }
        } else {

                                                                              ;
        }
    } else {

                                                                          ;
    }
}
# 311 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T, bool IsSetMask>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint64_t mask[], const uint8_t repeatTimes, const CopyRepeatParams &repeatParams)
{
    using PrimType = PrimT<T>;






    CopyImpl<PrimType, IsSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}


template <typename T, bool IsSetMask>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint64_t mask, const uint8_t repeatTimes, const CopyRepeatParams &repeatParams)
{
    using PrimType = PrimT<T>;






    CopyImpl<PrimType, IsSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 351 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T> &dstLocal, const GlobalTensor<T> &srcGlobal,
    const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[], const uint32_t dimValue)
{
    using PrimType = PrimT<T>;
    static_assert(IsSameType<PrimType, T>::value, "TensorTrait is not supported by DataCopy with SliceInfo!");





    uint32_t srcStartIndex = 0;
    uint32_t dstStartIndex = 0;
    uint32_t srcOffsetListSize = 0;
    uint32_t dstOffsetListSize = 0;
    uint32_t srcShapeInfo[8];
    uint32_t dstShapeInfo[8];
    bool useShapeValue = !(srcSliceInfo[0].shapeValue == 0);
    for (int i = 0; i < dimValue; i++) {
        srcShapeInfo[i] = useShapeValue ? srcSliceInfo[i].shapeValue : srcGlobal.GetShapeInfo().shape[i];
        dstShapeInfo[i] = useShapeValue ? dstSliceInfo[i].shapeValue : dstLocal.GetShapeInfo().shape[i];
    }

    srcStartIndex = DataCopyGetPhyStartIndex(srcSliceInfo, srcShapeInfo, dimValue);
    dstStartIndex = DataCopyGetPhyStartIndex(dstSliceInfo, dstShapeInfo, dimValue);
    uint32_t srcOffsetList[MAX_SLICE_SIZE];
    uint32_t dstOffsetList[MAX_SLICE_SIZE];
    DataCopyGetOffsetList(srcSliceInfo, srcShapeInfo, dimValue, &srcOffsetListSize, srcOffsetList);
    DataCopyGetOffsetList(dstSliceInfo, dstShapeInfo, dimValue, &dstOffsetListSize, dstOffsetList);
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = srcSliceInfo[0].burstLen;
    uint32_t oneSliceLen = srcSliceInfo[0].burstLen * AscendCUtils::GetC0Count(sizeof(T)) + srcSliceInfo[0].stride;
    repeatParams.blockCount =
        (srcSliceInfo[0].endIndex - srcSliceInfo[0].startIndex + 1 + srcSliceInfo[0].stride) / oneSliceLen;
    repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();

    if ((srcSliceInfo[0].stride * sizeof(T)) % AscendCUtils::GetC0Size() == 0) {
        repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_global)) T *)srcGlobal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    } else {
        repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T);
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopySliceGm2UBImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_global)) T *)srcGlobal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    }
}
# 411 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T> &dstGlobal, const LocalTensor<T> &srcLocal,
    const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[], const uint32_t dimValue)
{
    using PrimType = PrimT<T>;
    static_assert(IsSameType<PrimType, T>::value, "TensorTrait is not supported by DataCopy with SliceInfo!");





    uint32_t srcStartIndex = 0;
    uint32_t dstStartIndex = 0;
    uint32_t srcOffsetListSize = 0;
    uint32_t dstOffsetListSize = 0;
    uint32_t srcShapeInfo[8];
    uint32_t dstShapeInfo[8];
    bool useShapeValue = !(srcSliceInfo[0].shapeValue == 0);
    for (int i = 0; i < dimValue; i++) {
        srcShapeInfo[i] = useShapeValue ? srcSliceInfo[i].shapeValue : srcLocal.GetShapeInfo().shape[i];
        dstShapeInfo[i] = useShapeValue ? dstSliceInfo[i].shapeValue : dstGlobal.GetShapeInfo().shape[i];
    }

    srcStartIndex = DataCopyGetPhyStartIndex(srcSliceInfo, srcShapeInfo, dimValue);
    dstStartIndex = DataCopyGetPhyStartIndex(dstSliceInfo, dstShapeInfo, dimValue);
    uint32_t dstOffsetList[MAX_SLICE_SIZE];
    uint32_t srcOffsetList[MAX_SLICE_SIZE];
    DataCopyGetOffsetList(srcSliceInfo, srcShapeInfo, dimValue, &srcOffsetListSize, srcOffsetList);
    DataCopyGetOffsetList(dstSliceInfo, dstShapeInfo, dimValue, &dstOffsetListSize, dstOffsetList);

    struct DataCopyParams repeatParams;
    repeatParams.blockLen = srcSliceInfo[0].burstLen;
    uint32_t oneSliceLen = srcSliceInfo[0].burstLen * AscendCUtils::GetC0Count(sizeof(T)) + srcSliceInfo[0].stride;
    repeatParams.blockCount =
        (srcSliceInfo[0].endIndex - srcSliceInfo[0].startIndex + 1 + srcSliceInfo[0].stride) / oneSliceLen;
    repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();

    if ((dstSliceInfo[0].stride * sizeof(T)) % AscendCUtils::GetC0Size() == 0) {
        repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopyUB2GMImpl((__attribute__((cce_global)) T *)dstGlobal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    } else {
        repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T);
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {
            DataCopySliceUB2GMImpl((__attribute__((cce_global)) T *)dstGlobal.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);
        }
    }
}
# 470 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const uint32_t calCount)
{
    using PrimType = PrimT<T>;


                                                      ;
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
    DataCopy(dstLocal, srcGlobal, repeatParams);
}
# 490 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const uint32_t calCount)
{
    using PrimType = PrimT<T>;


                                                      ;
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
    DataCopy(dstGlobal, srcLocal, repeatParams);
}
# 510 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t calCount)
{
    using PrimType = PrimT<T>;


                                                      ;
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = calCount / AscendCUtils::GetC0Count(sizeof(PrimType));
    DataCopy(dstLocal, srcLocal, repeatParams);
}

[aicore] __inline__ __attribute__((always_inline)) void CheckNz2NdParams(const Nz2NdParamsFull& params)
{
    constexpr uint16_t NZ2ND_LIMIT = 8192;
                                                                                                    ;
                                                                                                       ;
                                                                                                       ;
                                                                                                                           ;
                                                                                                              ;
}







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const Nz2NdParamsFull& intriParams)
{
    CheckNz2NdParams(intriParams);
    using PrimType = PrimT<T>;







    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());
    if (srcHWPos != Hardware::UB) {



                                                                                             ;

    }
    DataCopyUB2GMNZ2NDImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        intriParams);






}
# 587 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());

    if (dstHWPos == Hardware::UB) {

        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
            intriParams);
    } else {

                                                                                    ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<T>& srcLocal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());

    if (srcHWPos == Hardware::UB) {

        DataCopyUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
            intriParams);
    } else if (srcHWPos == Hardware::L1) {

        DataCopyL12GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)srcLocal.GetPhyAddr(),
            intriParams);
    } else {

                                                                                    ;
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());

    if (srcHWPos == Hardware::UB) {
        if (dstHWPos == Hardware::L1) {

            DataCopyUB2L1Intf(dstLocal, srcLocal, intriParams);
        } else if (dstHWPos == Hardware::L0C) {

            DataCopyUB2L0CIntf(dstLocal, srcLocal, intriParams, enhancedParams);
        } else if (dstHWPos == Hardware::UB) {

            DataCopyUB2UBIntf(dstLocal, srcLocal, intriParams);
        } else {

                                                                                                                 ;
        }
    } else if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL12UBIntf(dstLocal, srcLocal, intriParams);
        } else if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CIntf(dstLocal, srcLocal, intriParams, enhancedParams);
        } else {

                                                                                                ;
        }
    } else if (srcHWPos == Hardware::L0C) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL0C2UBIntf(dstLocal, srcLocal, intriParams, enhancedParams);
        } else {

                                                                                            ;
        }
    } else {





                                                                                   ;

    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
    const DataCopyCO12DstParams& intriParams)
{
    CheckTensorPos<U>(srcLocal, Hardware::L0C, "srcLocal", "CO1",
        "DataCopy from LocalTensor to LocalTensor with DataCopyCO12DstParams");
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "A1",
        "DataCopy from LocalTensor to LocalTensor with DataCopyCO12DstParams");

    DataCopyL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<U>*)srcLocal.GetPhyAddr(), intriParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const GlobalTensor<T>& dstGlobal, const LocalTensor<U>& srcLocal,
    const DataCopyCO12DstParams& intriParams)
{
    CheckTensorPos<U>(srcLocal, Hardware::L0C, "srcLocal", "CO1",
        "DataCopy from LocalTensor to GlobalTensor with DataCopyCO12DstParams");

    DataCopyL0C2GMImpl((__attribute__((cce_global)) PrimT<T>*)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<U>*)srcLocal.GetPhyAddr(), intriParams);
}



template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, bfloat16_t>::value &&
    IsSameType<PrimT<U>, float>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<U>*)srcLocal.GetPhyAddr(),
                intriParams, enhancedParams);
        } else {

                                                                                             ;
        }
    } else {

                                                                                         ;
    }
}



template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, half>::value &&
    IsSameType<PrimT<U>, float>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void DataCopy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((QuePosition)dstLocal.GetPosition());
    const Hardware srcHWPos = GetPhyType((QuePosition)srcLocal.GetPosition());
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) float*)srcLocal.GetPhyAddr(), intriParams,
                enhancedParams);
        } else {

                                                                                              ;
        }
    } else if (srcHWPos == Hardware::L0C) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) float*)srcLocal.GetPhyAddr(), intriParams,
                enhancedParams);
        } else {

                                                                                            ;
        }
    } else {

                                                                                         ;
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CheckTensorL0C2UB(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal)
{
    CheckTensorPos<U>(srcLocal, Hardware::L0C, "srcLocal", "CO1",
        "DataCopy from LocalTensor(CO1) to LocalTensor(CO2) with DataCopyEnhancedParams");
    CheckTensorPos<T>(dstLocal, Hardware::UB, "dstLocal", "CO2",
        "DataCopy from LocalTensor(CO1) to LocalTensor(CO2) with DataCopyEnhancedParams");
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, half>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, int16_t>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) int16_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) int8_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, uint8_t>::value &&
    IsSameType<PrimT<U>, int32_t>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dstLocal, srcLocal);
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) uint8_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, float>::value &&
    IsSameType<PrimT<U>, half>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dstLocal, const LocalTensor<U> &srcLocal,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorPos<U>(srcLocal, Hardware::UB, "srcLocal", "CO2",
        "DataCopy from LocalTensor(CO2) to LocalTensor(CO1) with DataCopyEnhancedParams");
    CheckTensorPos<T>(dstLocal, Hardware::L0C, "dstLocal", "CO1",
        "DataCopy from LocalTensor(CO2) to LocalTensor(CO1) with DataCopyEnhancedParams");
    DataCopyUB2L0CImpl((__attribute__((cce_cube_c)) float*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half*)srcLocal.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const GlobalTensor<T> &srcGlobal, const DataCopyParams &dataCopyParams, const DataCopyPadParams &padParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    CheckTensorPos<T>(dstLocal, Hardware::UB, "dstLocal", "VECIN / VECOUT", "DataCopyPad from GM to VECIN / VECOUT");
    DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
        dataCopyParams, padParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T> &dstGlobal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &dataCopyParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECOUT", "DataCopyPad from VECIN / VECOUT to GM");
    DataCopyPadUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        dataCopyParams);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const LocalTensor<T> &srcLocal, const DataCopyParams &dataCopyParams, const Nd2NzParams &nd2nzParams)
{
    CheckNd2NzParams(nd2nzParams, "DataCopyPad with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "TSCM", "DataCopyPad from VECIN / VECOUT to TSCM");
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECOUT", "DataCopyPad from VECIN / VECOUT to TSCM");
    DataCopyPadUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        dataCopyParams, nd2nzParams);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const GlobalTensor<T> &srcGlobal, const DataCopyExtParams &dataCopyParams, const DataCopyPadExtParams<T> &padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    CheckTensorPos<T>(dstLocal, Hardware::UB, "dstLocal", "VECIN / VECOUT", "DataCopyPad from GM to VECIN / VECOUT");
    DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) T*)srcGlobal.GetPhyAddr(), dataCopyParams,
        padParams);
}



template <typename T, typename U, typename std::enable_if<IsSameType<PrimT<T>, U>::value &&
    (!IsSameType<T, U>::value), bool>::type>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dstLocal,
    const GlobalTensor<T> &srcGlobal, const DataCopyExtParams &dataCopyParams, const DataCopyPadExtParams<U> &padParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }





    CheckTensorPos<T>(dstLocal, Hardware::UB, "dstLocal", "VECIN / VECOUT", "DataCopyPad from GM to VECIN / VECOUT");
    DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)srcGlobal.GetPhyAddr(),
        dataCopyParams, padParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T> &dstGlobal,
    const LocalTensor<T> &srcLocal, const DataCopyExtParams &dataCopyParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECOUT", "DataCopyPad from VECIN / VECOUT to GM");
    DataCopyPadUB2GMImpl((__attribute__((cce_global)) PrimType*)dstGlobal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        dataCopyParams);
}


template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCopyPad(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const DataCopyExtParams &dataCopyParams, const Nd2NzParams &nd2nzParams)
{
    CheckNd2NzParams(nd2nzParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(dstLocal, Hardware::L1, "dstLocal", "TSCM", "DataCopyPad from VECIN / VECOUT to TSCM");
    CheckTensorPos<T>(srcLocal, Hardware::UB, "srcLocal", "VECIN / VECOUT", "DataCopyPad from VECIN / VECOUT to TSCM");
    DataCopyPadUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        dataCopyParams, nd2nzParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetPadValue(T paddingValue)
{

    if (g_coreType == AIC) {
        return;
    }
    set_mov_pad_val(GetScalarBitcodeValue((T)paddingValue));



}
}
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm" 1
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T> &tensor, uint32_t desc, uint32_t dumpSize)
{

    DumpTensorLocal2GMImpl(tensor, desc, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize)
{

    DumpTensorGM2GMImpl(tensor, desc, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize, const ShapeInfo& shapeInfo)
{

    DumpShapeImpl(shapeInfo);
    DumpTensorGM2GMImpl(tensor, desc, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpTensor(const LocalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize, const ShapeInfo& shapeInfo)
{

    DumpShapeImpl(shapeInfo);
    DumpTensorLocal2GMImpl(tensor, desc, dumpSize);

    return;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const LocalTensor<T> &tensor, uint32_t index, uint32_t countOff, uint32_t dumpSize)
{

    if (countOff > tensor.GetSize()) {


                                                       ;
        return;
    }
    LocalTensor<T> tmpTensor = tensor[countOff];
    DumpTensorLocal2GMImpl(tmpTensor, index, dumpSize);

    return;
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DumpAccChkPoint(const GlobalTensor<T> &tensor, uint32_t index, uint32_t countOff, uint32_t dumpSize)
{

    if (countOff > tensor.GetSize()) {


                                                       ;
        return;
    }
    GlobalTensor<T> tmpTensor = tensor[countOff];
    DumpTensorGM2GMImpl(tmpTensor, index, dumpSize);

    return;
}
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void PRINTF(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_SCALAR, fmt, args...);

}

template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void printf(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_SCALAR, fmt, args...);

}


template <class... Args>
[aicore] __inline__ __attribute__((always_inline)) void AssertImpl(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_ASSERT, fmt, args...);



}
# 175 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_dump_tensor_intf.cppm"
}
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
namespace AscendC {
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParams& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}
# 76 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcLocal,
    const LoadData2DParamsV2& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, const IsResetLoad3dConfig &defaultConfig>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV1<T>& loadDataParams)
{
    CheckLoadData3dParams(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.strideW, loadDataParams.strideH);

                                           ;

                                                           ;

                                                           ;

                                           ;

                                           ;

                                           ;
                                                                                                                ;
                                                                                                      ;
    LoadDataImpl<T, defaultConfig>(dstLocal, srcLocal, loadDataParams);
}
# 169 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, const IsResetLoad3dConfig &defaultConfig>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2<T>& loadDataParams)
{






    LoadDataImpl<T, defaultConfig>(dstLocal, srcLocal, loadDataParams);
}
# 208 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    LoadDataImpl<T>(dstLocal, srcLocal, loadDataParams);
}



template <>
[aicore] __inline__ __attribute__((always_inline)) void LoadData(const LocalTensor<bfloat16_t>& dstLocal, const LocalTensor<bfloat16_t>& srcLocal,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    LoadDataImpl(dstLocal, srcLocal, loadDataParams);
}
# 240 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParams& loadDataParams)
{
    LoadDataWithTransposeImpl(dstLocal, srcLocal, loadDataParams);
}
# 260 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithTranspose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LoadData2dTransposeParamsV2& loadDataParams)
{
    LoadDataWithTransposeImpl(dstLocal, srcLocal, loadDataParams);
}
# 287 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename DstT, typename Src0T, typename Src1T>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const MmadParams& mmadParams)
{
    MmadImpl(dstLocal, fmLocal, filterLocal, mmadParams);
}

template <typename DstT, typename Src0T, typename Src1T, typename BiasT>
[aicore] __inline__ __attribute__((always_inline)) void Mmad(const LocalTensor<DstT>& dstLocal, const LocalTensor<Src0T>& fmLocal,
    const LocalTensor<Src1T>& filterLocal, const LocalTensor<BiasT>& biasLocal, const MmadParams& mmadParams)
{
    MmadImpl(dstLocal, fmLocal, filterLocal, biasLocal, mmadParams);
}


[aicore] __inline__ __attribute__((always_inline)) void MmadWithSparse(const LocalTensor<int32_t>& dstLocal, const LocalTensor<int8_t>& fmLocal,
    const LocalTensor<int8_t>& filterLocal, const MmadParams& mmadParams)
{
    MmadSpImpl(dstLocal, fmLocal, filterLocal, mmadParams);
}

[aicore] __inline__ __attribute__((always_inline)) void LoadDataWithSparse(const LocalTensor<int8_t> &dstLocal, const LocalTensor<int8_t> &srcLocal,
    const LocalTensor<uint8_t> &idxLocal, const LoadData2dParams &loadDataParam)
{
    LoadDataWithSparseImpl(dstLocal, srcLocal, idxLocal, loadDataParam);
}
# 325 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void BroadCastVecToMM(const LocalTensor<T> &dstLocal,
    const LocalTensor<U> &srcLocal, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap)
{
    BroadCastVecToMMImpl(dstLocal, srcLocal, blockCount, blockLen, srcGap, dstGap);
}
# 345 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void InitConstValue(const LocalTensor<T> &dstLocal,
    const InitConstValueParams<T>& initConstValueParams)
{
# 362 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
    InitConstValueImpl(dstLocal, initConstValueParams);
}
# 372 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataPaddingValue(const T padValue)
{
    Load3DSetPaddingImpl(padValue);
}
# 388 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void SetFmatrix(uint16_t l1H, uint16_t l1W, const uint8_t padList[4], const FmatrixMode& fmatrixMode)
{
                                                                                     ;
                                                                                     ;
    SetFmatrixImpl(l1H, l1W, padList, fmatrixMode);
}
# 403 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataBoundary(uint32_t boundaryValue)
{
    SetLoadDataBoundaryImpl(boundaryValue);
}




[aicore] __inline__ __attribute__((always_inline)) void SetLoadDataRepeat(const LoadDataRepeatParam& repeatParams)
{
                                                                                               ;
    SetLoadDataRepeatImpl(repeatParams);
}
# 434 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("MTE2"))) void LoadImageToLocal(const LocalTensor<T>& dstLocal,
    const LoadImageToLocalParams& loadDataParams)
{





                                                                                                       ;
                                                                                                     ;
                                                                                                               ;
                                                                                                             ;
                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
                                                                                                   ;
                                                                                                     ;
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    if (dstScope == Hardware::L1) {
        LoadImageToLocalCal((__attribute__((cce_cube_buff)) T*)dstLocal.GetPhyAddr(), loadDataParams);
    } else {
                                                                                 ;
    }
}
# 469 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_mm_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void LoadDataUnzip(const LocalTensor<T>& dstLocal, const GlobalTensor<T>& srcGlobal)
{
    LoadDataUnzipImpl(dstLocal, srcGlobal);
}
}
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
namespace AscendC {

template <typename T>
[[deprecated("NOTICE: GetGemmTiling has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) GemmTiling GetGemmTiling(uint32_t m, uint32_t k, uint32_t n)
{
    uint32_t c0 = 0;
    uint32_t dSize = 1;
    if (IsSameType<T, uint8_t>::value || IsSameType<T, int8_t>::value) {
        c0 = 32;
        dSize = 1;
    } else {
        c0 = 16;
        dSize = 2;
    }
    GemmTiling tilling;
    tilling.c0Size = c0;
    tilling.dtypeSize = dSize;
    tilling.mNum = m;
    tilling.nNum = n;
    tilling.kNum = k;
    tilling.roundM = DivCeil(m, tilling.blockSize) * tilling.blockSize;
    tilling.roundN = DivCeil(n, tilling.blockSize) * tilling.blockSize;
    tilling.roundK = DivCeil(k, tilling.c0Size) * tilling.c0Size;
    uint32_t k0a = TOTAL_L0A_SIZE / 2 / (tilling.roundM * dSize);
    uint32_t k0b = TOTAL_L0B_SIZE / 2 / (tilling.roundN * dSize);
    uint32_t k0 = k0a > k0b ? k0b : k0a;
    k0 = k0 > k ? k : k0;

    tilling.kTileBlock = k0 / tilling.c0Size;
    if (tilling.kTileBlock == 0) {
        tilling.kTileBlock = 1;
    }
    tilling.loopMode = LoopMode::MODE_NM;

    tilling.mBlockNum = DivCeil(m, tilling.blockSize);
    tilling.nBlockNum = DivCeil(n, tilling.blockSize);
    tilling.kBlockNum = DivCeil(k, tilling.c0Size);

    CalculateGemmTiling(tilling);

    return tilling;
}
# 97 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
template <typename dst_T, typename src0_T, typename src1_T>
[[deprecated("NOTICE: Gemm has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("V"))) void Gemm(const LocalTensor<dst_T>& dstLocal, const LocalTensor<src0_T>& src0Local,
    const LocalTensor<src1_T>& src1Local, const uint32_t m, const uint32_t k, const uint32_t n, GemmTiling tilling,
    bool partialsum, int32_t initValue)
{
# 118 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    LocalTensor<dst_T> L0c;
    if (dstScope == Hardware::L0C) {
        L0c = dstLocal[0];
    } else {
# 132 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
    }

    if (tilling.loopMode == LoopMode::MODE_NM) {
        GemmExecNm(L0c, src0Local, src1Local, tilling, initValue);
    } else if (tilling.loopMode == LoopMode::MODE_MN) {
        GemmExecMn(L0c, src0Local, src1Local, tilling, initValue);
    } else {

    }
# 149 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_gemm_intf.cppm"
}
}
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_fixpipe_intf.cppm" 1
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_fixpipe_intf.cppm"
namespace AscendC {
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_fixpipe_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag)
{
    SetFixPipeConfigImpl<T>(reluPre, quantPre, isUnitFlag);
}

template <typename T, bool setRelu>
[aicore] __inline__ __attribute__((always_inline)) void SetFixPipeConfig(const LocalTensor<T> &preTensor, bool isUnitFlag)
{
    SetFixPipeConfigImpl<T, setRelu>(preTensor, isUnitFlag);
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipeNz2ndFlag(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride)
{
    SetFixpipeNz2ndFlagImpl(ndNum, srcNdStride, dstNdStride);
}

[aicore] __inline__ __attribute__((always_inline)) void SetFixpipePreQuantFlag(uint64_t config)
{
    SetFixpipePreQuantFlagImpl(config);
}


template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT> &dstLocal, const LocalTensor<SrcT> &srcLocal,
    const FixpipeParamsV220 &intriParams)
{
    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");

                                                                                                       ;

    if ((GetPhyType((QuePosition)dstLocal.GetPosition()) == Hardware::L1)) {
        FixpipeL0C2L1Impl<DstT, SrcT, config>((__attribute__((cce_cube_buff)) DstT *)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(), intriParams);
    } else if ((GetPhyType((QuePosition)dstLocal.GetPosition()) == Hardware::UB)) {
        FixpipeL0C2UBImpl<DstT, SrcT, config>((__attribute__((cce_unif_buff)) DstT *)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(), intriParams);
    }
}


template <typename DstT, typename SrcT, const FixpipeConfig& config>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const LocalTensor<DstT>& dstLocal, const LocalTensor<SrcT>& srcLocal,
    const LocalTensor<uint64_t>& cbufWorkspace, const FixpipeParamsV220& intriParams)
{
    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");

                                                                                                       ;
    CheckTensorPos<uint64_t>(cbufWorkspace, Hardware::L1, "cbufWorkspace", "A1", "Fixpipe");


                                                                                                       ;
    if ((GetPhyType((QuePosition)dstLocal.GetPosition()) == Hardware::L1)) {
        FixpipeL0C2L1Impl<DstT, SrcT, config>((__attribute__((cce_cube_buff)) DstT *)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) uint64_t *)cbufWorkspace.GetPhyAddr(), intriParams);
    } else if ((GetPhyType((QuePosition)dstLocal.GetPosition()) == Hardware::UB)) {
        FixpipeL0C2UBImpl<DstT, SrcT, config>((__attribute__((cce_unif_buff)) DstT *)dstLocal.GetPhyAddr(),
            (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(), (__attribute__((cce_cube_buff)) uint64_t *)cbufWorkspace.GetPhyAddr(), intriParams);
    }
}


template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT> &dstGlobal, const LocalTensor<SrcT> &srcLocal,
    const FixpipeParamsV220 &intriParams)
{







    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");
    FixpipeL0C2GMImpl<DstT, SrcT, config>((__attribute__((cce_global)) DstT *)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(),
        intriParams);






}


template <typename DstT, typename SrcT, const FixpipeConfig &config>
[aicore] __inline__ __attribute__((always_inline)) void Fixpipe(const GlobalTensor<DstT> &dstGlobal, const LocalTensor<SrcT> &srcLocal,
    const LocalTensor<uint64_t> &cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckTensorPos<SrcT>(srcLocal, Hardware::L0C, "srcLocal", "CO1", "Fixpipe");
    CheckTensorPos<uint64_t>(cbufWorkspace, Hardware::L1, "cbufWorkspace", "A1", "Fixpipe");


                                                                                                       ;
    FixpipeL0C2GMImpl<DstT, SrcT, config>((__attribute__((cce_global)) DstT *)dstGlobal.GetPhyAddr(), (__attribute__((cce_cube_c)) SrcT *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_cube_buff)) uint64_t *)cbufWorkspace.GetPhyAddr(), intriParams);
}
}
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm" 1
# 27 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
namespace AscendC {

template <typename T>
[[deprecated("NOTICE: GetConv2dTiling has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) Conv2dTilling GetConv2dTiling(Conv2dParams& conv2dParams)
{
    Conv2dTilling tilling;
    GetTypeforC0<T>(conv2dParams, tilling);

    tilling.loopMode = LoopMode::MODE_NM;

    tilling.strideH = conv2dParams.stride[0];
    tilling.strideW = conv2dParams.stride[1];
    tilling.dilationH = conv2dParams.dilation[0];
    tilling.dilationW = conv2dParams.dilation[1];
    tilling.hi = conv2dParams.imgShape[0];
    tilling.wi = conv2dParams.imgShape[1];
    tilling.ho = (tilling.hi + conv2dParams.padList[2] + conv2dParams.padList[3] -
        tilling.dilationH * (conv2dParams.kernelShape[0] - 1) - 1) /
        tilling.strideH +
        1;
    tilling.wo = (tilling.wi + conv2dParams.padList[0] + conv2dParams.padList[1] -
        tilling.dilationW * (conv2dParams.kernelShape[1] - 1) - 1) /
        tilling.strideW +
        1;

    tilling.height = conv2dParams.kernelShape[0];
    tilling.width = conv2dParams.kernelShape[1];

    tilling.howo = tilling.ho * tilling.wo;

    tilling.mNum = tilling.howo;
    tilling.nNum = conv2dParams.cout;
    tilling.kNum = conv2dParams.cin * tilling.height * tilling.width;

    CalculateConv2dTiling(tilling);

    return tilling;
}
# 86 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
template <typename dst_T, typename src_T>
[[deprecated("NOTICE: Conv2D has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))
    __attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal, const LocalTensor<src_T> &featureMap,
    const LocalTensor<src_T> &weight, Conv2dParams &conv2dParams, Conv2dTilling &tilling)
{
    if (conv2dParams.initY == 2) {
        return;
    }

    Conv2D(dstLocal, dstLocal, featureMap, weight, conv2dParams, tilling);
}

template <typename dst_T, typename src_T>
[[deprecated("NOTICE: Conv2D has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("MTE2")))__attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<dst_T> &dstLocal,
    const LocalTensor<dst_T> &bias, const LocalTensor<src_T> &featureMap, const LocalTensor<src_T> &weight,
    Conv2dParams &conv2dParams, Conv2dTilling &tilling)
{
# 120 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
    const Hardware dstScope = GetPhyType((QuePosition)dstLocal.GetPosition());
    LocalTensor<dst_T> L0c;
    if (dstScope == Hardware::L0C) {
        L0c = dstLocal[0];
    } else {
# 134 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
    }

    if (tilling.loopMode == LoopMode::MODE_NM) {
        Conv2DExecNm(L0c, bias, featureMap, weight, conv2dParams, tilling);
    } else if (tilling.loopMode == LoopMode::MODE_MN) {
        Conv2DExecMn(L0c, bias, featureMap, weight, conv2dParams, tilling);
    } else {

    }
# 151 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_conv2d_intf.cppm"
}
}
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_common_intf.cppm" 1
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_common_intf.cppm"
namespace AscendC {
# 61 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_common_intf.cppm"
template <bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void IBSet(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, int32_t blockIdx, int32_t eventID)
{
    int32_t blockNum = get_block_num();

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    if (!isAIVOnly) {
        blockNum = get_block_num() * 2;
    }


    __ib_set_stub(blockIdx, eventID, isAIVOnly);

    auto localSyncGM = gmWorkspace[blockNum * 8 * eventID + blockIdx * 8];
    pipe_barrier(PIPE_ALL);

    while (true) {
        DataCopy(ubWorkspace, localSyncGM, ONE_BLK_SIZE / sizeof(int32_t));
        event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        if (ubWorkspace.GetValue(0) == 0) {
            ubWorkspace.SetValue(0, 1);
            event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            DataCopy(localSyncGM, ubWorkspace, ONE_BLK_SIZE / sizeof(int32_t));
            break;
        }
    }
    pipe_barrier(PIPE_ALL);

    __ib_set_stub(blockIdx, eventID, isAIVOnly);

}

template <bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void IBWait(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, int32_t blockIdx, int32_t eventID)
{
    int32_t blockNum = get_block_num();

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    if (!isAIVOnly) {
        blockNum = get_block_num() * 2;
    }


    __ib_wait_stub(blockIdx, eventID, isAIVOnly);

    auto localSyncGM = gmWorkspace[blockNum * 8 * eventID + blockIdx * 8];
    pipe_barrier(PIPE_ALL);

    while (true) {
        DataCopy(ubWorkspace, localSyncGM, ONE_BLK_SIZE / sizeof(int32_t));
        event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        if (ubWorkspace.GetValue(0) == 1) {
            ubWorkspace.SetValue(0, 0);
            event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            DataCopy(localSyncGM, ubWorkspace, ONE_BLK_SIZE / sizeof(int32_t));
            break;
        }
    }
    pipe_barrier(PIPE_ALL);

    __ib_wait_stub(blockIdx, eventID, isAIVOnly);

}







template <bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, const int usedCores)
{




    SoftSyncAllImpl<isAIVOnly>((__attribute__((cce_global)) int32_t*)gmWorkspace.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)ubWorkspace.GetPhyAddr(), usedCores);

}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockIdx()
{
    return GetBlockIdxImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetBlockNum()
{
    return get_block_num();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockIdx()
{
    return GetSubBlockIdxImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetTaskRation()
{
    return GetTaskRationImpl();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitOutput(GlobalTensor<T> gmWorkspaceAddr, uint32_t size, T value)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<T> popBuffer;
    bool ret = PopStackBuffer<T, TPosition::LCM>(popBuffer);
    uint32_t maxBurstSize = (MAX_REPEAT_TIMES * ONE_BLK_SIZE) / sizeof(T);
    uint32_t popSize = popBuffer.GetSize() >= maxBurstSize ? maxBurstSize : popBuffer.GetSize();
    uint32_t round = size / popSize;
    uint32_t tail = size % popSize;
    uint32_t roundSize = round != 0 ? popSize : 0;
    DuplicateImpl<T>((__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(), value, popSize);
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    struct DataCopyExtParams repeatParams;
    uint32_t comOffset = 0;

    repeatParams = { 1, static_cast<uint32_t>(roundSize * sizeof(T)), 0, 0, 0 };
    for (int index = 0; index < round; ++index) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmWorkspaceAddr.GetPhyAddr() + comOffset,
            (__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(),
            repeatParams);
        comOffset += roundSize;
    }

    repeatParams = {1, static_cast<uint32_t>(tail * sizeof(T)), 0, 0, 0};
    if (tail != 0) {
        comOffset = round * roundSize;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmWorkspaceAddr.GetPhyAddr() + comOffset,
            (__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(),
            repeatParams);
    }

}

template<bool isAIVOnly>
[aicore] __inline__ __attribute__((always_inline)) void SyncAll()
{
    SyncAllImpl<isAIVOnly>();
}

template <AtomicDtype type, AtomicOp op>
[aicore] __inline__ __attribute__((always_inline)) void SetStoreAtomicConfig()
{
    SetStoreAtomicConfigImpl<static_cast<atomic_type_t>(type), static_cast<atomic_op_t>(op)>();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetStoreAtomicConfig()
{
    return GetStoreAtomicConfigImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void GetStoreAtomicConfig(uint16_t &atomicType, uint16_t &atomicOp)
{
    GetStoreAtomicConfigImpl(atomicType, atomicOp);
}

template <pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void NotifyEvent(uint16_t flagId)
{
    constexpr uint8_t subBlockSyncMode = 0x02;
    NotifyEventImpl<subBlockSyncMode, pipe>(flagId);
}

[aicore] __inline__ __attribute__((always_inline)) void WaitEvent(uint16_t flagId)
{
    WaitEventImpl(flagId);
}

template<uint8_t modeId, pipe_t pipe>
[aicore] __inline__ __attribute__((always_inline)) void CrossCoreSetFlag(uint16_t flagId)
{
    NotifyEventImpl<modeId, pipe>(flagId);
}

[aicore] __inline__ __attribute__((always_inline)) void CrossCoreWaitFlag(uint16_t flagId)
{
    WaitEventImpl(flagId);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void DataCachePreload(const GlobalTensor<uint64_t> &srcTensor, const T cacheOffset)
{
    DataCachePreloadImpl(srcTensor, cacheOffset);
}

[aicore] __inline__ __attribute__((always_inline)) void ICachePreLoad(const int64_t preFetchLen)
{
    PreLoad(preFetchLen);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetICachePreloadStatus()
{
    return GetICachePreloadStatusImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void CheckLocalMemoryIA(const CheckLocalMemoryIAParam& checkParams)
{
    CheckLocalMemoryIAImpl(checkParams);
}


template <HardEvent event, MemoryT memT, bool isVirtual> [aicore] __inline__ __attribute__((always_inline)) void HSetFlag(int32_t eventID)
{
    if (g_coreType == AIV) {
        return;
    }
    HSetFlagImpl<event, memT, isVirtual>(eventID);
}

template <HardEvent event, MemoryT memT, bool isVirtual> [aicore] __inline__ __attribute__((always_inline)) void HWaitFlag(int32_t eventID)
{
    if (g_coreType == AIV) {
        return;
    }
    HWaitFlagImpl<event, memT, isVirtual>(eventID);
}


}
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm" 1
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
namespace AscendC {


constexpr int32_t REGION_PROPOSAL_LABEL_POSITION = 5;
constexpr int32_t REGION_PROPOSAL_Y1_POSITION = 1;
constexpr uint8_t GATHER_MASK_MODE_FOR_INDEX_EVEN = 1;
constexpr uint8_t GATHER_MASK_MODE_FOR_INDEX_ODD = 2;

constexpr uint8_t GATHER_MASK_MODE_FOR_EXTRACT_INDEX = 4;
constexpr int32_t REGION_PROPOSAL_SCORE_POSITION = 4;

#pragma begin_pipe(V)
# 66 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort4(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params)
{


                                            ;
    for (int8_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
                                                                                                  ;
    }

                                                                                                                   ;





    uint64_t config = 0;
    config |= (params.repeatTimes & 0xFF);
    config |= (uint64_t(params.elementLengths[0] & 0xFFF) << 8);
    config |= (uint64_t(params.elementLengths[1] & 0xFFF) << 20);
    config |= (uint64_t(params.elementLengths[2] & 0xFFF) << 32);
    config |= (uint64_t(params.elementLengths[3] & 0xFFF) << 44);
    config |= (uint64_t(params.ifExhaustedSuspension & 0x1) << 59);
    config |= (uint64_t(params.validBit & 0xF) << 60);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)srcLocal.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src4.GetPhyAddr()};
    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), addrArray, config);
}
# 107 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void RpSort16(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes)
{


                                            ;
                                                                             ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    VbitsortCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeatParams);
}
# 137 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T>& dstLocal, const MrgSortSrcList<T>& srcLocal,
    const MrgSort4Info& params)
{


                                            ;
    for (int8_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
                                                                                                 ;
    }

                                                                                                                  ;





    uint64_t config = 0;
    config |= (params.repeatTimes & 0xFF);
    config |= (uint64_t(params.validBit & 0xF) << 8);
    config |= (uint64_t(params.ifExhaustedSuspension & 0x1) << 12);

    uint64_t src1 = 0;
    src1 |= (uint64_t(params.elementLengths[0] & 0xFFFF));
    src1 |= (uint64_t(params.elementLengths[1] & 0xFFFF) << 16);
    src1 |= (uint64_t(params.elementLengths[2] & 0xFFFF) << 32);
    src1 |= (uint64_t(params.elementLengths[3] & 0xFFFF) << 48);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)srcLocal.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)srcLocal.src4.GetPhyAddr()};

    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), addrArray, src1, config);
}
# 182 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sort32(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& src1Local, const int32_t repeatTimes)
{


                                            ;
                                                                           ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    VbitsortCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t*)src1Local.GetPhyAddr(), repeatParams);
}
# 211 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalConcat(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber)
{


                                            ;
                                                                                   ;
                                                                               ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    repeatParams.modeNumber = modeNumber;
    VconcatCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeatParams);
}
# 240 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ProposalExtract(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeatTimes, const int32_t modeNumber)
{


                                            ;
                                                                                    ;
                                                                                ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTimes;
    repeatParams.modeNumber = modeNumber;
    VextractCal((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeatParams);
}
# 269 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Concat(LocalTensor<T> &concatLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &tmpLocal, const int32_t repeatTimes)
{


                                            ;
                                                                           ;






    concatLocal = srcLocal;




}
# 299 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Extract(const LocalTensor<T> &dstValueLocal, const LocalTensor<uint32_t> &dstIndexLocal,
    const LocalTensor<T> &sortedLocal, const int32_t repeatTimes)
{


                                            ;
                                                                            ;






    uint64_t rsvdCnt;
    if constexpr (IsSameType<T, half>::value) {
        constexpr uint8_t GATHER_MASK_PATTERN_3 = 3;
        constexpr uint8_t GATHER_MASK_PATTERN_2 = 2;
        GatherMaskCal((__attribute__((cce_unif_buff)) T *)dstValueLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_3, false, (uint32_t)0, { 1, (uint16_t)repeatTimes, DEFAULT_REPEAT_STRIDE, 0 }, rsvdCnt);
        PipeBarrier<PIPE_V>();
        GatherMaskCal((__attribute__((cce_unif_buff)) uint32_t *)dstIndexLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint32_t *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_2, false, (uint32_t)0, { 1, (uint16_t)(repeatTimes * 2), 8, 0 }, rsvdCnt);
    } else {
        constexpr uint8_t GATHER_MASK_PATTERN_1 = 1;
        constexpr uint8_t GATHER_MASK_PATTERN_2 = 2;
        GatherMaskCal((__attribute__((cce_unif_buff)) T *)dstValueLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_1, false, (uint32_t)0, { 1, (uint16_t)repeatTimes, DEFAULT_REPEAT_STRIDE, 0 }, rsvdCnt);
        PipeBarrier<PIPE_V>();
        GatherMaskCal((__attribute__((cce_unif_buff)) uint32_t *)dstIndexLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint32_t *)sortedLocal.GetPhyAddr(),
            GATHER_MASK_PATTERN_2, false, (uint32_t)0, { 1, (uint16_t)repeatTimes, 8, 0 }, rsvdCnt);
    }
# 347 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
}
# 360 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T, bool isExhaustedSuspension>
[aicore] __inline__ __attribute__((always_inline)) void MrgSort(const LocalTensor<T> &dstLocal, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTimes)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


                                            ;
    MrgSort4Info mrgSortInfo(elementCountList, isExhaustedSuspension, validBit, (uint16_t)repeatTimes);

    MrgSort(dstLocal, sortList, mrgSortInfo);



    if (isExhaustedSuspension) {

        constexpr uint32_t validBitMask = 0xFFFF;
        constexpr uint32_t shiftBase = 16;







        auto res = get_vms4_sr();
        sortedNum[0] = res & validBitMask;
        sortedNum[1] = (res >> shiftBase) & validBitMask;
        sortedNum[2] = (res >> (2 * shiftBase)) & validBitMask;
        sortedNum[3] = (res >> (3 * shiftBase)) & validBitMask;
    }
}
# 405 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
template <typename T, bool isFullSort>
[aicore] __inline__ __attribute__((always_inline)) void Sort(const LocalTensor<T> &dstLocal, const LocalTensor<T> &concatLocal,
    const LocalTensor<uint32_t> &indexLocal, LocalTensor<T> &tmpLocal, const int32_t repeatTimes)
{


                                ;
                                                                         ;






    Sort32(dstLocal, concatLocal, indexLocal, repeatTimes);
# 450 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_proposal_intf.cppm"
    if constexpr (isFullSort) {
        PipeBarrier<PIPE_V>();
        DoFullSort(dstLocal, concatLocal, indexLocal, tmpLocal, repeatTimes);
    }
}

constexpr uint32_t halfSortedDataSize = 4;
constexpr uint32_t floatSortedDataSize = 2;






template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortOffset(const uint32_t elemOffset)
{


                          ;

    if constexpr (IsSameType<T, half>::value) {
        return elemOffset * halfSortedDataSize;
    } else {
        return elemOffset * floatSortedDataSize;
    }



}







template <typename T>
[aicore] __inline__ __attribute__((always_inline)) uint32_t GetSortLen(const uint32_t elemCount)
{


                          ;

    if constexpr (IsSameType<T, half>::value) {
        return elemCount * halfSortedDataSize;
    } else {
        return elemCount * floatSortedDataSize;
    }



}
#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetMrgSortResult(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4)
{

    if (g_coreType == AIC) {
        return;
    }

    GetMrgSortResultImpl(mrgSortList1, mrgSortList2, mrgSortList3, mrgSortList4);
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_bilinearinterpalation_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_bilinearinterpalation_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
constexpr uint32_t brcbEleNum = 8;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolationCalc(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    auto sharedTmpBufferT = sharedTmpBuffer.ReinterpretCast<T>();
    GatherRepeatParams gatherbRepeatParams;
    uint8_t innerRepeatTimes = hRepeat * vRepeat;
    constexpr uint32_t eleCntOfOneRep = DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);

    GatherbImpl((__attribute__((cce_unif_buff)) uint16_t *)sharedTmpBufferT.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint16_t *)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src0OffsetLocal.GetPhyAddr(), src0Local.GetSize(), innerRepeatTimes, gatherbRepeatParams);
    uint32_t posSrc1Brcb = hRepeat * vRepeat * DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);
    BrcbRepeatParams brcbRepeatParams;
    Brcb(sharedTmpBufferT[posSrc1Brcb], src1Local, src1Local.GetSize() / brcbEleNum, brcbRepeatParams);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams mulRepeatParams;
    if (repeatMode == false) {
        mulRepeatParams.src0RepStride = 1;
        mulRepeatParams.src0BlkStride = 0;
    }

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, innerRepeatTimes * eleCntOfOneRep);
    Mul<T, false>(sharedTmpBufferT, sharedTmpBufferT[posSrc1Brcb], sharedTmpBufferT, mask, innerRepeatTimes,
        mulRepeatParams);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams addRepeatParams;
    addRepeatParams.dstRepStride = 0;
    addRepeatParams.src1RepStride = 0;
    for (int i = 0; i < vRepeat; i++) {
        if (hRepeat > 1) {
            Add(sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], sharedTmpBufferT[(i * hRepeat + 1) * eleCntOfOneRep],
                sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], mask, hRepeat - 1, addRepeatParams);
        }
    }
    PipeBarrier<PIPE_V>();

    UnaryRepeatParams addsRepeatParams;
    addsRepeatParams.srcRepStride = hRepeat * DEFAULT_REPEAT_STRIDE;
    addsRepeatParams.dstBlkStride = dstBlkStride;
    addsRepeatParams.dstRepStride = vROffset * sizeof(T) / ONE_BLK_SIZE;
    Adds(dstLocal, sharedTmpBufferT, (T)0, mask, vRepeat, addsRepeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolationCalc(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    auto sharedTmpBufferT = sharedTmpBuffer.ReinterpretCast<T>();
    GatherRepeatParams gatherbRepeatParams;
    uint8_t innerRepeatTimes = hRepeat * vRepeat;
    constexpr uint32_t eleCntOfOneRep = DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);

    GatherbImpl((__attribute__((cce_unif_buff)) uint16_t *)sharedTmpBufferT.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint16_t *)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src0OffsetLocal.GetPhyAddr(), src0Local.GetSize(), innerRepeatTimes, gatherbRepeatParams);
    uint32_t posSrc1Brcb = hRepeat * vRepeat * DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(T);
    BrcbRepeatParams brcbRepeatParams;
    Brcb(sharedTmpBufferT[posSrc1Brcb], src1Local, src1Local.GetSize() / brcbEleNum, brcbRepeatParams);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams mulRepeatParams;
    if (repeatMode == false) {
        mulRepeatParams.src0RepStride = 1;
        mulRepeatParams.src0BlkStride = 0;
    }

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, innerRepeatTimes * eleCntOfOneRep);
    Mul<T, false>(sharedTmpBufferT, sharedTmpBufferT[posSrc1Brcb], sharedTmpBufferT, mask, innerRepeatTimes,
        mulRepeatParams);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams addRepeatParams;
    addRepeatParams.dstRepStride = 0;
    addRepeatParams.src1RepStride = 0;
    for (int i = 0; i < vRepeat; i++) {
        if (hRepeat > 1) {
            Add(sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], sharedTmpBufferT[(i * hRepeat + 1) * eleCntOfOneRep],
                sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], mask, hRepeat - 1, addRepeatParams);
        }
    }
    PipeBarrier<PIPE_V>();

    UnaryRepeatParams addsRepeatParams;
    addsRepeatParams.srcRepStride = hRepeat * DEFAULT_REPEAT_STRIDE;
    addsRepeatParams.dstBlkStride = dstBlkStride;
    addsRepeatParams.dstRepStride = vROffset * sizeof(T) / ONE_BLK_SIZE;
    Adds(dstLocal, sharedTmpBufferT, (T)0, mask, vRepeat, addsRepeatParams);
}
}
#pragma end_pipe
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm" 2






#pragma begin_pipe(V)
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
# 63 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm"
                                                                            ;
                                                                                             ;
    BilinearInterpolationCalc(dstLocal, src0Local, src0OffsetLocal, src1Local, mask, hRepeat,
        repeatMode, dstBlkStride, vROffset, vRepeat, sharedTmpBuffer);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void BilinearInterpolation(const LocalTensor<T> &dstLocal, const LocalTensor<T> &src0Local,
    const LocalTensor<uint32_t> &src0OffsetLocal, const LocalTensor<T> &src1Local, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
# 92 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_bilinearinterpalation_intf.cppm"
                                                                            ;
                                                                                             ;
    BilinearInterpolationCalc(dstLocal, src0Local, src0OffsetLocal, src1Local, mask, hRepeat,
        repeatMode, dstBlkStride, vROffset, vRepeat, sharedTmpBuffer);
}
}
#pragma end_pipe
# 33 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_createvecindex_intf.cppm" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_createvecindex_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_createvecindex_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_createvecindex_impl.h"
namespace AscendC {
constexpr int32_t maskBitNum = 64;

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexOneBlk(const LocalTensor<T> &dstLocal, const T &firstValue, uint32_t calCount)
{
    for (int32_t i = 0; i < static_cast<int32_t>(calCount); i++) {
        dstLocal.SetValue(i, static_cast<float>(firstValue) + static_cast<float>(i));
    }
    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexOneRep(const LocalTensor<T> &dstLocal, const T &firstValue, uint64_t mask[],
    uint16_t dstBlkStride)
{
    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int i = 0; i < 2; i++) {
            uint64_t maskValue = 1;
            for (int j = 0; j < maskBitNum; j++) {
                if (mask[i] & maskValue) {
                    uint32_t index = i * maskBitNum + j;
                    uint32_t blkIndex = index / eleCntOfOneBlk;
                    uint32_t eleIndex = blkIndex * eleCntOfOneBlk * dstBlkStride + index % eleCntOfOneBlk;
                    dstLocal.SetValue(eleIndex, (float)firstValue + (float)(i * maskBitNum + j));
                }
                maskValue <<= 1;
            }
        }
    } else {
        uint64_t maskValue = 1;
        for (int j = 0; j < maskBitNum; j++) {
            if (mask[0] & maskValue) {
                uint32_t blkIndex = j / eleCntOfOneBlk;
                uint32_t eleIndex = blkIndex * eleCntOfOneBlk * dstBlkStride + j % eleCntOfOneBlk;
                dstLocal.SetValue(eleIndex, (float)firstValue + (float)j);
            }
            maskValue <<= 1;
        }
    }
    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexCalc(LocalTensor<T> &dstLocal, const T &firstValue, uint64_t mask,
    uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    if (mask < eleCntOfOneBlk) {
        CreateVecIndexOneBlk(dstLocal, firstValue, mask);
    } else {
        CreateVecIndexOneBlk(dstLocal, firstValue, eleCntOfOneBlk);
    }
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    UnaryRepeatParams addsParams;

    int32_t loopN = mask / eleCntOfOneBlk - 1;
    int32_t tailSize = mask % eleCntOfOneBlk;
    int32_t blkEleStride = dstBlkStride * eleCntOfOneBlk;
    int32_t repEleStride = dstRepStride * eleCntOfOneBlk;
    for (int i = 0; i < loopN; i++) {
        Adds(dstLocal[(i + 1) * blkEleStride], dstLocal[i * blkEleStride], (T)(eleCntOfOneBlk), eleCntOfOneBlk, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    addsParams.dstBlkStride = dstBlkStride;
    addsParams.srcBlkStride = dstBlkStride;
    int32_t offsetTailDst = mask / eleCntOfOneBlk * eleCntOfOneBlk * dstBlkStride;
    int32_t offsetTailSrc = offsetTailDst - eleCntOfOneBlk * dstBlkStride;
    if (tailSize > 0) {
        Adds(dstLocal[offsetTailDst], dstLocal[offsetTailSrc], (T)eleCntOfOneBlk, tailSize, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }


    for (int i = 0; i < repeatTimes - 1; i++) {
        Adds(dstLocal[(i + 1) * repEleStride], dstLocal[i * repEleStride], (T)(eleCntOfOneRep), mask, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexCalc(LocalTensor<T> &dstLocal, const T &firstValue, uint64_t mask[],
    uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

    CreateVecIndexOneRep(dstLocal, firstValue, mask, dstBlkStride);

    UnaryRepeatParams addsParams;
    addsParams.dstBlkStride = dstBlkStride;
    addsParams.srcBlkStride = dstBlkStride;
    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    int32_t blkEleStride = dstBlkStride * eleCntOfOneBlk;
    int32_t repEleStride = dstRepStride * eleCntOfOneBlk;
    for (int i = 0; i < repeatTimes - 1; i++) {
        Adds(dstLocal[(i + 1) * repEleStride], dstLocal[i * repEleStride], (T)(eleCntOfOneRep), mask, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void CreateVecIndexCalc(LocalTensor<T> dstLocal, const T &firstValue, uint32_t calCount)
{

    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    if (calCount <= eleCntOfOneBlk) {
        CreateVecIndexOneBlk(dstLocal, firstValue, (uint32_t)eleCntOfOneBlk);
        return;
    }
    CreateVecIndexOneBlk(dstLocal, firstValue, (uint32_t)eleCntOfOneBlk);

    UnaryRepeatParams addsParams;
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));

    int32_t loopN = 0, tailSize = 0, offsetTailDst, offsetTailSrc;
    if (calCount >= eleCntOfOneRep) {
        loopN = DEFAULT_REPEAT_STRIDE - 1;
    } else {
        loopN = calCount / eleCntOfOneBlk - 1;
        tailSize = calCount % eleCntOfOneBlk;
    }
    for (int i = 0; i < loopN; i++) {
        Adds(dstLocal[(i + 1) * eleCntOfOneBlk], dstLocal[i * eleCntOfOneBlk], (T)eleCntOfOneBlk, eleCntOfOneBlk, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    offsetTailDst = calCount / eleCntOfOneBlk * eleCntOfOneBlk;
    offsetTailSrc = offsetTailDst - eleCntOfOneBlk;
    if (tailSize > 0) {
        Adds(dstLocal[offsetTailDst], dstLocal[offsetTailSrc], (T)eleCntOfOneBlk, tailSize, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
    if (calCount <= eleCntOfOneRep) {
        return;
    }

    loopN = calCount / eleCntOfOneRep - 1;
    tailSize = calCount % eleCntOfOneRep;
    for (int i = 0; i < loopN; i++) {
        Adds(dstLocal[(i + 1) * eleCntOfOneRep], dstLocal[i * eleCntOfOneRep], (T)(eleCntOfOneRep), eleCntOfOneRep, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    offsetTailDst = calCount / eleCntOfOneRep * eleCntOfOneRep;
    offsetTailSrc = offsetTailDst - eleCntOfOneRep;
    if (tailSize > 0) {
        Adds(dstLocal[offsetTailDst], dstLocal[offsetTailSrc], (T)(eleCntOfOneRep), tailSize, 1, addsParams);
    }
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_createvecindex_intf.cppm" 2






namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask, uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

                                                                                                                     ;





    CreateVecIndexCalc(dstLocal, firstValue, mask, repeatTimes, dstBlkStride, dstRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dstLocal, const T &firstValue,
    uint64_t mask[], uint8_t repeatTimes, uint16_t dstBlkStride, uint8_t dstRepStride)
{

                                                                                                                     ;





    CreateVecIndexCalc(dstLocal, firstValue, mask, repeatTimes, dstBlkStride, dstRepStride);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> dstLocal, const T &firstValue,
    uint32_t calCount)
{

                                                                                                                     ;





    CreateVecIndexCalc(dstLocal, firstValue, calCount);
}
}
# 34 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_mulcast_intf.cppm" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_mulcast_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_mulcast_impl.h" 1
# 28 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_mulcast_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastCalc(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<U>(mask);
        if constexpr (IsSameType<T, int8_t>::value) {
            vmulconv_f162s8((__attribute__((cce_unif_buff)) int8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0Local.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) half *)src1Local.GetPhyAddr(), repeatTimes, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
        } else {
            vmulconv_f162u8((__attribute__((cce_unif_buff)) uint8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0Local.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) half *)src1Local.GetPhyAddr(), repeatTimes, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastCalc(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        if constexpr (IsSameType<T, int8_t>::value) {
            vmulconv_f162s8((__attribute__((cce_unif_buff)) int8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0Local.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) half *)src1Local.GetPhyAddr(), repeatTimes, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
        } else {
            vmulconv_f162u8((__attribute__((cce_unif_buff)) uint8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0Local.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) half *)src1Local.GetPhyAddr(), repeatTimes, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
        }
    }
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCastCalc(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIV) {
        BinaryRepeatParams repeatParams;
        repeatParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        set_mask_count();
        set_vector_mask(0, calCount);
        if constexpr (IsSameType<T, int8_t>::value) {
            vmulconv_f162s8((__attribute__((cce_unif_buff)) int8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0Local.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) half *)src1Local.GetPhyAddr(), 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
                repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
                repeatParams.src1RepStride);
        } else {
            vmulconv_f162u8((__attribute__((cce_unif_buff)) uint8_t *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0Local.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) half *)src1Local.GetPhyAddr(), 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
                repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
                repeatParams.src1RepStride);
        }
        set_mask_norm();
        ResetMask();
    }
}
}
#pragma end_pipe
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_mulcast_intf.cppm" 2






#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams &repeatParams)
{

                                                                                                                  ;





    MulCastCalc(dstLocal, src0Local, src1Local, mask, repeatTimes, repeatParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{

                                                                                                                  ;





    MulCastCalc(dstLocal, src0Local, src1Local, mask, repeatTimes, repeatParams);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulCast(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint32_t calCount)
{

                                                                                                                  ;





    MulCastCalc(dstLocal, src0Local, src1Local, calCount);
}
}
#pragma end_pipe
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_determine_compute_sync_intf.cppm" 1
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_determine_compute_sync_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_determine_compute_sync_impl.h" 1
# 30 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_determine_compute_sync_intf.cppm" 2




namespace AscendC {





[aicore] __inline__ __attribute__((always_inline)) void InitDetermineComputeWorkspace(GlobalTensor<int32_t> &gmWorkspace,
    LocalTensor<int32_t> &ubWorkspace)
{
    InitDetermineComputeWorkspaceCalc(gmWorkspace, ubWorkspace);
}





[aicore] __inline__ __attribute__((always_inline)) void WaitPreBlock(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    WaitPreBlockCalc(gmWorkspace, ubWorkspace);
}






[aicore] __inline__ __attribute__((always_inline)) void NotifyNextBlock(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    NotifyNextBlockCalc(gmWorkspace, ubWorkspace);
}
}
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
template <typename T> [aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal)
{


                                                       ;





    TransposeImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr());
}
# 74 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HD(const LocalTensor<T> (&dstLocalList)[NCHW_CONV_ADDR_LIST_SIZE],
    const LocalTensor<T> (&srcLocalList)[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams)
{


                                                                                                  ;





    __attribute__((cce_unif_buff)) T* dstList[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList[NCHW_CONV_ADDR_LIST_SIZE];

    for (int32_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (__attribute__((cce_unif_buff)) T*)dstLocalList[i].GetPhyAddr();
        srcList[i] = (__attribute__((cce_unif_buff)) T*)srcLocalList[i].GetPhyAddr();
    }

    TransDataTo5HDImpl(dstList, srcList, nchwconvParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void TransDataTo5HD(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams)
{


                                                                                                     ;
# 118 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
    TransDataTo5HDImpl<T>(dstList, srcList, nchwconvParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Transpose(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams)
{
# 151 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_transpose_intf.cppm"
    if ((transposeParams.transposeType == TransposeType::TRANSPOSE_ND2ND_B16) &&
        (transposeParams.hSize == NCHW_CONV_ADDR_LIST_SIZE) && (transposeParams.wSize == NCHW_CONV_ADDR_LIST_SIZE)) {
        TransposeImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr());
    } else if (transposeParams.transposeType == TransposeType::TRANSPOSE_NCHW2NHWC ||
        transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        if (transposeParams.cSize == 1) {
            struct DataCopyParams repeatParams;
            repeatParams.blockLen = transposeParams.nSize * transposeParams.cSize * transposeParams.hSize *
                transposeParams.wSize / AscendCUtils::GetC0Count(sizeof(T));
            DataCopyUB2UBImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), repeatParams);
        } else {
            Transpose4DImpl(dstLocal, srcLocal, sharedTmpBuffer, transposeParams);
        }
    }
}
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void TransDataTo5HD(const LocalTensor<uint64_t> &dstLocal,
    const LocalTensor<uint64_t> &srcLocal, const TransDataTo5HDParams &nchwconvParams)
{


                                                                                                  ;







    constexpr uint32_t vaRegSize = VA_REG_ARRAY_LEN / HALF_FACTOR;
    constexpr uint32_t vaOne = 1;
    constexpr uint32_t vaTwo = 2;
    constexpr uint32_t vaThree = 3;
    constexpr uint64_t vaAddr = 5;
    constexpr uint64_t vaMask = 0x1fff;
    constexpr uint64_t vaBit1 = 16;
    constexpr uint64_t vaBit2 = 32;
    constexpr uint64_t vaBit3 = 48;

    for (uint32_t i = 0; i < vaRegSize; i++)
    {
        uint64_t dstAddrConfig = (((dstLocal.GetValue(vaRegSize * i) >> vaAddr) & vaMask) |
                                  (((dstLocal.GetValue(vaRegSize * i + vaOne) >> vaAddr) & vaMask) << vaBit1) |
                                  (((dstLocal.GetValue(vaRegSize * i + vaTwo) >> vaAddr) & vaMask) << vaBit2) |
                                  (((dstLocal.GetValue(vaRegSize * i + vaThree) >> vaAddr) & vaMask) << vaBit3));
        dstLocal.SetValue(i, dstAddrConfig);

        uint64_t srcAddrConfig = (((srcLocal.GetValue(vaRegSize * i) >> vaAddr) & vaMask) |
                                  (((srcLocal.GetValue(vaRegSize * i + vaOne) >> vaAddr) & vaMask) << vaBit1) |
                                  (((srcLocal.GetValue(vaRegSize * i + vaTwo) >> vaAddr) & vaMask) << vaBit2) |
                                  (((srcLocal.GetValue(vaRegSize * i + vaThree) >> vaAddr) & vaMask) << vaBit3));
        srcLocal.SetValue(i, srcAddrConfig);
    }

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    TransDataTo5HDVldVaRegImpl<T>(
        (__attribute__((cce_unif_buff)) uint64_t*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint64_t*)srcLocal.GetPhyAddr(), nchwconvParams);

}
}
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gatherb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<uint32_t>& offsetLocal, const uint8_t repeatTimes, const GatherRepeatParams& repeatParams)
{





    uint32_t srcLength = src0Local.GetSize();
    GatherbImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t*)offsetLocal.GetPhyAddr(), srcLength, repeatTimes, repeatParams);
}
# 75 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstRepStride)
{



                                                                                                          ;
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
    const uint32_t srcLength = srcLocal.GetSize();
    GatherImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal.GetPhyAddr(), srcLength, srcBaseAddr, mask, repeatTimes, dstRepStride);
}
# 110 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstRepStride)
{







                                                                                           ;
# 133 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
    const uint32_t srcLength = srcLocal.GetSize();
    GatherImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal.GetPhyAddr(), srcLength, srcBaseAddr, mask, repeatTimes, dstRepStride);
}
# 147 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Gather(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& srcOffsetLocal, const uint32_t srcBaseAddr, const uint32_t count)
{







    uint32_t elementCountSingleRepeat;
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        elementCountSingleRepeat = 128;
    } else {
        elementCountSingleRepeat = 64;
    }
    const uint32_t elementCountTail = count % elementCountSingleRepeat;
    const uint8_t repeatTimes = count / elementCountSingleRepeat;
    if (repeatTimes > 0) {
        Gather(dstLocal, srcLocal, srcOffsetLocal, srcBaseAddr, (uint64_t)elementCountSingleRepeat, repeatTimes,
            DEFAULT_REPEAT_STRIDE);
    }
    if (elementCountTail > 0) {
        const uint32_t offset = count - elementCountTail;
        Gather(dstLocal[offset], srcLocal, srcOffsetLocal[offset], srcBaseAddr, (uint64_t)elementCountTail, 1,
            DEFAULT_REPEAT_STRIDE);
    }

}
}
#pragma end_pipe
# 38 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask,
    const uint8_t repeatTimes, const uint8_t srcRepStride)
{







                                                                              ;






    const uint32_t dstLength = dstLocal.GetSize();
    ScatterImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)dstOffsetLocal.GetPhyAddr(), dstLength, dstBaseAddr, mask, repeatTimes, srcRepStride);
}
# 84 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint64_t mask[],
    const uint8_t repeatTimes, const uint8_t srcRepStride)
{







                                                                              ;






    const uint32_t dstLength = dstLocal.GetSize();
    ScatterImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)dstOffsetLocal.GetPhyAddr(), dstLength, dstBaseAddr, mask, repeatTimes, srcRepStride);
}
# 117 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Scatter(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint32_t>& dstOffsetLocal, const uint32_t dstBaseAddr, const uint32_t count)
{







                                                                              ;

                                                                               ;
                                                                                     ;
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_scatter_intf.cppm"
    uint32_t elementCountSingleRepeat;
    if constexpr (sizeof(T) == sizeof(uint16_t)) {
        elementCountSingleRepeat = 128;
    } else {
        elementCountSingleRepeat = 64;
    }
    const uint32_t elementCountTail = count % elementCountSingleRepeat;
    const uint8_t repeatTimes = count / elementCountSingleRepeat;
    if (repeatTimes > 0) {
        Scatter(dstLocal, srcLocal, dstOffsetLocal, dstBaseAddr, (uint64_t)elementCountSingleRepeat, repeatTimes,
            DEFAULT_REPEAT_STRIDE);
    }
    if (elementCountTail > 0) {
        const uint32_t offset = count - elementCountTail;
        Scatter(dstLocal, srcLocal[offset], dstOffsetLocal[offset], dstBaseAddr, (uint64_t)elementCountTail, 1,
            DEFAULT_REPEAT_STRIDE);
    }

}
}
#pragma end_pipe
# 39 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_brcb_intf.cppm" 1
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_brcb_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_brcb_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Brcb(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local, const uint8_t repeatTimes,
    const BrcbRepeatParams& repeatParams)
{





    BrcbImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), repeatTimes, repeatParams);
}
}
#pragma end_pipe
# 40 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm" 1
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 94 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Add(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    AddImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 126 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    SubImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    SubImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 165 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sub(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    SubImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 197 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 236 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Mul(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    MulImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 268 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    DivImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    DivImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 307 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Div(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    DivImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 339 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, const uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
# 354 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
    MulAddDstImpl<PrimDstType, PrimSrcType, isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, uint64_t mask, const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
# 374 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
    MulAddDstImpl<PrimDstType, PrimSrcType, isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 387 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void MulAddDst(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, const int32_t& calCount)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;







    MulAddDstImpl((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), calCount);
}
# 422 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 461 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Max(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    MaxImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 493 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 532 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Min(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    MinImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 564 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AndImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AndImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 603 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void And(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    AndImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 635 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    OrImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    OrImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 674 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Or(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    OrImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 706 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    AddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    AddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void AddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    AddReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 786 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<half>& dstLocal, const LocalTensor<int32_t>& src0Local,
    const LocalTensor<int32_t>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((IsSameType<PrimDstType, half>::value && IsSameType<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<half> &dstLocal, const LocalTensor<int32_t> &src0Local,
    const LocalTensor<int32_t> &src1Local, uint64_t mask, const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) half*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T> &dstLocal, const LocalTensor<U> &src0Local,
    const LocalTensor<U> &src1Local, uint64_t mask, const uint8_t repeatTimes,
    const BinaryRepeatParams &repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((IsSameType<PrimDstType, half>::value && IsSameType<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 864 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<half>& dstLocal, const LocalTensor<int32_t>& src0Local,
    const LocalTensor<int32_t>& src1Local, const int32_t& calCount)
{





    AddDeqReluImpl((__attribute__((cce_unif_buff)) half *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t *)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t *)src1Local.GetPhyAddr(), calCount);
}

template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void AddDeqRelu(const LocalTensor<T>& dstLocal, const LocalTensor<U>& src0Local,
    const LocalTensor<U>& src1Local, const int32_t& calCount)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((IsSameType<PrimDstType, half>::value && IsSameType<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");





    AddDeqReluImpl((__attribute__((cce_unif_buff)) PrimDstType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src1Local.GetPhyAddr(), calCount);
}
# 911 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 962 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAdd(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    FusedMulAddImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 999 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}
# 1050 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void FusedMulAddRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    FusedMulAddReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
# 1087 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), mask, repeatTimes,
        repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SubRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const int32_t& calCount)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    SubReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1Local.GetPhyAddr(), calCount);
}
}
#pragma end_pipe
# 41 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm" 1
# 35 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}
# 119 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Adds(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 164 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}
# 230 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Muls(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 275 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}
# 341 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Maxs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 386 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, mask, repeatTimes, repeatParams);
}
# 452 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void Mins(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        scalarValue, calCount);
}
# 497 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}
# 563 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftLeft(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
# 608 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams, roundEn);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams, roundEn);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams, roundEn);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams, bool roundEn)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams, roundEn);
}
# 674 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void ShiftRight(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
# 719 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, repeatParams);
}
# 785 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_binary_scalar_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const T& scalarValue,
    const int32_t& calCount)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), scalarValue,
        calCount);
}

template <typename T, typename U, bool isSetMask, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type>
[aicore] __inline__ __attribute__((always_inline)) void LeakyRelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
}
#pragma end_pipe
# 42 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 57 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    VcmpvImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    VcmpvImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, CMPMODE cmpMode,
    const uint64_t mask[], const BinaryRepeatParams& repeatParams)
{







    VcmpImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, CMPMODE cmpMode,
    const uint64_t mask, const BinaryRepeatParams& repeatParams)
{







    VcmpImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, mask,
        repeatParams);
}
# 126 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Compare(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, CMPMODE cmpMode, uint32_t calCount)
{





    VcmpvImpl((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), cmpMode, calCount);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetCmpMask(const LocalTensor<T>& dst)
{

    if (g_coreType == AIC) {
        return;
    }






    GetCmpMaskImpl((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr());
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetCmpMask(const LocalTensor<T>& src)
{

    if (g_coreType == AIC) {
        return;
    }






    SetCmpMaskImpl((__attribute__((cce_unif_buff)) T*)src.GetPhyAddr());

}
# 189 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{






    VcmpvsImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Scalar,
        cmpMode, mask, repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{






    VcmpvsImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Scalar,
        cmpMode, mask, repeatTimes, repeatParams);
}
# 228 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void CompareScalar(const LocalTensor<U>& dstLocal, const LocalTensor<T>& src0Local,
    const T src1Scalar, CMPMODE cmpMode, uint32_t calCount)
{





    VcmpvsImpl((__attribute__((cce_unif_buff)) U*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Scalar, cmpMode, calCount);
}
# 265 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask[],
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
# 278 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
                                                                                                                ;
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), selMode, mask, repeatTimes, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint64_t mask,
    uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{
# 297 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
                                                                                                                ;
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), selMode, mask, repeatTimes, repeatParams);
}
# 313 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, const LocalTensor<T>& src1Local, SELMODE selMode, uint32_t calCount)
{







                                                                                                                ;
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), selMode, calCount);
}
# 348 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask[], uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");


                                                                                                                ;
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        src1Local, selMode, mask, repeatTimes, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint64_t mask, uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{






    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");


                                                                                                                ;
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        src1Local, selMode, mask, repeatTimes, repeatParams);
}
# 398 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_cmpsel_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Select(const LocalTensor<T>& dstLocal, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0Local, T src1Local, SELMODE selMode, uint32_t calCount)
{





    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");


                                                                                                                ;
    VselImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        src1Local, selMode, calCount);
}
}
#pragma end_pipe
# 43 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    CheckDuplicateSupportedType<T>();






    DuplicateImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, dstBlockStride,
        dstRepeatStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTimes, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    CheckDuplicateSupportedType<T>();






    DuplicateImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), scalarValue, mask, repeatTimes, dstBlockStride,
        dstRepeatStride);
}
# 88 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_duplicate_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Duplicate(const LocalTensor<T>& dstLocal, const T& scalarValue, const int32_t& calCount)
{
    CheckDuplicateSupportedType<T>();





    DuplicateImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), scalarValue, calCount);
}
}
#pragma end_pipe
# 44 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        maskCount, dstRepStride, srcBlkStride, srcRepStride);
}
# 81 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        maskCount, dstRepStride, srcBlkStride, srcRepStride);
}
# 111 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        maskCount, dstRepStride, srcBlkStride, srcRepStride);
}
# 141 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t maskCount, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                    ;
                                                                        ;







    PairReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        maskCount, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void BlockReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                         ;







    BlockReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void PairReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                    ;
                                                                        ;







    PairReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void RepeatReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t repeat, const int32_t elemsInOneRepeat, const int32_t dstBlkStride, const int32_t srcBlkStride,
    const int32_t dstRepStride, const int32_t srcRepStride)
{

                                                                                                      ;
                                                                          ;







    RepeatReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), repeat,
        elemsInOneRepeat, dstBlkStride, srcBlkStride, dstRepStride, srcRepStride);
}
# 267 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                                   ;







    WholeReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), mask,
        repeatTimes, dstRepStride, srcBlkStride, srcRepStride);
}
# 297 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 317 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), mask,
        repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 332 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint64_t mask[], const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 352 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{

                                                                                                     ;
                                                                                   ;







    WholeReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 394 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMaxImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void WholeReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t mask, const int32_t repeatTimes, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{

                                                                                                     ;
                                                                                   ;

                                                                          ;
# 417 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
    WholeReduceMinImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 433 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{

                                                                                     ;





    ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);
}
# 462 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{

                                                                                     ;





    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);
}
# 490 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t mask, const int32_t repeatTimes, const int32_t srcRepStride)
{

                                                                                     ;





    ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, 0, ReduceMode::REDUCE_SUM);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{

                                                                                     ;





    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride,
    bool calIndex)
{

                                                                                     ;





    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);
}
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const uint64_t mask[], const int32_t repeatTimes, const int32_t srcRepStride)
{

                                                                                     ;





    struct ReduceRepeatParams params(mask, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, 0, ReduceMode::REDUCE_SUM);
}
# 567 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMin(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex)
{

                                                                                     ;
                                                                                        ;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t repeatTimes = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;
    int32_t bodyCount = elementNumPerRep;

    if (repeatTimes == 0) {
        repeatTimes = 1;
        bodyCount = count;
        tailCount = 0;
    }





    struct ReduceRepeatParams params(bodyCount, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);

    if (tailCount != 0) {
        ReduceTailCompute(dstLocal, srcLocal, workLocal, count, calIndex, ReduceMode::REDUCE_MIN);
    }
}
# 608 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void ReduceMax(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count, bool calIndex)
{

                                                                                     ;
                                                                                        ;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t repeatTimes = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;
    int32_t bodyCount = elementNumPerRep;

    if (repeatTimes == 0) {
        repeatTimes = 1;
        bodyCount = count;
        tailCount = 0;
    }






    struct ReduceRepeatParams params(bodyCount, repeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceImpl<T>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)workLocal.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);

    if (tailCount != 0) {
        ReduceTailCompute(dstLocal, srcLocal, workLocal, count, calIndex, ReduceMode::REDUCE_MAX);
    }
}
# 649 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void ReduceSum(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& workLocal, const int32_t count)
{

                                                                                     ;
                                                                                        ;

    ReduceSumImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)srcLocal.GetPhyAddr(), count);
# 708 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_reduce_intf.cppm"
}
#pragma end_pipe
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCount(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{

    if (g_coreType == AIC) {
        return;
    }

    GetReduceMaxMinCountImpl<T>(maxMinValue, maxMinIndex);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void GetReduceMaxMinCount(uint32_t &maxMinValue)
{
    GetReduceMaxMinCountImpl<T>(maxMinValue);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetAccVal()
{

    if (g_coreType == AIC) {
        return 0;
    }

    return get_acc_val();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue, T &maxMinIndex)
{


                   ;

    if (g_coreType == AIC) {
        return;
    }

    GetReduceMaxMinCountImpl<T>(maxMinValue, maxMinIndex);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue)
{

                                                                                                      ;
    GetReduceMaxMinCountImpl<T>(maxMinValue);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) T GetAccVal()
{

                                                                   ;

    if (g_coreType == AIC) {
        return 0;
    }

    return GetAccValImpl<T>();
}
}
# 45 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
template <typename T, typename U, GatherMaskMode mode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<U>& src1Pattern, const bool reduceMode, const uint32_t mask,
    const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt)
{
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm"
    GatherMaskCal<T, mode>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) U*)src1Pattern.GetPhyAddr(), reduceMode, mask, gatherMaskParams, rsvdCnt);

}

template <typename T, GatherMaskMode mode>
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const uint8_t src1Pattern, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt)
{
# 75 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_gather_mask_intf.cppm"
    GatherMaskCal<T, mode>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), src1Pattern,
        reduceMode, mask, gatherMaskParams, rsvdCnt);

}

template <typename T>
[[deprecated("NOTICE: This GatheMask in this form has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const LocalTensor<T>& src1Local, const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
    GatherMaskImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T*)src1Local.GetPhyAddr(), patternMode, gatherMaskParams);
}

template <typename T>
[[deprecated("NOTICE: This GatheMask in this form has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] __inline__ __attribute__((always_inline)) void GatherMask(const LocalTensor<T>& dstLocal, const LocalTensor<T>& src0Local,
    const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
    GatherMaskImpl((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0Local.GetPhyAddr(), patternMode,
        gatherMaskParams);
}
#pragma end_pipe

[aicore] __inline__ __attribute__((always_inline)) __attribute__((inout_pipe("S"))) int64_t GetGatherMaskRemainCount()
{

    if (g_coreType == AIC) {
        return 0;
    }

    return GetGatherMaskRemainCountImpl();
}
}
# 46 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm"
template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AxpyImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{






    AxpyImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), scalarValue, mask,
        repeatTimes, repeatParams);
}
# 87 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_ternary_scalar_intf.cppm"
template <typename T, typename U>
[aicore] __inline__ __attribute__((always_inline)) void Axpy(const LocalTensor<T>& dstLocal, const LocalTensor<U>& srcLocal, const U& scalarValue,
    const int32_t& calCount)
{





    AxpyImpl<T, U>((__attribute__((cce_unif_buff)) T*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)srcLocal.GetPhyAddr(), scalarValue, calCount);
}
}
#pragma end_pipe
# 47 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
#pragma begin_pipe(V)
namespace AscendC {
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 93 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Relu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    ReluImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 118 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ExpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ExpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 156 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    ExpImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 181 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LnImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LnImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 217 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Ln(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    LnImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 242 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    AbsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    AbsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 280 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Abs(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    AbsImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 305 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReciprocalImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReciprocalImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 343 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Reciprocal(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    ReciprocalImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 369 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    RsqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    RsqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 407 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Rsqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    RsqrtImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 432 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    SqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    SqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 470 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Sqrt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    SqrtImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
# 495 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask[],
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    NotImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, uint64_t mask,
    const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    NotImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(),
        mask, repeatTimes, repeatParams);
}
# 533 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_unary_intf.cppm"
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void Not(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const int32_t& calCount)
{
    using PrimType = PrimT<T>;





    NotImpl((__attribute__((cce_unif_buff)) PrimType*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)srcLocal.GetPhyAddr(), calCount);
}
}
#pragma end_pipe
# 48 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm" 1
# 36 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
namespace AscendC {
#pragma begin_pipe(V)
# 55 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{
# 73 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), round_mode,
        mask, repeatTimes, repeatParams);
}


template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint64_t mask, const uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
# 95 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), round_mode,
        mask, repeatTimes, repeatParams);
}
# 107 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void Cast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const RoundMode& round_mode, const uint32_t calCount)
{
# 122 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    if constexpr (!(IsSameType<T1, int4b_t>::value) && !(IsSameType<T2, int4b_t>::value)) {



                                                                  ;
    }
    CastImpl((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), round_mode, calCount);
}
# 142 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
# 157 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastDeqImpl<T1, T2, isSetMask, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
template <typename T1, typename T2, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const int32_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
# 174 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastDeqImpl<T1, T2, isSetMask, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 189 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isVecDeq, bool halfBlock>
[aicore] __inline__ __attribute__((always_inline)) void CastDeq(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& srcLocal,
    const uint32_t calCount)
{
# 202 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    CastDeqImpl<T1, T2, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)srcLocal.GetPhyAddr(),
        calCount);
}
# 225 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    AddReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}


template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    AddReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 273 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void AddReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }






    AddReluCastImpl((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), calCount);
}
# 310 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask, const uint8_t repeatTimes, const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    SubReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}


template <typename T1, typename T2, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, uint64_t mask[], const uint8_t repeatTimes,
    const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    SubReluCastImpl<T1, T2, isSetMask>((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), mask, repeatTimes, repeatParams);
}
# 358 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
template <typename T1, typename T2>
[aicore] __inline__ __attribute__((always_inline)) void SubReluCast(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& src0Local,
    const LocalTensor<T2>& src1Local, const uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }






    SubReluCastImpl((__attribute__((cce_unif_buff)) T1*)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T2*)src0Local.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T2*)src1Local.GetPhyAddr(), calCount);
}

#pragma end_pipe
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(half scale)
{

    if (g_coreType == AIC) {
        return;
    }




    SetDeqScaleImpl(scale);
}

[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(float scale, int16_t offset, bool signMode)
{

    if (g_coreType == AIC) {
        return;
    }




    SetDeqScaleImpl(scale, offset, signMode);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetDeqScale(const LocalTensor<T>& vdeqTensor, const VdeqInfo& vdeqInfo)
{

    if (g_coreType == AIC) {
        return;
    }
# 418 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm"
    SetDeqScaleImpl<T>(vdeqTensor, vdeqInfo);
}

template <bool castMode>
[aicore] __inline__ __attribute__((always_inline)) void SetCastOverflowMode()
{
    SetCastOverflowModeImpl<castMode>();
}
}
# 49 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vpadding_intf.cppm" 1
# 31 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vpadding_intf.cppm"
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vpadding_impl.h" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c220/kernel_operator_vec_vpadding_impl.h"
namespace AscendC {
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint64_t mask, uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
                                                      ;
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint64_t mask[], uint8_t repeatTimes, const UnaryRepeatParams& repeatParams)
{
                                                      ;
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint32_t calCount)
{
                                                      ;
}
}
# 32 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vpadding_intf.cppm" 2






#pragma begin_pipe(V)
namespace AscendC {
template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask, const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{






    VectorPaddingImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), padMode,
        padSide, mask, repeatTimes, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint64_t mask[], const uint8_t repeatTimes,
    const UnaryRepeatParams& repeatParams)
{






    VectorPaddingImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), padMode,
        padSide, mask, repeatTimes, repeatParams);
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void VectorPadding(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint8_t padMode, const bool padSide, const uint32_t calCount)
{





    VectorPaddingImpl((__attribute__((cce_unif_buff)) T *)dstLocal.GetPhyAddr(), (__attribute__((cce_unif_buff)) T *)srcLocal.GetPhyAddr(), padMode, padSide,
        calCount);
}
}
#pragma end_pipe
# 50 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_scalar_intf.cppm" 1
# 26 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_scalar_intf.cppm"
namespace AscendC {
template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetCountOfValue(uint64_t valueIn)
{
    return ScalarGetCountOfValueImpl<countValue>(valueIn);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarCountLeadingZero(uint64_t valueIn)
{
    return ScalarCountLeadingZeroImpl(valueIn);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t CountBitsCntSameAsSignBit(int64_t valueIn)
{
    return CountBitsCntSameAsSignBitImpl(valueIn);
}

template <int countValue>
[aicore] __inline__ __attribute__((always_inline)) int64_t ScalarGetSFFValue(uint64_t valueIn)
{
    return ScalarGetSFFValueImpl<countValue>(valueIn);
}

template <typename srcT, typename dstT, RoundMode roundMode>
[aicore] __inline__ __attribute__((always_inline)) dstT ScalarCast(srcT valueIn)
{
    return ScalarCastImpl<srcT, dstT, roundMode>(valueIn);
}
}
# 51 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_sys_var_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_sys_var_intf.cppm"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void GetArchVersion(uint32_t& coreVersion)
{
    GetArchVersionImpl(coreVersion);
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSubBlockNum()
{
    return GetSubBlockNumImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetProgramCounter()
{
    return GetProgramCounterImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void Trap()
{
    TrapImpl();
}

[aicore] __inline__ __attribute__((always_inline)) int64_t GetSystemCycle()
{
    return GetSystemCycleImpl();
}
}
# 52 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_set_atomic_intf.cppm" 1
# 37 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_set_atomic_intf.cppm"
namespace AscendC {
template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicType()
{
    SetAtomicTypeImpl<T>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicAdd()
{
    SetAtomicAddImpl<T>();
}

[aicore] __inline__ __attribute__((always_inline)) void SetAtomicNone()
{
    SetAtomicNoneImpl();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMax()
{
    SetAtomicMaxImpl<T>();
}

template <typename T>
[aicore] __inline__ __attribute__((always_inline)) void SetAtomicMin()
{
    SetAtomicMinImpl<T>();
}
}
# 53 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 1 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_prof_trace_intf.cppm" 1
# 25 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_prof_trace_intf.cppm"
namespace AscendC {
[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStart()
{
    ProfStartImpl();
}

[aicore] __inline__ __attribute__((always_inline)) void MetricsProfStop()
{
    ProfStopImpl();
}
}
# 54 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h" 2
# 29 "/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h" 2
# 11 "/home/user3/npu-nvme/hello_world.cpp" 2

extern "C" __attribute__((cce_kernel)) [aicore] void hello_world()
{
    AscendC::do { auto __enable_feature_for_compile_printf = 1; auto __enable_feature_for_compile_printfBufSize = 1048576; } while (0);
}

void hello_world_do(uint32_t blockDim, void *stream)
{
    hello_world<<<blockDim, nullptr, stream>>>();
}
